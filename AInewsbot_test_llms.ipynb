{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb072688",
   "metadata": {},
   "source": [
    "# Test LLM calling\n",
    "- test different ways of calling LLMS, native API, LangChain, sync/async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74032f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to selectively re-import as needed\n",
    "import sys\n",
    "# del sys.modules['ainb_llm']\n",
    "# del sys.modules['ainb_const']\n",
    "# del sys.modules['ainb_utilities']\n",
    "# del sys.modules['ainb_webscrape']\n",
    "# del sys.modules['AInewsbot_langgraph']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562be45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# import dotenv\n",
    "# import subprocess\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "# import operator\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate,\n",
    "                                    SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.errors import NodeInterrupt\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import bs4\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "\n",
    "import asyncio\n",
    "from asyncio import Semaphore\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "# import pyperclip\n",
    "# import shlex\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, TypedDict, Annotated, Any\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "import google.generativeai as genai\n",
    "import httpx\n",
    "\n",
    "import trafilatura   # web scrape uses this to get clean news stories w/o a lot of js and boilerplate\n",
    "\n",
    "from ainb_const import (\n",
    "                        MODEL, LOWCOST_MODEL, HIGHCOST_MODEL, FINAL_SUMMARY_PROMPT,\n",
    "                        REWRITE_PROMPT,\n",
    "                        SCREENSHOT_DIR, SUMMARIZE_SYSTEM_PROMPT, SUMMARIZE_USER_PROMPT\n",
    "                       )\n",
    "\n",
    "from ainb_utilities import log\n",
    "\n",
    "from AInewsbot_langgraph import (newscatcher_sources, fn_initialize, fn_download_sources, fn_extract_urls,\n",
    "                                 fn_verify_download, fn_extract_newscatcher, fn_filter_urls, fn_topic_clusters,\n",
    "                                 fn_topic_analysis, fn_download_pages, fn_summarize_pages, fn_propose_cats,\n",
    "                                 fn_compose_summary, fn_rewrite_summary, fn_is_revision_complete, fn_send_mail\n",
    "                                )\n",
    "\n",
    "\n",
    "import podcastfy\n",
    "from podcastfy.client import generate_podcast, process_content\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from IPython.display import Audio, display, Markdown\n",
    "\n",
    "import pdb\n",
    "\n",
    "# need this to run async in jupyter since it already has an asyncio event loop running\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Activate global verbose logging\n",
    "set_debug(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ba13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python            3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 14:21:42) [Clang 18.1.8 ]\n",
      "LangChain         0.3.18\n",
      "OpenAI            1.63.1\n",
      "trafilatura       2.0.0\n",
      "numpy             1.26.4\n",
      "pandas            2.2.3\n",
      "sklearn           1.6.1\n",
      "umap              0.5.7\n",
      "podcastfy         0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python            {sys.version}\")\n",
    "print(f\"LangChain         {langchain.__version__}\")\n",
    "print(f\"OpenAI            {openai.__version__}\")\n",
    "# print(f\"smtplib           {smtplib.sys.version}\")\n",
    "print(f\"trafilatura       {trafilatura.__version__}\")\n",
    "# print(f\"bs4               {bs4.__version__}\")\n",
    "print(f\"numpy             {np.__version__}\")\n",
    "print(f\"pandas            {pd.__version__}\")\n",
    "print(f\"sklearn           {sklearn.__version__}\")\n",
    "print(f\"umap              {umap.__version__}\")\n",
    "print(f\"podcastfy         {podcastfy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic LLM call with langchain\\\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response = openai_model.invoke([\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian\"),\n",
    "    HumanMessage(content='Listen to me. You are beautiful. You are perfect and I love you.'),\n",
    "])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52d702-271a-4caf-96dc-e8a47700fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8451135-7b09-4b78-9d7e-dab8d43cdbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.usage_metadata\n",
    "# no rate limit info like tokens remaining, available in headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0bd0f-b529-4786-b325-2eae57835ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "# Retrieve the list of available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print out the model IDs\n",
    "models = [model.id for model in models.data]\n",
    "models.sort()\n",
    "print(\"\\n\".join(models))\n",
    "# yay, we got o3-mini API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357cdc3-c02a-499f-9ef2-c8d725173ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gemini_models():\n",
    "    try:\n",
    "        # Configure the library\n",
    "        genai.configure()\n",
    "\n",
    "        # List available models\n",
    "        models = genai.list_models()\n",
    "\n",
    "        print(\"Available Gemini Models:\")\n",
    "        print(\"-----------------------\")\n",
    "        for m in models:\n",
    "            if \"gemini\" in m.name.lower():\n",
    "                print(f\"Name: {m.name}\")\n",
    "                print(f\"Description: {m.description}\")\n",
    "                print(f\"Generation Methods: {m.supported_generation_methods}\")\n",
    "                print(\"-----------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "list_gemini_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d1c0b-342e-49db-b311-f4a141e73db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try Gemini\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "# GEMINI_MODEL = \"gemini-1.5-pro\"  # or \"flash\" depending on the desired model\n",
    "GEMINI_MODEL = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "gmodel = ChatGoogleGenerativeAI(\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = gmodel.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceaee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anthropic\n",
    "\n",
    "# Initialize the ChatAnthropic model\n",
    "claude_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    anthropic_api_key=os.environ[\"CLAUDE_API_KEY\"],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Create the messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = claude_model.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a langchain template\n",
    "system_template = \"You are a translator. Translate the following from English into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt_template | openai_model | parser\n",
    "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382e44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time multiple templates (single-threaded)\n",
    "prompt_inputs = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "system_template = 'You are a translator. Translate the following text from English into {language}. Provide only the translation, no other information:'\n",
    "user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "ochain = prompt_template | openai_model | parser\n",
    "gchain = prompt_template | gmodel | parser\n",
    "cchain = prompt_template | claude_model | parser\n",
    "\n",
    "start_time = datetime.now()\n",
    "for tpl in prompt_inputs:\n",
    "    for chain in [ochain, gchain, cchain]:\n",
    "        response = \"\"\n",
    "        #     print()\n",
    "        #     print(prompt_template.format(**tpl))\n",
    "        # stream tokens as they are generated\n",
    "        for r in chain.stream(tpl):\n",
    "            print(r, end=\"\")\n",
    "            response += r\n",
    "        print()\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry decorator with exponential backoff\n",
    "# if I run google too fast I get a 429 error, need to update something in gcp probably\n",
    "\n",
    "def should_retry_exception(exception):\n",
    "    \"\"\"Determine if the exception should trigger a retry. (always retry)\"\"\"\n",
    "    print(type(exception))\n",
    "    print(exception)\n",
    "    return True\n",
    "\n",
    "\n",
    "# @retry(\n",
    "#     stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "#     wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "#     retry=retry_if_exception_type(should_retry_exception),\n",
    "#     before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    "# )\n",
    "async def process_translation(chain, inputs, name):\n",
    "    response = \"\"\n",
    "    async for chunk in chain.astream(inputs):\n",
    "        response += chunk\n",
    "    return response, name\n",
    "\n",
    "\n",
    "async def main():\n",
    "    prompt_inputs = [\n",
    "        {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "        {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "        {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "        {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "        {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "    ]\n",
    "\n",
    "    system_template = 'You are a translator. Translate the following text into {language}. Provide only the translation, no other information:'\n",
    "    user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", user_template)\n",
    "    ])\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    ochain = prompt_template | openai_model | parser\n",
    "    gchain = prompt_template | gmodel | parser\n",
    "    cchain = prompt_template | claude_model | parser\n",
    "    chains = {'openai': ochain, 'google': gchain, 'claude': cchain}\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    tasks = []\n",
    "    for tpl in prompt_inputs:\n",
    "        print(f\"Queuing {tpl['language']} translations...\")\n",
    "        for name, chain in chains.items():\n",
    "            task = asyncio.create_task(process_translation(chain, tpl, name))\n",
    "            tasks.append(task)\n",
    "\n",
    "    try:\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        for response, name in responses:\n",
    "            print(f\"{name}: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {str(e)}\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    difference = end_time - start_time\n",
    "    total_seconds = difference.total_seconds()\n",
    "    print(f\"\\nElapsed seconds: {total_seconds:.6f}\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659782d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same but use ainvoke, no stream\n",
    "# Rate limit settings\n",
    "# CALLS_PER_MINUTE = 60  # Adjust based on your quota\n",
    "# MAX_CONCURRENT = 5     # Maximum concurrent requests\n",
    "# sem = Semaphore(MAX_CONCURRENT) # semaphore for controlling concurrent requests\n",
    "\n",
    "# @sleep_and_retry\n",
    "# @limits(calls=CALLS_PER_MINUTE, period=60)\n",
    "@retry(\n",
    "    stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "    retry=retry_if_exception_type(should_retry_exception),\n",
    "    before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    ")\n",
    "async def async_langchain(chain, input_dict, name=\"\"):\n",
    "#     async with sem:\n",
    "        response = await chain.ainvoke(input_dict)\n",
    "        return response, name\n",
    "\n",
    "prompt_templates = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "\n",
    "chains = {'openai': ochain, \n",
    "          'google': gchain, \n",
    "          'claude': cchain}\n",
    "\n",
    "start_time = datetime.now()\n",
    "tasks = []\n",
    "for d in prompt_templates:\n",
    "    for name, chain in chains.items():\n",
    "        task = asyncio.create_task(async_langchain(chain, d, name))\n",
    "        tasks.append(task)\n",
    "responses = await asyncio.gather(*tasks)\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "for response, name in responses:\n",
    "    print(f\"{name}: {response}\")\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ddd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test o3-mini\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    reasoning_effort = \"low\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will act as an expert Python developer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You will act as an expert Python developer.\"),\n",
    "     (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "openai_model = ChatOpenAI(model=\"o3-mini\", reasoning_effort=\"low\")\n",
    "\n",
    "ochain = prompt_template | openai_model | parser\n",
    "\n",
    "response = ochain.invoke(\"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e0db6-a2f1-4faa-b0e0-ebdf5a638e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use the metadata to saturate the OpenAI API and use as many tokens per second as available\n",
    "# but not supported by langchain across multiple models so exponential backoff seems to be the best alternative\n",
    "\n",
    "apikey = os.environ.get(\"OPENAI_API_KEY\")\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"OpenAI-Beta\": \"assistants=v2\",\n",
    "    \"Authorization\": f\"Bearer {apikey}\"\n",
    "}\n",
    "body = {\n",
    "    \"model\": \"gpt-4o-2024-08-06\", \"max_tokens\": 25, \"top_p\": 0.8,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is the airspeed velocity of an unladen swallow\"}]\n",
    "        }\n",
    "    ]}\n",
    "\n",
    "try:\n",
    "    response = httpx.post(url, headers=headers, json=body)\n",
    "    response_json = response.json()\n",
    "    print(json.dumps(response_json, indent=3))\n",
    "\n",
    "    # Extract headers starting with 'x-' and load them into a dictionary\n",
    "    x_headers = {k: v for k, v in response.headers.items() if k.lower().startswith('x-')}\n",
    "\n",
    "    # Convert time values into seconds\n",
    "    time_multipliers = {'h': 3600, 'm': 60, 's': 1, 'ms': 0.001}\n",
    "    rate_headers = ['x-ratelimit-limit-requests', 'x-ratelimit-limit-tokens',\n",
    "                    'x-ratelimit-remaining-requests', 'x-ratelimit-remaining-tokens',\n",
    "                    'x-ratelimit-reset-requests', 'x-ratelimit-reset-tokens']\n",
    "    for key in rate_headers:\n",
    "        if key in x_headers:\n",
    "            if 'reset' in key:\n",
    "                total_time = 0\n",
    "                for time_part in re.findall(r'(\\d+)([hms]+)', x_headers[key]):\n",
    "                    total_time += int(time_part[0]) * time_multipliers[time_part[1]]\n",
    "                x_headers[key] = total_time\n",
    "            else:\n",
    "                x_headers[key] = int(x_headers[key])\n",
    "\n",
    "    # Print the headers\n",
    "    print(\"\\nHeaders starting with 'x-':\")\n",
    "    for key, value in x_headers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51a7b3-5aae-4021-8a5e-55e213d95757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured response from openai using pydantic and openai api\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "\n",
    "print(json.dumps(json.loads(event.json()), indent=2))\n",
    "\n",
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95dad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using langchain and with_structured_output\n",
    "formatted_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "system_prompt = f\"\"\"You are a precise calendar event extractor. Your task is to extract event details from natural language and format them as structured data.\n",
    "\n",
    "TASK REQUIREMENTS:\n",
    "1. Extract exactly three pieces of information:\n",
    "   - Event name (be specific and descriptive)\n",
    "   - Event date (convert relative dates to YYYY-MM-DD format)\n",
    "   - List of all participants mentioned\n",
    "\n",
    "RULES:\n",
    "- Convert relative dates using today's date ({formatted_date}) as reference\n",
    "- Include a descriptive event name even if only implied\n",
    "- List ALL participants mentioned, even in passing\n",
    "- Never include participants who aren't explicitly mentioned\n",
    "- If any required information is missing, make reasonable assumptions based on context\n",
    "\n",
    "Example Input: \"Alice and Bob are going to a science fair on Friday\"\n",
    "\n",
    "Example Output:\n",
    "{{{{\n",
    "    \"name\": \"Science Fair\",\n",
    "    \"date\": \"2024-02-02\",\n",
    "    \"participants\": [\"Alice\", \"Bob\"]\n",
    "}}}}\n",
    "\n",
    "FORMAT INSTRUCTIONS:\n",
    "The output should be a JSON object with the following schema:\n",
    "{{{{\n",
    "    \"name\": string,       // The name or title of the event\n",
    "    \"date\": string,       // The date in YYYY-MM-DD format\n",
    "    \"participants\": [     // Array of participant names\n",
    "        string,\n",
    "        ...\n",
    "    ]\n",
    "}}}}\n",
    "\"\"\"\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str = Field(description=\"The name or title of the event\")\n",
    "    date: str = Field(description=\"The date of the event in ISO format (YYYY-MM-DD)\")\n",
    "    participants: List[str] = Field(description=\"List of people participating in the event\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=CalendarEvent)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input_text}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | openai_model.with_structured_output(CalendarEvent)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\n",
    "    \"input_text\": \"Alice and Bob are going to a science fair on Friday.\"\n",
    "})\n",
    "\n",
    "# Print the formatted result\n",
    "print(json.dumps(response.dict(), indent=2))\n",
    "\n",
    "# The response is a CalendarEvent object\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b2e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainewsbot",
   "language": "python",
   "name": "ainewsbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
