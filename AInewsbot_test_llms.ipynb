{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb072688",
   "metadata": {},
   "source": [
    "# Test LLM calling\n",
    "- test different ways of calling LLMS, native API, LangChain, sync/async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74032f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to selectively re-import as needed\n",
    "import sys\n",
    "# del sys.modules['ainb_llm']\n",
    "# del sys.modules['ainb_const']\n",
    "# del sys.modules['ainb_utilities']\n",
    "# del sys.modules['ainb_webscrape']\n",
    "# del sys.modules['AInewsbot_langgraph']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562be45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# import dotenv\n",
    "# import subprocess\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "# import operator\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate,\n",
    "                                    SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.errors import NodeInterrupt\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import bs4\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "\n",
    "import asyncio\n",
    "from asyncio import Semaphore\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "# import pyperclip\n",
    "# import shlex\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "import google.generativeai as genai\n",
    "\n",
    "import anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, TypedDict, Annotated, Any\n",
    "\n",
    "import httpx\n",
    "\n",
    "import trafilatura   # web scrape uses this to get clean news stories w/o a lot of js and boilerplate\n",
    "\n",
    "from ainb_const import (\n",
    "                        \n",
    "                        REWRITE_PROMPT, FINAL_SUMMARY_PROMPT,\n",
    "                        SCREENSHOT_DIR, SUMMARIZE_SYSTEM_PROMPT, SUMMARIZE_USER_PROMPT\n",
    "                       )\n",
    "\n",
    "from ainb_utilities import log\n",
    "\n",
    "from AInewsbot_langgraph import (newscatcher_sources, fn_initialize, fn_download_sources, fn_extract_urls,\n",
    "                                 fn_verify_download, fn_extract_newscatcher, fn_filter_urls, fn_topic_clusters,\n",
    "                                 fn_topic_analysis, fn_download_pages, fn_summarize_pages, fn_propose_cats,\n",
    "                                 fn_compose_summary, fn_rewrite_summary, fn_is_revision_complete, fn_send_mail\n",
    "                                )\n",
    "\n",
    "\n",
    "import podcastfy\n",
    "from podcastfy.client import generate_podcast, process_content\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from IPython.display import Audio, display, Markdown\n",
    "\n",
    "import pdb\n",
    "\n",
    "# need this to run async in jupyter since it already has an asyncio event loop running\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Activate global verbose logging\n",
    "set_debug(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ba13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python            3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 14:21:42) [Clang 18.1.8 ]\n",
      "LangChain         0.3.18\n",
      "OpenAI            1.63.1\n",
      "Anthropic         0.45.2\n",
      "trafilatura       2.0.0\n",
      "numpy             1.26.4\n",
      "pandas            2.2.3\n",
      "sklearn           1.6.1\n",
      "umap              0.5.7\n",
      "podcastfy         0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python            {sys.version}\")\n",
    "print(f\"LangChain         {langchain.__version__}\")\n",
    "print(f\"OpenAI            {openai.__version__}\")\n",
    "print(f\"Anthropic         {anthropic.__version__}\")\n",
    "# print(f\"smtplib           {smtplib.sys.version}\")\n",
    "print(f\"trafilatura       {trafilatura.__version__}\")\n",
    "# print(f\"bs4               {bs4.__version__}\")\n",
    "print(f\"numpy             {np.__version__}\")\n",
    "print(f\"pandas            {pd.__version__}\")\n",
    "print(f\"sklearn           {sklearn.__version__}\")\n",
    "print(f\"umap              {umap.__version__}\")\n",
    "print(f\"podcastfy         {podcastfy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ebdfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LOWCOST_MODEL, MODEL, HIGHCOST_MODEL = 'gpt-4o-mini', 'gpt-4o-2024-11-20', 'o3-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ed3b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:00,505 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ascoltami. Sei bellissima. Sei perfetta e ti amo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 39, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_7fcd609668', 'finish_reason': 'stop', 'logprobs': None}, id='run-1c2a9760-8483-46fa-9c04-b0cb530255fc-0', usage_metadata={'input_tokens': 39, 'output_tokens': 17, 'total_tokens': 56, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a basic LLM call with langchain\\\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response = openai_model.invoke([\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian\"),\n",
    "    HumanMessage(content='Listen to me. You are beautiful. You are perfect and I love you.'),\n",
    "])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa52d702-271a-4caf-96dc-e8a47700fc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 17,\n",
       "  'prompt_tokens': 39,\n",
       "  'total_tokens': 56,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_7fcd609668',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8451135-7b09-4b78-9d7e-dab8d43cdbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 39,\n",
       " 'output_tokens': 17,\n",
       " 'total_tokens': 56,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata\n",
    "# no rate limit info like tokens remaining, available in headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27c0bd0f-b529-4786-b325-2eae57835ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:03,760 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage-002\n",
      "chatgpt-4o-latest\n",
      "dall-e-2\n",
      "dall-e-3\n",
      "davinci-002\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4\n",
      "gpt-4-0125-preview\n",
      "gpt-4-0613\n",
      "gpt-4-1106-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4-turbo-preview\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-realtime-preview\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "o1\n",
      "o1-2024-12-17\n",
      "o1-mini\n",
      "o1-mini-2024-09-12\n",
      "o1-preview\n",
      "o1-preview-2024-09-12\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "omni-moderation-2024-09-26\n",
      "omni-moderation-latest\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "tts-1\n",
      "tts-1-1106\n",
      "tts-1-hd\n",
      "tts-1-hd-1106\n",
      "whisper-1\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "# Retrieve the list of available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print out the model IDs\n",
    "models = [model.id for model in models.data]\n",
    "models.sort()\n",
    "print(\"\\n\".join(models))\n",
    "# yay, we got o3-mini API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9357cdc3-c02a-499f-9ef2-c8d725173ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini Models:\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-latest\n",
      "Description: The original Gemini 1.0 Pro model. This model will be discontinued on February 15th, 2025. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro\n",
      "Description: The best model for scaling across a wide range of tasks\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-pro\n",
      "Description: The best model for scaling across a wide range of tasks\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-001\n",
      "Description: The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 Pro will be discontinued on February 15th, 2025. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-vision-latest\n",
      "Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-pro-vision\n",
      "Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-001\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-002\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-001\n",
      "Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-001-tuning\n",
      "Description: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash\n",
      "Description: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-002\n",
      "Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b\n",
      "Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-001\n",
      "Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-exp-0827\n",
      "Description: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-exp-0924\n",
      "Description: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-exp\n",
      "Description: Gemini 2.0 Flash Experimental\n",
      "Generation Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash\n",
      "Description: Gemini 2.0 Flash\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-001\n",
      "Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite-preview\n",
      "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-pro-exp\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-pro-exp-02-05\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-exp-1206\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp\n",
      "Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp-1219\n",
      "Description: Gemini 2.0 Flash Thinking Experimental\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def list_gemini_models():\n",
    "    try:\n",
    "        # Configure the library\n",
    "        genai.configure()\n",
    "\n",
    "        # List available models\n",
    "        models = genai.list_models()\n",
    "\n",
    "        print(\"Available Gemini Models:\")\n",
    "        print(\"-----------------------\")\n",
    "        for m in models:\n",
    "            if \"gemini\" in m.name.lower():\n",
    "                print(f\"Name: {m.name}\")\n",
    "                print(f\"Description: {m.description}\")\n",
    "                print(f\"Generation Methods: {m.supported_generation_methods}\")\n",
    "                print(\"-----------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "list_gemini_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82d1c0b-342e-49db-b311-f4a141e73db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei bellissima. Sei perfetto/a e ti amo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try Gemini\n",
    "# GEMINI_MODEL = \"gemini-1.5-pro\"  # or \"flash\" depending on the desired model\n",
    "GEMINI_MODEL = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "gmodel = ChatGoogleGenerativeAI(\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = gmodel.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893686e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:09,271 - httpx - INFO - HTTP Request: GET https://api.anthropic.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Anthropic Models:\n",
      "- claude-3-7-sonnet-20250219\n",
      "  Description: Claude 3.7 Sonnet\n",
      "  Created: 2025-02-19 00:00:00+00:00\n",
      "\n",
      "- claude-3-5-sonnet-20241022\n",
      "  Description: Claude 3.5 Sonnet (New)\n",
      "  Created: 2024-10-22 00:00:00+00:00\n",
      "\n",
      "- claude-3-5-haiku-20241022\n",
      "  Description: Claude 3.5 Haiku\n",
      "  Created: 2024-10-22 00:00:00+00:00\n",
      "\n",
      "- claude-3-5-sonnet-20240620\n",
      "  Description: Claude 3.5 Sonnet (Old)\n",
      "  Created: 2024-06-20 00:00:00+00:00\n",
      "\n",
      "- claude-3-haiku-20240307\n",
      "  Description: Claude 3 Haiku\n",
      "  Created: 2024-03-07 00:00:00+00:00\n",
      "\n",
      "- claude-3-opus-20240229\n",
      "  Description: Claude 3 Opus\n",
      "  Created: 2024-02-29 00:00:00+00:00\n",
      "\n",
      "- claude-3-sonnet-20240229\n",
      "  Description: Claude 3 Sonnet\n",
      "  Created: 2024-02-29 00:00:00+00:00\n",
      "\n",
      "- claude-2.1\n",
      "  Description: Claude 2.1\n",
      "  Created: 2023-11-21 00:00:00+00:00\n",
      "\n",
      "- claude-2.0\n",
      "  Description: Claude 2.0\n",
      "  Created: 2023-07-11 00:00:00+00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Anthropic client with your API key\n",
    "client = Anthropic(api_key=os.environ.get(\"CLAUDE_API_KEY\"))\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print model information\n",
    "print(\"Available Anthropic Models:\")\n",
    "for model in models.data:\n",
    "    print(f\"- {model.id}\")\n",
    "    print(f\"  Description: {model.display_name}\")\n",
    "    print(f\"  Created: {model.created_at}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ceaee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:10,706 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei bellissima. Sei perfetta e ti amo.\n"
     ]
    }
   ],
   "source": [
    "# anthropic\n",
    "\n",
    "# Initialize the ChatAnthropic model\n",
    "claude_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    anthropic_api_key=os.environ[\"CLAUDE_API_KEY\"],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Create the messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = claude_model.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79ff6dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:11,584 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ciao! Come posso aiutarti oggi?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a langchain template\n",
    "system_template = \"You are a translator. Translate the following from English into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt_template | openai_model | parser\n",
    "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6382e44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:12,929 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écoute-moi. Tu es parfaite. Tu es belle et je t'aime.\n",
      "Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:14,057 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:14,728 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:22,387 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hör mir zu. Du bist makellos. Du bist wunderschön und ich liebe dich.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:23,131 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escúchame. Eres perfecta. Eres hermosa y te amo.\n",
      "Escúchame. Eres perfecto/perfecta. Eres precioso/preciosa y te amo/quiero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:24,488 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escúchame. Eres perfecta. Eres preciosa y te amo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:25,327 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei straordinario. Sei magnifico e ti amo.\n",
      "Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:26,411 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:27,381 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figyelj rám. Elbűvölő vagy. Lélegzetelállító vagy és szeretlek.\n",
      "Hallgass rám. Elragadó vagy. Lenyűgöző vagy, és szeretlek.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:29,058 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figyelj rám. Elragadó vagy. Gyönyörű vagy és szeretlek.\n",
      "\n",
      "\n",
      "Elapsed seconds: 17.386070\n"
     ]
    }
   ],
   "source": [
    "# time multiple templates (single-threaded)\n",
    "prompt_inputs = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "system_template = 'You are a translator. Translate the following text from English into {language}. Provide only the translation, no other information:'\n",
    "user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "ochain = prompt_template | openai_model | parser\n",
    "gchain = prompt_template | gmodel | parser\n",
    "cchain = prompt_template | claude_model | parser\n",
    "\n",
    "start_time = datetime.now()\n",
    "for tpl in prompt_inputs:\n",
    "    for chain in [ochain, gchain, cchain]:\n",
    "        response = \"\"\n",
    "        #     print()\n",
    "        #     print(prompt_template.format(**tpl))\n",
    "        # stream tokens as they are generated\n",
    "        for r in chain.stream(tpl):\n",
    "            print(r, end=\"\")\n",
    "            response += r\n",
    "        print()\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe2d99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queuing French translations...\n",
      "Queuing German translations...\n",
      "Queuing Spanish translations...\n",
      "Queuing Italian translations...\n",
      "Queuing Hungarian translations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:29,598 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "2025-02-25 10:56:29,779 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:29,867 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:29,874 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:29,877 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:29,912 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:29,925 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:29,943 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:30,047 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:30,077 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:30,092 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai: Écoute-moi. Tu es parfaite. Tu es belle et je t'aime.\n",
      "\n",
      "google: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n",
      "\n",
      "claude: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n",
      "openai: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n",
      "google: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n",
      "\n",
      "claude: Hör mir zu. Du bist makellos. Du bist wunderschön und ich liebe dich.\n",
      "\n",
      "openai: Escúchame. Eres perfecto. Eres hermosa y te amo.\n",
      "\n",
      "google: Escúchame. Eres perfecto/a. Eres precioso/a y te amo.\n",
      "\n",
      "\n",
      "claude: Escúchame. Eres perfecta. Eres preciosa y te amo.\n",
      "\n",
      "openai: Ascoltami. Sei fantastico. Sei magnifico e ti amo.\n",
      "\n",
      "google: Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "\n",
      "\n",
      "claude: Ascoltami. Sei fantastico. Sei magnifico e ti amo.\n",
      "\n",
      "openai: Figyelj rám. Elbűvölő vagy. Lélegzetelállító vagy, és szeretlek.\n",
      "\n",
      "google: Hallgass rám. Elbűvölő vagy. Lenyűgöző vagy, és szeretlek.\n",
      "\n",
      "\n",
      "claude: Figyelj rám. Elragadó vagy. Gyönyörű vagy és szeretlek.\n",
      "\n",
      "\n",
      "Elapsed seconds: 2.651842\n"
     ]
    }
   ],
   "source": [
    "# Retry decorator with exponential backoff\n",
    "# if I run google too fast I get a 429 error, need to update something in gcp probably\n",
    "\n",
    "def should_retry_exception(exception):\n",
    "    \"\"\"Determine if the exception should trigger a retry. (always retry)\"\"\"\n",
    "    print(type(exception))\n",
    "    print(exception)\n",
    "    return True\n",
    "\n",
    "\n",
    "# @retry(\n",
    "#     stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "#     wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "#     retry=retry_if_exception_type(should_retry_exception),\n",
    "#     before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    "# )\n",
    "async def process_translation(chain, inputs, name):\n",
    "    response = \"\"\n",
    "    async for chunk in chain.astream(inputs):\n",
    "        response += chunk\n",
    "    return response, name\n",
    "\n",
    "\n",
    "async def main():\n",
    "    prompt_inputs = [\n",
    "        {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "        {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "        {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "        {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "        {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "    ]\n",
    "\n",
    "    system_template = 'You are a translator. Translate the following text into {language}. Provide only the translation, no other information:'\n",
    "    user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", user_template)\n",
    "    ])\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    ochain = prompt_template | openai_model | parser\n",
    "    gchain = prompt_template | gmodel | parser\n",
    "    cchain = prompt_template | claude_model | parser\n",
    "    chains = {'openai': ochain, 'google': gchain, 'claude': cchain}\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    tasks = []\n",
    "    for tpl in prompt_inputs:\n",
    "        print(f\"Queuing {tpl['language']} translations...\")\n",
    "        for name, chain in chains.items():\n",
    "            task = asyncio.create_task(process_translation(chain, tpl, name))\n",
    "            tasks.append(task)\n",
    "\n",
    "    try:\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        for response, name in responses:\n",
    "            print(f\"{name}: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {str(e)}\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    difference = end_time - start_time\n",
    "    total_seconds = difference.total_seconds()\n",
    "    print(f\"\\nElapsed seconds: {total_seconds:.6f}\")\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "659782d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:35,020 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "2025-02-25 10:56:35,021 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "2025-02-25 10:56:35,455 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:35,603 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:35,613 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:35,677 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:35,699 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:35,797 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:35,814 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:35,825 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:35,907 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:56:36,847 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai: Écoute-moi. Tu es parfaite. Tu es belle et je t'aime.\n",
      "google: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n",
      "claude: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "openai: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "google: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n",
      "claude: Hör mir zu. Du bist makellos. Du bist wunderschön und ich liebe dich.\n",
      "openai: Escúchame. Eres perfecto. Eres hermosa y te amo.\n",
      "google: Escúchame. Eres perfecto/perfecta. Eres precioso/preciosa y te amo/quiero.\n",
      "\n",
      "claude: Escúchame. Eres perfecta. Eres preciosa y te amo.\n",
      "openai: Ascoltami. Sei straordinario. Sei magnifico e ti amo.\n",
      "google: Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "\n",
      "claude: Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "openai: Hallgass rám. Elbűvölő vagy. Lenyűgöző vagy, és szeretlek.\n",
      "google: Hallgass rám. Elbűvölő vagy. Lenyűgöző vagy, és szeretlek.\n",
      "\n",
      "claude: Figyelj rám. Elragadó vagy. Gyönyörű vagy és szeretlek.\n",
      "\n",
      "\n",
      "Elapsed seconds: 2.571748\n"
     ]
    }
   ],
   "source": [
    "# same but use ainvoke, no stream\n",
    "# Rate limit settings\n",
    "# CALLS_PER_MINUTE = 60  # Adjust based on your quota\n",
    "# MAX_CONCURRENT = 5     # Maximum concurrent requests\n",
    "# sem = Semaphore(MAX_CONCURRENT) # semaphore for controlling concurrent requests\n",
    "\n",
    "# @sleep_and_retry\n",
    "# @limits(calls=CALLS_PER_MINUTE, period=60)\n",
    "@retry(\n",
    "    stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "    retry=retry_if_exception_type(should_retry_exception),\n",
    "    before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    ")\n",
    "async def async_langchain(chain, input_dict, name=\"\"):\n",
    "#     async with sem:\n",
    "        response = await chain.ainvoke(input_dict)\n",
    "        return response, name\n",
    "\n",
    "prompt_templates = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "\n",
    "chains = {'openai': ochain, \n",
    "          'google': gchain, \n",
    "          'claude': cchain}\n",
    "\n",
    "start_time = datetime.now()\n",
    "tasks = []\n",
    "for d in prompt_templates:\n",
    "    for name, chain in chains.items():\n",
    "        task = asyncio.create_task(async_langchain(chain, d, name))\n",
    "        tasks.append(task)\n",
    "responses = await asyncio.gather(*tasks)\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "for response, name in responses:\n",
    "    print(f\"{name}: {response}\")\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "203ddd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:56:53,330 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is one way to write a Python script that accomplishes this task:\n",
      "\n",
      "---------------------------\n",
      "\n",
      "#!/usr/bin/env python3\n",
      "import sys\n",
      "\n",
      "def parse_matrix(matrix_str):\n",
      "    # First, remove spaces and then split on '],'\n",
      "    # Remove leading and trailing square brackets if present\n",
      "    matrix_str = matrix_str.strip()\n",
      "    # Remove any outer brackets - if the string was something like '[[1,2],[3,4]]'\n",
      "    if matrix_str.startswith('[') and matrix_str.endswith(']'):\n",
      "        matrix_str = matrix_str[1:-1]\n",
      "    # Now split along '],'\n",
      "    rows = []\n",
      "    # A simple approach: split by '],'\n",
      "    parts = matrix_str.split('],')\n",
      "    for part in parts:\n",
      "        # cleanup part\n",
      "        part = part.strip('[] ')\n",
      "        if part:\n",
      "            # convert each number to an integer\n",
      "            numbers = [int(x.strip()) for x in part.split(',')]\n",
      "            rows.append(numbers)\n",
      "    return rows\n",
      "\n",
      "def transpose(matrix):\n",
      "    # Use zip(*matrix) to produce the transpose.\n",
      "    return list(map(list, zip(*matrix)))\n",
      "\n",
      "def format_matrix(matrix):\n",
      "    # Format matrix as '[a,b],[c,d],...'\n",
      "    formatted_rows = []\n",
      "    for row in matrix:\n",
      "        row_str = ','.join(str(x) for x in row)\n",
      "        formatted_rows.append(f'[{row_str}]')\n",
      "    return ','.join(formatted_rows)\n",
      "\n",
      "def main():\n",
      "    # For demonstration, let the input string be given as an argument.\n",
      "    if len(sys.argv) != 2:\n",
      "        print(\"Usage: {} '<matrix_string>'\".format(sys.argv[0]))\n",
      "        print(\"Example: {} '[1,2],[3,4],[5,6]'\".format(sys.argv[0]))\n",
      "        sys.exit(1)\n",
      "        \n",
      "    input_str = sys.argv[1]\n",
      "    try:\n",
      "        matrix = parse_matrix(input_str)\n",
      "    except Exception as e:\n",
      "        print(\"Error parsing matrix:\", e)\n",
      "        sys.exit(1)\n",
      "    \n",
      "    # Check if the matrix is well-formed (all rows having equal numbers of columns)\n",
      "    if not matrix:\n",
      "        print(\"Error: The matrix is empty.\")\n",
      "        sys.exit(1)\n",
      "    num_cols = len(matrix[0])\n",
      "    for row in matrix:\n",
      "        if len(row) != num_cols:\n",
      "            print(\"Error: All rows must have the same number of elements.\")\n",
      "            sys.exit(1)\n",
      "    \n",
      "    transposed = transpose(matrix)\n",
      "    output_str = format_matrix(transposed)\n",
      "    print(output_str)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "---------------------------\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. The script takes a single command line argument: a string representing the matrix in the format \"[1,2],[3,4],[5,6]\".  \n",
      "2. The parse_matrix() function splits this string into rows, removes the extra brackets, and converts each element to an integer.\n",
      "3. The transpose() function uses Python’s built-in zip function to transpose the matrix.\n",
      "4. The format_matrix() function converts the transposed matrix back into the same string format.\n",
      "5. The main() function parses the input, verifies that the matrix is well-formed, computes the transpose, prints the output, and handles possible errors.\n",
      "\n",
      "You can run the script like so:  \n",
      "$ python3 script.py \"[1,2],[3,4],[5,6]\"\n",
      "\n",
      "Expected output:  \n",
      "[1,3,5],[2,4,6]\n"
     ]
    }
   ],
   "source": [
    "# test o3-mini\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    reasoning_effort = \"low\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will act as an expert Python developer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c736089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:57:13,712 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is one solution that reads the matrix from a string, computes its transpose, and prints the result in the same format:\n",
      "\n",
      "------------------------------------------------------------\n",
      "#!/usr/bin/env python3\n",
      "import sys\n",
      "\n",
      "def parse_matrix(matrix_str):\n",
      "    \"\"\"\n",
      "    Given a string like \"[1,2],[3,4],[5,6]\",\n",
      "    convert it to a list of lists [[1, 2], [3, 4], [5, 6]].\n",
      "    \"\"\"\n",
      "    # Surround the string with brackets to create a valid literal\n",
      "    try:\n",
      "        # Use eval safely by controlling globals and locals\n",
      "        matrix = eval(f'[{matrix_str}]', {\"__builtins__\":None}, {})\n",
      "    except Exception as e:\n",
      "        print(\"Error parsing matrix:\", e)\n",
      "        sys.exit(1)\n",
      "    return matrix\n",
      "\n",
      "def transpose(matrix):\n",
      "    \"\"\"\n",
      "    Transpose the given matrix.\n",
      "    \"\"\"\n",
      "    if not matrix:\n",
      "        return []\n",
      "    # Use zip(*matrix) to extract the columns as rows\n",
      "    return [list(row) for row in zip(*matrix)]\n",
      "\n",
      "def format_matrix(matrix):\n",
      "    \"\"\"\n",
      "    Format the matrix as a string in the form \"[a,b],[c,d],...\"\n",
      "    \"\"\"\n",
      "    row_strings = []\n",
      "    for row in matrix:\n",
      "        # Convert each element to string and join with commas\n",
      "        row_str = \"[\" + \",\".join(map(str, row)) + \"]\"\n",
      "        row_strings.append(row_str)\n",
      "    return \",\".join(row_strings)\n",
      "\n",
      "def main():\n",
      "    # For demonstration, you can use command line argument or input.\n",
      "    # For example, if a matrix is passed as the first command-line argument:\n",
      "    if len(sys.argv) > 1:\n",
      "        matrix_str = sys.argv[1]\n",
      "    else:\n",
      "        matrix_str = input(\"Enter matrix (e.g. '[1,2],[3,4],[5,6]'): \")\n",
      "\n",
      "    # Parse, compute transpose, and format the result.\n",
      "    matrix = parse_matrix(matrix_str)\n",
      "    transposed = transpose(matrix)\n",
      "    output = format_matrix(transposed)\n",
      "    print(output)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "------------------------------------------------------------\n",
      "\n",
      "How it works:\n",
      "1. The function parse_matrix() adds outer square brackets around your input string so that it becomes a valid list literal (e.g. turning \"[1,2],[3,4],[5,6]\" into \"[[1,2],[3,4],[5,6]]\"). It then uses eval in a restricted context.\n",
      "2. The transpose() function uses Python’s built‑in zip(*) function.\n",
      "3. The format_matrix() function converts the list of lists back into the given string format.\n",
      "4. The script reads the matrix either from the command-line arguments or from standard input, computes its transpose, and prints it.\n",
      "\n",
      "For the input \"[1,2],[3,4],[5,6]\", the output will be: \n",
      "[1,3,5],[2,4,6]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You will act as an expert Python developer.\"),\n",
    "     (\"user\", \"{input}\")]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "openai_model = ChatOpenAI(model=\"o3-mini\", reasoning_effort=\"low\")\n",
    "ochain = prompt_template | openai_model | parser\n",
    "response = ochain.invoke(\"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "085e0db6-a2f1-4faa-b0e0-ebdf5a638e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:57:14,661 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"id\": \"chatcmpl-B4razvoi1wanjloNgST3PfijerAsr\",\n",
      "   \"object\": \"chat.completion\",\n",
      "   \"created\": 1740499033,\n",
      "   \"model\": \"gpt-4o-2024-08-06\",\n",
      "   \"choices\": [\n",
      "      {\n",
      "         \"index\": 0,\n",
      "         \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"The question about the airspeed velocity of an unladen swallow is a humorous reference from the movie \\\"Monty Python and the\",\n",
      "            \"refusal\": null\n",
      "         },\n",
      "         \"logprobs\": null,\n",
      "         \"finish_reason\": \"length\"\n",
      "      }\n",
      "   ],\n",
      "   \"usage\": {\n",
      "      \"prompt_tokens\": 18,\n",
      "      \"completion_tokens\": 25,\n",
      "      \"total_tokens\": 43,\n",
      "      \"prompt_tokens_details\": {\n",
      "         \"cached_tokens\": 0,\n",
      "         \"audio_tokens\": 0\n",
      "      },\n",
      "      \"completion_tokens_details\": {\n",
      "         \"reasoning_tokens\": 0,\n",
      "         \"audio_tokens\": 0,\n",
      "         \"accepted_prediction_tokens\": 0,\n",
      "         \"rejected_prediction_tokens\": 0\n",
      "      }\n",
      "   },\n",
      "   \"service_tier\": \"default\",\n",
      "   \"system_fingerprint\": \"fp_eb9dce56a8\"\n",
      "}\n",
      "\n",
      "Headers starting with 'x-':\n",
      "x-ratelimit-limit-requests: 10000\n",
      "x-ratelimit-limit-tokens: 30000000\n",
      "x-ratelimit-remaining-requests: 9999\n",
      "x-ratelimit-remaining-tokens: 29999961\n",
      "x-ratelimit-reset-requests: 0.006\n",
      "x-ratelimit-reset-tokens: 0\n",
      "x-request-id: req_ea4023d4bf801f132c35ae09596ef840\n",
      "x-content-type-options: nosniff\n"
     ]
    }
   ],
   "source": [
    "# could use the metadata to saturate the OpenAI API and use as many tokens per second as available\n",
    "# but not supported by langchain across multiple models so exponential backoff seems to be the best alternative\n",
    "\n",
    "apikey = os.environ.get(\"OPENAI_API_KEY\")\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"OpenAI-Beta\": \"assistants=v2\",\n",
    "    \"Authorization\": f\"Bearer {apikey}\"\n",
    "}\n",
    "body = {\n",
    "    \"model\": \"gpt-4o-2024-08-06\", \"max_tokens\": 25, \"top_p\": 0.8,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is the airspeed velocity of an unladen swallow\"}]\n",
    "        }\n",
    "    ]}\n",
    "\n",
    "try:\n",
    "    response = httpx.post(url, headers=headers, json=body)\n",
    "    response_json = response.json()\n",
    "    print(json.dumps(response_json, indent=3))\n",
    "\n",
    "    # Extract headers starting with 'x-' and load them into a dictionary\n",
    "    x_headers = {k: v for k, v in response.headers.items() if k.lower().startswith('x-')}\n",
    "\n",
    "    # Convert time values into seconds\n",
    "    time_multipliers = {'h': 3600, 'm': 60, 's': 1, 'ms': 0.001}\n",
    "    rate_headers = ['x-ratelimit-limit-requests', 'x-ratelimit-limit-tokens',\n",
    "                    'x-ratelimit-remaining-requests', 'x-ratelimit-remaining-tokens',\n",
    "                    'x-ratelimit-reset-requests', 'x-ratelimit-reset-tokens']\n",
    "    for key in rate_headers:\n",
    "        if key in x_headers:\n",
    "            if 'reset' in key:\n",
    "                total_time = 0\n",
    "                for time_part in re.findall(r'(\\d+)([hms]+)', x_headers[key]):\n",
    "                    total_time += int(time_part[0]) * time_multipliers[time_part[1]]\n",
    "                x_headers[key] = total_time\n",
    "            else:\n",
    "                x_headers[key] = int(x_headers[key])\n",
    "\n",
    "    # Print the headers\n",
    "    print(\"\\nHeaders starting with 'x-':\")\n",
    "    for key, value in x_headers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f51a7b3-5aae-4021-8a5e-55e213d95757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:57:17,871 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Science Fair\",\n",
      "  \"date\": \"Friday\",\n",
      "  \"participants\": [\n",
      "    \"Alice\",\n",
      "    \"Bob\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structured response from openai using pydantic and openai api\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "\n",
    "print(json.dumps(json.loads(event.json()), indent=2))\n",
    "\n",
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d95dad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:57:23,245 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Science Fair\",\n",
      "  \"date\": \"2025-02-28\",\n",
      "  \"participants\": [\n",
      "    \"Alice\",\n",
      "    \"Bob\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='2025-02-28', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using langchain and with_structured_output\n",
    "formatted_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "system_prompt = f\"\"\"You are a precise calendar event extractor. Your task is to extract event details from natural language and format them as structured data.\n",
    "\n",
    "TASK REQUIREMENTS:\n",
    "1. Extract exactly three pieces of information:\n",
    "   - Event name (be specific and descriptive)\n",
    "   - Event date (convert relative dates to YYYY-MM-DD format)\n",
    "   - List of all participants mentioned\n",
    "\n",
    "RULES:\n",
    "- Convert relative dates using today's date ({formatted_date}) as reference\n",
    "- Include a descriptive event name even if only implied\n",
    "- List ALL participants mentioned, even in passing\n",
    "- Never include participants who aren't explicitly mentioned\n",
    "- If any required information is missing, make reasonable assumptions based on context\n",
    "\n",
    "Example Input: \"Alice and Bob are going to a science fair on Friday\"\n",
    "\n",
    "Example Output:\n",
    "{{{{\n",
    "    \"name\": \"Science Fair\",\n",
    "    \"date\": \"2024-02-02\",\n",
    "    \"participants\": [\"Alice\", \"Bob\"]\n",
    "}}}}\n",
    "\n",
    "FORMAT INSTRUCTIONS:\n",
    "The output should be a JSON object with the following schema:\n",
    "{{{{\n",
    "    \"name\": string,       // The name or title of the event\n",
    "    \"date\": string,       // The date in YYYY-MM-DD format\n",
    "    \"participants\": [     // Array of participant names\n",
    "        string,\n",
    "        ...\n",
    "    ]\n",
    "}}}}\n",
    "\"\"\"\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str = Field(description=\"The name or title of the event\")\n",
    "    date: str = Field(description=\"The date of the event in ISO format (YYYY-MM-DD)\")\n",
    "    participants: List[str] = Field(description=\"List of people participating in the event\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=CalendarEvent)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input_text}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | openai_model.with_structured_output(CalendarEvent)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\n",
    "    \"input_text\": \"Alice and Bob are going to a science fair on Friday.\"\n",
    "})\n",
    "\n",
    "# Print the formatted result\n",
    "print(json.dumps(response.dict(), indent=2))\n",
    "\n",
    "# The response is a CalendarEvent object\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b2e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainewsbot",
   "language": "python",
   "name": "ainewsbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
