{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb072688",
   "metadata": {},
   "source": [
    "# Test LLM calling\n",
    "- test different ways of calling LLMS, native API, LangChain, sync/async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74032f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to selectively re-import as needed\n",
    "import sys\n",
    "# del sys.modules['ainb_llm']\n",
    "# del sys.modules['ainb_const']\n",
    "# del sys.modules['ainb_utilities']\n",
    "# del sys.modules['ainb_webscrape']\n",
    "# del sys.modules['AInewsbot_langgraph']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562be45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# import dotenv\n",
    "# import subprocess\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "# import operator\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate,\n",
    "                                    SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.errors import NodeInterrupt\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import bs4\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "\n",
    "import asyncio\n",
    "from asyncio import Semaphore\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "# import pyperclip\n",
    "# import shlex\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, TypedDict, Annotated, Any\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "import google.generativeai as genai\n",
    "import httpx\n",
    "\n",
    "import trafilatura   # web scrape uses this to get clean news stories w/o a lot of js and boilerplate\n",
    "\n",
    "from ainb_const import (\n",
    "                        MODEL, LOWCOST_MODEL, HIGHCOST_MODEL, FINAL_SUMMARY_PROMPT,\n",
    "                        REWRITE_PROMPT,\n",
    "                        SCREENSHOT_DIR, SUMMARIZE_SYSTEM_PROMPT, SUMMARIZE_USER_PROMPT\n",
    "                       )\n",
    "\n",
    "from ainb_utilities import log\n",
    "\n",
    "from AInewsbot_langgraph import (newscatcher_sources, fn_initialize, fn_download_sources, fn_extract_urls,\n",
    "                                 fn_verify_download, fn_extract_newscatcher, fn_filter_urls, fn_topic_clusters,\n",
    "                                 fn_topic_analysis, fn_download_pages, fn_summarize_pages, fn_propose_cats,\n",
    "                                 fn_compose_summary, fn_rewrite_summary, fn_is_revision_complete, fn_send_mail\n",
    "                                )\n",
    "\n",
    "\n",
    "import podcastfy\n",
    "from podcastfy.client import generate_podcast, process_content\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from IPython.display import Audio, display, Markdown\n",
    "\n",
    "import pdb\n",
    "\n",
    "# need this to run async in jupyter since it already has an asyncio event loop running\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Activate global verbose logging\n",
    "set_debug(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ba13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python            3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 14:21:42) [Clang 18.1.8 ]\n",
      "LangChain         0.3.18\n",
      "OpenAI            1.63.1\n",
      "trafilatura       2.0.0\n",
      "numpy             1.26.4\n",
      "pandas            2.2.3\n",
      "sklearn           1.6.1\n",
      "umap              0.5.7\n",
      "podcastfy         0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python            {sys.version}\")\n",
    "print(f\"LangChain         {langchain.__version__}\")\n",
    "print(f\"OpenAI            {openai.__version__}\")\n",
    "# print(f\"smtplib           {smtplib.sys.version}\")\n",
    "print(f\"trafilatura       {trafilatura.__version__}\")\n",
    "# print(f\"bs4               {bs4.__version__}\")\n",
    "print(f\"numpy             {np.__version__}\")\n",
    "print(f\"pandas            {pd.__version__}\")\n",
    "print(f\"sklearn           {sklearn.__version__}\")\n",
    "print(f\"umap              {umap.__version__}\")\n",
    "print(f\"podcastfy         {podcastfy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ed3b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:24:59,365 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ascoltami. Sei bellissima. Sei perfetta e ti amo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 39, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_7fcd609668', 'finish_reason': 'stop', 'logprobs': None}, id='run-3000ffc1-a993-4378-bfe1-a47f5ad4f55b-0', usage_metadata={'input_tokens': 39, 'output_tokens': 17, 'total_tokens': 56, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a basic LLM call with langchain\\\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response = openai_model.invoke([\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian\"),\n",
    "    HumanMessage(content='Listen to me. You are beautiful. You are perfect and I love you.'),\n",
    "])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa52d702-271a-4caf-96dc-e8a47700fc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 17,\n",
       "  'prompt_tokens': 39,\n",
       "  'total_tokens': 56,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_7fcd609668',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8451135-7b09-4b78-9d7e-dab8d43cdbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 39,\n",
       " 'output_tokens': 17,\n",
       " 'total_tokens': 56,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata\n",
    "# no rate limit info like tokens remaining, available in headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c0bd0f-b529-4786-b325-2eae57835ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:00,120 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage-002\n",
      "chatgpt-4o-latest\n",
      "dall-e-2\n",
      "dall-e-3\n",
      "davinci-002\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4\n",
      "gpt-4-0125-preview\n",
      "gpt-4-0613\n",
      "gpt-4-1106-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4-turbo-preview\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-realtime-preview\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "o1\n",
      "o1-2024-12-17\n",
      "o1-mini\n",
      "o1-mini-2024-09-12\n",
      "o1-preview\n",
      "o1-preview-2024-09-12\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "omni-moderation-2024-09-26\n",
      "omni-moderation-latest\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "tts-1\n",
      "tts-1-1106\n",
      "tts-1-hd\n",
      "tts-1-hd-1106\n",
      "whisper-1\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "# Retrieve the list of available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print out the model IDs\n",
    "models = [model.id for model in models.data]\n",
    "models.sort()\n",
    "print(\"\\n\".join(models))\n",
    "# yay, we got o3-mini API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9357cdc3-c02a-499f-9ef2-c8d725173ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini Models:\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-latest\n",
      "Description: The original Gemini 1.0 Pro model. This model will be discontinued on February 15th, 2025. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro\n",
      "Description: The best model for scaling across a wide range of tasks\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-pro\n",
      "Description: The best model for scaling across a wide range of tasks\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-001\n",
      "Description: The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 Pro will be discontinued on February 15th, 2025. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-vision-latest\n",
      "Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-pro-vision\n",
      "Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-001\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-002\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-001\n",
      "Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-001-tuning\n",
      "Description: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash\n",
      "Description: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-002\n",
      "Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b\n",
      "Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-001\n",
      "Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-exp-0827\n",
      "Description: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-exp-0924\n",
      "Description: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-exp\n",
      "Description: Gemini 2.0 Flash Experimental\n",
      "Generation Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash\n",
      "Description: Gemini 2.0 Flash\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-001\n",
      "Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite-preview\n",
      "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-pro-exp\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-pro-exp-02-05\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-exp-1206\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp\n",
      "Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp-1219\n",
      "Description: Gemini 2.0 Flash Thinking Experimental\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def list_gemini_models():\n",
    "    try:\n",
    "        # Configure the library\n",
    "        genai.configure()\n",
    "\n",
    "        # List available models\n",
    "        models = genai.list_models()\n",
    "\n",
    "        print(\"Available Gemini Models:\")\n",
    "        print(\"-----------------------\")\n",
    "        for m in models:\n",
    "            if \"gemini\" in m.name.lower():\n",
    "                print(f\"Name: {m.name}\")\n",
    "                print(f\"Description: {m.description}\")\n",
    "                print(f\"Generation Methods: {m.supported_generation_methods}\")\n",
    "                print(\"-----------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "list_gemini_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a82d1c0b-342e-49db-b311-f4a141e73db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei bellissima. Sei perfetto/a e ti amo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try Gemini\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "# GEMINI_MODEL = \"gemini-1.5-pro\"  # or \"flash\" depending on the desired model\n",
    "GEMINI_MODEL = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "gmodel = ChatGoogleGenerativeAI(\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = gmodel.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ceaee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:01,418 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei bellissima. Sei perfetta e ti amo.\n"
     ]
    }
   ],
   "source": [
    "# anthropic\n",
    "\n",
    "# Initialize the ChatAnthropic model\n",
    "claude_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    anthropic_api_key=os.environ[\"CLAUDE_API_KEY\"],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Create the messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = claude_model.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79ff6dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:01,920 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ciao! Come posso aiutarti oggi?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a langchain template\n",
    "system_template = \"You are a translator. Translate the following from English into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt_template | openai_model | parser\n",
    "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6382e44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:02,332 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écoute-moi. Tu es parfaite. Tu es belle et je t'aime.\n",
      "Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:03,664 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:04,211 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:05,266 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hör mir zu. Du bist makellos. Du bist wunderschön und ich liebe dich.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:06,019 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escúchame. Eres perfecto. Eres hermosa y te amo.\n",
      "Escúchame. Eres perfecto/perfecta. Eres precioso/preciosa y te amo/quiero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:07,048 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escúchame. Eres perfecta. Eres preciosa y te amo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:07,759 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei incredibile. Sei magnifico e ti amo.\n",
      "Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:08,578 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:09,293 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallgass rám. Elbűvölő vagy. Pompás vagy, és szeretlek.\n",
      "Hallgass rám. Elbűvölő vagy. Lenyűgöző vagy, és szeretlek.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:10,507 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figyelj rám. Elragadó vagy. Gyönyörű vagy és szeretlek.\n",
      "\n",
      "\n",
      "Elapsed seconds: 8.862191\n"
     ]
    }
   ],
   "source": [
    "# time multiple templates (single-threaded)\n",
    "prompt_inputs = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "system_template = 'You are a translator. Translate the following text from English into {language}. Provide only the translation, no other information:'\n",
    "user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "ochain = prompt_template | openai_model | parser\n",
    "gchain = prompt_template | gmodel | parser\n",
    "cchain = prompt_template | claude_model | parser\n",
    "\n",
    "start_time = datetime.now()\n",
    "for tpl in prompt_inputs:\n",
    "    for chain in [ochain, gchain, cchain]:\n",
    "        response = \"\"\n",
    "        #     print()\n",
    "        #     print(prompt_template.format(**tpl))\n",
    "        # stream tokens as they are generated\n",
    "        for r in chain.stream(tpl):\n",
    "            print(r, end=\"\")\n",
    "            response += r\n",
    "        print()\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe2d99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queuing French translations...\n",
      "Queuing German translations...\n",
      "Queuing Spanish translations...\n",
      "Queuing Italian translations...\n",
      "Queuing Hungarian translations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:11,147 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:11,182 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:11,200 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:11,239 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:11,271 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:11,285 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:11,312 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:11,364 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:11,763 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:15,134 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai: Écoute-moi. Tu es parfaite. Tu es belle et je t'aime.\n",
      "\n",
      "google: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n",
      "\n",
      "claude: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n",
      "openai: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n",
      "google: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n",
      "\n",
      "claude: Hör mir zu. Du bist makellos. Du bist wunderschön und ich liebe dich.\n",
      "\n",
      "openai: Escúchame. Eres perfecto. Eres hermosa y te amo.\n",
      "\n",
      "google: Escúchame. Eres perfecto/a. Eres precioso/a y te amo.\n",
      "\n",
      "\n",
      "claude: Escúchame. Eres perfecta. Eres preciosa y te amo.\n",
      "\n",
      "openai: Ascoltami. Sei straordinario. Sei magnifico e ti amo.\n",
      "\n",
      "google: Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "\n",
      "\n",
      "claude: Ascoltami. Sei fantastico. Sei magnifico e ti amo.\n",
      "\n",
      "openai: Figyelj rám. Elbűvölő vagy. Lenygőző vagy, és szeretlek.\n",
      "\n",
      "google: Hallgass rám. Elbűvölő vagy. Lenyűgöző vagy, és szeretlek.\n",
      "\n",
      "\n",
      "claude: Figyelj rám. Elragadó vagy. Gyönyörű vagy és szeretlek.\n",
      "\n",
      "\n",
      "Elapsed seconds: 4.552198\n"
     ]
    }
   ],
   "source": [
    "# Retry decorator with exponential backoff\n",
    "# if I run google too fast I get a 429 error, need to update something in gcp probably\n",
    "\n",
    "def should_retry_exception(exception):\n",
    "    \"\"\"Determine if the exception should trigger a retry. (always retry)\"\"\"\n",
    "    print(type(exception))\n",
    "    print(exception)\n",
    "    return True\n",
    "\n",
    "\n",
    "# @retry(\n",
    "#     stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "#     wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "#     retry=retry_if_exception_type(should_retry_exception),\n",
    "#     before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    "# )\n",
    "async def process_translation(chain, inputs, name):\n",
    "    response = \"\"\n",
    "    async for chunk in chain.astream(inputs):\n",
    "        response += chunk\n",
    "    return response, name\n",
    "\n",
    "\n",
    "async def main():\n",
    "    prompt_inputs = [\n",
    "        {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "        {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "        {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "        {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "        {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "    ]\n",
    "\n",
    "    system_template = 'You are a translator. Translate the following text into {language}. Provide only the translation, no other information:'\n",
    "    user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", user_template)\n",
    "    ])\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    ochain = prompt_template | openai_model | parser\n",
    "    gchain = prompt_template | gmodel | parser\n",
    "    cchain = prompt_template | claude_model | parser\n",
    "    chains = {'openai': ochain, 'google': gchain, 'claude': cchain}\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    tasks = []\n",
    "    for tpl in prompt_inputs:\n",
    "        print(f\"Queuing {tpl['language']} translations...\")\n",
    "        for name, chain in chains.items():\n",
    "            task = asyncio.create_task(process_translation(chain, tpl, name))\n",
    "            tasks.append(task)\n",
    "\n",
    "    try:\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        for response, name in responses:\n",
    "            print(f\"{name}: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {str(e)}\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    difference = end_time - start_time\n",
    "    total_seconds = difference.total_seconds()\n",
    "    print(f\"\\nElapsed seconds: {total_seconds:.6f}\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "659782d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:25:15,505 - langchain_google_genai.chat_models - WARNING - Retrying langchain_google_genai.chat_models._achat_with_retry.<locals>._achat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
      "2025-02-23 16:25:15,941 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:15,971 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:15,982 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:15,984 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:15,995 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:16,030 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:16,064 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:16,106 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:16,257 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-23 16:25:16,668 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "isinstance() arg 2 must be a type, a tuple of types, or a union",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m         task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(async_langchain(chain, d, name))\n\u001b[1;32m     37\u001b[0m         tasks\u001b[38;5;241m.\u001b[39mappend(task)\n\u001b[0;32m---> 38\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m     39\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     41\u001b[0m difference \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/asyncio/tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:189\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    188\u001b[0m async_wrapped\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:111\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:153\u001b[0m, in \u001b[0;36mAsyncRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/tenacity/asyncio/__init__.py:129\u001b[0m, in \u001b[0;36mAsyncRetrying._run_retry\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_retry\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _utils\u001b[38;5;241m.\u001b[39mwrap_to_async_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry)(\n\u001b[1;32m    130\u001b[0m         retry_state\n\u001b[1;32m    131\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/tenacity/_utils.py:99\u001b[0m, in \u001b[0;36mwrap_to_async_func.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/tenacity/retry.py:82\u001b[0m, in \u001b[0;36mretry_if_exception.__call__\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutcome failed but the exception is None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/tenacity/retry.py:98\u001b[0m, in \u001b[0;36mretry_if_exception_type.__init__.<locals>.<lambda>\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     92\u001b[0m     exception_types: typing\u001b[38;5;241m.\u001b[39mUnion[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mException\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception_types \u001b[38;5;241m=\u001b[39m exception_types\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m e: \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_types\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: isinstance() arg 2 must be a type, a tuple of types, or a union"
     ]
    }
   ],
   "source": [
    "# same but use ainvoke, no stream\n",
    "# Rate limit settings\n",
    "# CALLS_PER_MINUTE = 60  # Adjust based on your quota\n",
    "# MAX_CONCURRENT = 5     # Maximum concurrent requests\n",
    "# sem = Semaphore(MAX_CONCURRENT) # semaphore for controlling concurrent requests\n",
    "\n",
    "# @sleep_and_retry\n",
    "# @limits(calls=CALLS_PER_MINUTE, period=60)\n",
    "@retry(\n",
    "    stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "    retry=retry_if_exception_type(should_retry_exception),\n",
    "    before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    ")\n",
    "async def async_langchain(chain, input_dict, name=\"\"):\n",
    "#     async with sem:\n",
    "        response = await chain.ainvoke(input_dict)\n",
    "        return response, name\n",
    "\n",
    "prompt_templates = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "\n",
    "chains = {'openai': ochain, \n",
    "          'google': gchain, \n",
    "          'claude': cchain}\n",
    "\n",
    "start_time = datetime.now()\n",
    "tasks = []\n",
    "for d in prompt_templates:\n",
    "    for name, chain in chains.items():\n",
    "        task = asyncio.create_task(async_langchain(chain, d, name))\n",
    "        tasks.append(task)\n",
    "responses = await asyncio.gather(*tasks)\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "for response, name in responses:\n",
    "    print(f\"{name}: {response}\")\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ddd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test o3-mini\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    reasoning_effort = \"low\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will act as an expert Python developer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You will act as an expert Python developer.\"),\n",
    "     (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "openai_model = ChatOpenAI(model=\"o3-mini\", reasoning_effort=\"low\")\n",
    "\n",
    "ochain = prompt_template | openai_model | parser\n",
    "\n",
    "response = ochain.invoke(\"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e0db6-a2f1-4faa-b0e0-ebdf5a638e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use the metadata to saturate the OpenAI API and use as many tokens per second as available\n",
    "# but not supported by langchain across multiple models so exponential backoff seems to be the best alternative\n",
    "\n",
    "apikey = os.environ.get(\"OPENAI_API_KEY\")\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"OpenAI-Beta\": \"assistants=v2\",\n",
    "    \"Authorization\": f\"Bearer {apikey}\"\n",
    "}\n",
    "body = {\n",
    "    \"model\": \"gpt-4o-2024-08-06\", \"max_tokens\": 25, \"top_p\": 0.8,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is the airspeed velocity of an unladen swallow\"}]\n",
    "        }\n",
    "    ]}\n",
    "\n",
    "try:\n",
    "    response = httpx.post(url, headers=headers, json=body)\n",
    "    response_json = response.json()\n",
    "    print(json.dumps(response_json, indent=3))\n",
    "\n",
    "    # Extract headers starting with 'x-' and load them into a dictionary\n",
    "    x_headers = {k: v for k, v in response.headers.items() if k.lower().startswith('x-')}\n",
    "\n",
    "    # Convert time values into seconds\n",
    "    time_multipliers = {'h': 3600, 'm': 60, 's': 1, 'ms': 0.001}\n",
    "    rate_headers = ['x-ratelimit-limit-requests', 'x-ratelimit-limit-tokens',\n",
    "                    'x-ratelimit-remaining-requests', 'x-ratelimit-remaining-tokens',\n",
    "                    'x-ratelimit-reset-requests', 'x-ratelimit-reset-tokens']\n",
    "    for key in rate_headers:\n",
    "        if key in x_headers:\n",
    "            if 'reset' in key:\n",
    "                total_time = 0\n",
    "                for time_part in re.findall(r'(\\d+)([hms]+)', x_headers[key]):\n",
    "                    total_time += int(time_part[0]) * time_multipliers[time_part[1]]\n",
    "                x_headers[key] = total_time\n",
    "            else:\n",
    "                x_headers[key] = int(x_headers[key])\n",
    "\n",
    "    # Print the headers\n",
    "    print(\"\\nHeaders starting with 'x-':\")\n",
    "    for key, value in x_headers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51a7b3-5aae-4021-8a5e-55e213d95757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured response from openai using pydantic and openai api\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "\n",
    "print(json.dumps(json.loads(event.json()), indent=2))\n",
    "\n",
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95dad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using langchain and with_structured_output\n",
    "formatted_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "system_prompt = f\"\"\"You are a precise calendar event extractor. Your task is to extract event details from natural language and format them as structured data.\n",
    "\n",
    "TASK REQUIREMENTS:\n",
    "1. Extract exactly three pieces of information:\n",
    "   - Event name (be specific and descriptive)\n",
    "   - Event date (convert relative dates to YYYY-MM-DD format)\n",
    "   - List of all participants mentioned\n",
    "\n",
    "RULES:\n",
    "- Convert relative dates using today's date ({formatted_date}) as reference\n",
    "- Include a descriptive event name even if only implied\n",
    "- List ALL participants mentioned, even in passing\n",
    "- Never include participants who aren't explicitly mentioned\n",
    "- If any required information is missing, make reasonable assumptions based on context\n",
    "\n",
    "Example Input: \"Alice and Bob are going to a science fair on Friday\"\n",
    "\n",
    "Example Output:\n",
    "{{{{\n",
    "    \"name\": \"Science Fair\",\n",
    "    \"date\": \"2024-02-02\",\n",
    "    \"participants\": [\"Alice\", \"Bob\"]\n",
    "}}}}\n",
    "\n",
    "FORMAT INSTRUCTIONS:\n",
    "The output should be a JSON object with the following schema:\n",
    "{{{{\n",
    "    \"name\": string,       // The name or title of the event\n",
    "    \"date\": string,       // The date in YYYY-MM-DD format\n",
    "    \"participants\": [     // Array of participant names\n",
    "        string,\n",
    "        ...\n",
    "    ]\n",
    "}}}}\n",
    "\"\"\"\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str = Field(description=\"The name or title of the event\")\n",
    "    date: str = Field(description=\"The date of the event in ISO format (YYYY-MM-DD)\")\n",
    "    participants: List[str] = Field(description=\"List of people participating in the event\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=CalendarEvent)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input_text}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | openai_model.with_structured_output(CalendarEvent)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\n",
    "    \"input_text\": \"Alice and Bob are going to a science fair on Friday.\"\n",
    "})\n",
    "\n",
    "# Print the formatted result\n",
    "print(json.dumps(response.dict(), indent=2))\n",
    "\n",
    "# The response is a CalendarEvent object\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b2e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainewsbot",
   "language": "python",
   "name": "ainewsbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
