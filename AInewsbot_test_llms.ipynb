{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb072688",
   "metadata": {},
   "source": [
    "# Test LLM calling\n",
    "- test different ways of calling LLMS, native API, LangChain, sync/async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74032f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to selectively re-import as needed\n",
    "import sys\n",
    "# del sys.modules['ainb_llm']\n",
    "# del sys.modules['ainb_const']\n",
    "# del sys.modules['ainb_utilities']\n",
    "# del sys.modules['ainb_webscrape']\n",
    "# del sys.modules['AInewsbot_langgraph']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562be45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# import dotenv\n",
    "# import subprocess\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "# import operator\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate,\n",
    "                                    SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.errors import NodeInterrupt\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import bs4\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "\n",
    "import asyncio\n",
    "from asyncio import Semaphore\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "# import pyperclip\n",
    "# import shlex\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "import google.generativeai as genai\n",
    "\n",
    "import anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, TypedDict, Annotated, Any\n",
    "\n",
    "import httpx\n",
    "\n",
    "import trafilatura   # web scrape uses this to get clean news stories w/o a lot of js and boilerplate\n",
    "\n",
    "from ainb_const import (\n",
    "                        \n",
    "                        REWRITE_PROMPT, FINAL_SUMMARY_PROMPT,\n",
    "                        SCREENSHOT_DIR, SUMMARIZE_SYSTEM_PROMPT, SUMMARIZE_USER_PROMPT\n",
    "                       )\n",
    "\n",
    "from ainb_utilities import log\n",
    "\n",
    "from AInewsbot_langgraph import (newscatcher_sources, fn_initialize, fn_download_sources, fn_extract_urls,\n",
    "                                 fn_verify_download, fn_extract_newscatcher, fn_filter_urls, fn_topic_clusters,\n",
    "                                 fn_topic_analysis, fn_download_pages, fn_summarize_pages, fn_propose_cats,\n",
    "                                 fn_compose_summary, fn_rewrite_summary, fn_is_revision_complete, fn_send_mail\n",
    "                                )\n",
    "\n",
    "\n",
    "import podcastfy\n",
    "from podcastfy.client import generate_podcast, process_content\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from IPython.display import Audio, display, Markdown\n",
    "\n",
    "import pdb\n",
    "\n",
    "# need this to run async in jupyter since it already has an asyncio event loop running\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Activate global verbose logging\n",
    "set_debug(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ba13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python            3.11.11 (main, Dec 11 2024, 10:25:04) [Clang 14.0.6 ]\n",
      "LangChain         0.3.19\n",
      "OpenAI            1.64.0\n",
      "Anthropic         0.47.2\n",
      "trafilatura       2.0.0\n",
      "numpy             1.26.4\n",
      "pandas            2.2.3\n",
      "sklearn           1.6.1\n",
      "umap              0.5.7\n",
      "podcastfy         0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python            {sys.version}\")\n",
    "print(f\"LangChain         {langchain.__version__}\")\n",
    "print(f\"OpenAI            {openai.__version__}\")\n",
    "print(f\"Anthropic         {anthropic.__version__}\")\n",
    "# print(f\"smtplib           {smtplib.sys.version}\")\n",
    "print(f\"trafilatura       {trafilatura.__version__}\")\n",
    "# print(f\"bs4               {bs4.__version__}\")\n",
    "print(f\"numpy             {np.__version__}\")\n",
    "print(f\"pandas            {pd.__version__}\")\n",
    "print(f\"sklearn           {sklearn.__version__}\")\n",
    "print(f\"umap              {umap.__version__}\")\n",
    "print(f\"podcastfy         {podcastfy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ebdfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LOWCOST_MODEL, MODEL, HIGHCOST_MODEL = 'gpt-4o-mini', 'gpt-4o-2024-11-20', 'o3-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ed3b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 19:55:12,634 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Invalid URL (POST /v1/chat/completions)', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# a basic LLM call with langchain\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m openai_model \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4.5-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a translator. Translate the following from English into Italian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mListen to me. You are beautiful. You are perfect and I love you.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m response\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:860\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    854\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    859\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:690\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 690\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:925\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 925\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    929\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:800\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 800\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    876\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    877\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    878\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1278\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1287\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1288\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1289\u001b[0m     )\n\u001b[0;32m-> 1290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/_base_client.py:1071\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1070\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1074\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1075\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1080\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'Invalid URL (POST /v1/chat/completions)', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "# a basic LLM call with langchain\\\n",
    "openai_model = ChatOpenAI(model=\"gpt-4.5-preview\")\n",
    "\n",
    "response = openai_model.invoke([\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian\"),\n",
    "    HumanMessage(content='Listen to me. You are beautiful. You are perfect and I love you.'),\n",
    "])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52d702-271a-4caf-96dc-e8a47700fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8451135-7b09-4b78-9d7e-dab8d43cdbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.usage_metadata\n",
    "# no rate limit info like tokens remaining, available in headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c0bd0f-b529-4786-b325-2eae57835ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 18:12:27,408 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt-4o-latest\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4\n",
      "gpt-4-0125-preview\n",
      "gpt-4-0613\n",
      "gpt-4-1106-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4-turbo-preview\n",
      "gpt-4.5-preview\n",
      "gpt-4.5-preview-2025-02-27\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-realtime-preview\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "o1-2024-12-17\n",
      "o1-mini-2024-09-12\n",
      "o1-preview-2024-09-12\n",
      "omni-moderation-2024-09-26\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "# Retrieve the list of available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print out the model IDs\n",
    "models = [model.id for model in models.data if '4' in model.id]\n",
    "models.sort()\n",
    "print(\"\\n\".join(models))\n",
    "# yay, we got o3-mini API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9357cdc3-c02a-499f-9ef2-c8d725173ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini Models:\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-vision-latest\n",
      "Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-pro-vision\n",
      "Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-001\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-002\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-001\n",
      "Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-001-tuning\n",
      "Description: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash\n",
      "Description: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-002\n",
      "Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b\n",
      "Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-001\n",
      "Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-exp-0827\n",
      "Description: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-exp-0924\n",
      "Description: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-exp\n",
      "Description: Gemini 2.0 Flash Experimental\n",
      "Generation Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash\n",
      "Description: Gemini 2.0 Flash\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-001\n",
      "Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite-001\n",
      "Description: Stable version of Gemini 2.0 Flash Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite\n",
      "Description: Gemini 2.0 Flash-Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite-preview\n",
      "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-pro-exp\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-pro-exp-02-05\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-exp-1206\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp\n",
      "Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp-1219\n",
      "Description: Gemini 2.0 Flash Thinking Experimental\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def list_gemini_models():\n",
    "    try:\n",
    "        # Configure the library\n",
    "        genai.configure()\n",
    "\n",
    "        # List available models\n",
    "        models = genai.list_models()\n",
    "\n",
    "        print(\"Available Gemini Models:\")\n",
    "        print(\"-----------------------\")\n",
    "        for m in models:\n",
    "            if \"gemini\" in m.name.lower():\n",
    "                print(f\"Name: {m.name}\")\n",
    "                print(f\"Description: {m.description}\")\n",
    "                print(f\"Generation Methods: {m.supported_generation_methods}\")\n",
    "                print(\"-----------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "list_gemini_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d1c0b-342e-49db-b311-f4a141e73db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try Gemini\n",
    "# GEMINI_MODEL = \"gemini-1.5-pro\"  # or \"flash\" depending on the desired model\n",
    "GEMINI_MODEL = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "gmodel = ChatGoogleGenerativeAI(\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = gmodel.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893686e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Anthropic client with your API key\n",
    "client = Anthropic(api_key=os.environ.get(\"CLAUDE_API_KEY\"))\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print model information\n",
    "print(\"Available Anthropic Models:\")\n",
    "for model in models.data:\n",
    "    print(f\"- {model.id}\")\n",
    "    print(f\"  Description: {model.display_name}\")\n",
    "    print(f\"  Created: {model.created_at}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceaee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anthropic\n",
    "\n",
    "# Initialize the ChatAnthropic model\n",
    "claude_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    anthropic_api_key=os.environ[\"CLAUDE_API_KEY\"],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Create the messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = claude_model.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a langchain template\n",
    "system_template = \"You are a translator. Translate the following from English into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt_template | openai_model | parser\n",
    "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382e44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time multiple templates (single-threaded)\n",
    "prompt_inputs = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "system_template = 'You are a translator. Translate the following text from English into {language}. Provide only the translation, no other information:'\n",
    "user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "ochain = prompt_template | openai_model | parser\n",
    "gchain = prompt_template | gmodel | parser\n",
    "cchain = prompt_template | claude_model | parser\n",
    "\n",
    "start_time = datetime.now()\n",
    "for tpl in prompt_inputs:\n",
    "    for chain in [ochain, gchain, cchain]:\n",
    "        response = \"\"\n",
    "        #     print()\n",
    "        #     print(prompt_template.format(**tpl))\n",
    "        # stream tokens as they are generated\n",
    "        for r in chain.stream(tpl):\n",
    "            print(r, end=\"\")\n",
    "            response += r\n",
    "        print()\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry decorator with exponential backoff\n",
    "# if I run google too fast I get a 429 error, need to update something in gcp probably\n",
    "\n",
    "def should_retry_exception(exception):\n",
    "    \"\"\"Determine if the exception should trigger a retry. (always retry)\"\"\"\n",
    "    print(type(exception))\n",
    "    print(exception)\n",
    "    return True\n",
    "\n",
    "\n",
    "# @retry(\n",
    "#     stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "#     wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "#     retry=retry_if_exception_type(should_retry_exception),\n",
    "#     before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    "# )\n",
    "async def process_translation(chain, inputs, name):\n",
    "    response = \"\"\n",
    "    async for chunk in chain.astream(inputs):\n",
    "        response += chunk\n",
    "    return response, name\n",
    "\n",
    "\n",
    "async def main():\n",
    "    prompt_inputs = [\n",
    "        {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "        {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "        {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "        {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "        {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "    ]\n",
    "\n",
    "    system_template = 'You are a translator. Translate the following text into {language}. Provide only the translation, no other information:'\n",
    "    user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", user_template)\n",
    "    ])\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    ochain = prompt_template | openai_model | parser\n",
    "    gchain = prompt_template | gmodel | parser\n",
    "    cchain = prompt_template | claude_model | parser\n",
    "    chains = {'openai': ochain, 'google': gchain, 'claude': cchain}\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    tasks = []\n",
    "    for tpl in prompt_inputs:\n",
    "        print(f\"Queuing {tpl['language']} translations...\")\n",
    "        for name, chain in chains.items():\n",
    "            task = asyncio.create_task(process_translation(chain, tpl, name))\n",
    "            tasks.append(task)\n",
    "\n",
    "    try:\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        for response, name in responses:\n",
    "            print(f\"{name}: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {str(e)}\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    difference = end_time - start_time\n",
    "    total_seconds = difference.total_seconds()\n",
    "    print(f\"\\nElapsed seconds: {total_seconds:.6f}\")\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659782d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same but use ainvoke, no stream\n",
    "# Rate limit settings\n",
    "# CALLS_PER_MINUTE = 60  # Adjust based on your quota\n",
    "# MAX_CONCURRENT = 5     # Maximum concurrent requests\n",
    "# sem = Semaphore(MAX_CONCURRENT) # semaphore for controlling concurrent requests\n",
    "\n",
    "# @sleep_and_retry\n",
    "# @limits(calls=CALLS_PER_MINUTE, period=60)\n",
    "@retry(\n",
    "    stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "    retry=retry_if_exception_type(should_retry_exception),\n",
    "    before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    ")\n",
    "async def async_langchain(chain, input_dict, name=\"\"):\n",
    "#     async with sem:\n",
    "        response = await chain.ainvoke(input_dict)\n",
    "        return response, name\n",
    "\n",
    "prompt_templates = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "\n",
    "chains = {'openai': ochain, \n",
    "          'google': gchain, \n",
    "          'claude': cchain}\n",
    "\n",
    "start_time = datetime.now()\n",
    "tasks = []\n",
    "for d in prompt_templates:\n",
    "    for name, chain in chains.items():\n",
    "        task = asyncio.create_task(async_langchain(chain, d, name))\n",
    "        tasks.append(task)\n",
    "responses = await asyncio.gather(*tasks)\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "for response, name in responses:\n",
    "    print(f\"{name}: {response}\")\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203ddd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 19:59:41,162 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 404 Not Found\"\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Invalid URL (POST /v1/chat/completions)', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test o3-mini\u001b[39;00m\n\u001b[1;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4.5-preview-2025-02-27\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;43;03m#     reasoning_effort = \"low\",\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou will act as an expert Python developer.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWrite a Python script that takes a matrix represented as a string with format \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[1,2],[3,4],[5,6]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m and prints the transpose in the same format.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:879\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    876\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    877\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    878\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/_base_client.py:1290\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1278\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1286\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1287\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1288\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1289\u001b[0m     )\n\u001b[0;32m-> 1290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/_base_client.py:967\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.11/site-packages/openai/_base_client.py:1071\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1070\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1071\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1074\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1075\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1080\u001b[0m )\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'Invalid URL (POST /v1/chat/completions)', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "# test o3-mini\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.5-preview-2025-02-27\",\n",
    "#     reasoning_effort = \"low\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will act as an expert Python developer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You will act as an expert Python developer.\"),\n",
    "     (\"user\", \"{input}\")]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "openai_model = ChatOpenAI(model=\"o3-mini\", reasoning_effort=\"low\")\n",
    "ochain = prompt_template | openai_model | parser\n",
    "response = ochain.invoke(\"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e0db6-a2f1-4faa-b0e0-ebdf5a638e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use the metadata to saturate the OpenAI API and use as many tokens per second as available\n",
    "# but not supported by langchain across multiple models so exponential backoff seems to be the best alternative\n",
    "\n",
    "apikey = os.environ.get(\"OPENAI_API_KEY\")\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"OpenAI-Beta\": \"assistants=v2\",\n",
    "    \"Authorization\": f\"Bearer {apikey}\"\n",
    "}\n",
    "body = {\n",
    "    \"model\": \"gpt-4.5-preview\", \"max_tokens\": 25, \"top_p\": 0.8,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is the airspeed velocity of an unladen swallow\"}]\n",
    "        }\n",
    "    ]}\n",
    "\n",
    "try:\n",
    "    response = httpx.post(url, headers=headers, json=body)\n",
    "    response_json = response.json()\n",
    "    print(json.dumps(response_json, indent=3))\n",
    "\n",
    "    # Extract headers starting with 'x-' and load them into a dictionary\n",
    "    x_headers = {k: v for k, v in response.headers.items() if k.lower().startswith('x-')}\n",
    "\n",
    "    # Convert time values into seconds\n",
    "    time_multipliers = {'h': 3600, 'm': 60, 's': 1, 'ms': 0.001}\n",
    "    rate_headers = ['x-ratelimit-limit-requests', 'x-ratelimit-limit-tokens',\n",
    "                    'x-ratelimit-remaining-requests', 'x-ratelimit-remaining-tokens',\n",
    "                    'x-ratelimit-reset-requests', 'x-ratelimit-reset-tokens']\n",
    "    for key in rate_headers:\n",
    "        if key in x_headers:\n",
    "            if 'reset' in key:\n",
    "                total_time = 0\n",
    "                for time_part in re.findall(r'(\\d+)([hms]+)', x_headers[key]):\n",
    "                    total_time += int(time_part[0]) * time_multipliers[time_part[1]]\n",
    "                x_headers[key] = total_time\n",
    "            else:\n",
    "                x_headers[key] = int(x_headers[key])\n",
    "\n",
    "    # Print the headers\n",
    "    print(\"\\nHeaders starting with 'x-':\")\n",
    "    for key, value in x_headers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51a7b3-5aae-4021-8a5e-55e213d95757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured response from openai using pydantic and openai api\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4.5-preview\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "\n",
    "print(json.dumps(json.loads(event.json()), indent=2))\n",
    "\n",
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95dad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using langchain and with_structured_output\n",
    "formatted_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "system_prompt = f\"\"\"You are a precise calendar event extractor. Your task is to extract event details from natural language and format them as structured data.\n",
    "\n",
    "TASK REQUIREMENTS:\n",
    "1. Extract exactly three pieces of information:\n",
    "   - Event name (be specific and descriptive)\n",
    "   - Event date (convert relative dates to YYYY-MM-DD format)\n",
    "   - List of all participants mentioned\n",
    "\n",
    "RULES:\n",
    "- Convert relative dates using today's date ({formatted_date}) as reference\n",
    "- Include a descriptive event name even if only implied\n",
    "- List ALL participants mentioned, even in passing\n",
    "- Never include participants who aren't explicitly mentioned\n",
    "- If any required information is missing, make reasonable assumptions based on context\n",
    "\n",
    "Example Input: \"Alice and Bob are going to a science fair on Friday\"\n",
    "\n",
    "Example Output:\n",
    "{{{{\n",
    "    \"name\": \"Science Fair\",\n",
    "    \"date\": \"2024-02-02\",\n",
    "    \"participants\": [\"Alice\", \"Bob\"]\n",
    "}}}}\n",
    "\n",
    "FORMAT INSTRUCTIONS:\n",
    "The output should be a JSON object with the following schema:\n",
    "{{{{\n",
    "    \"name\": string,       // The name or title of the event\n",
    "    \"date\": string,       // The date in YYYY-MM-DD format\n",
    "    \"participants\": [     // Array of participant names\n",
    "        string,\n",
    "        ...\n",
    "    ]\n",
    "}}}}\n",
    "\"\"\"\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str = Field(description=\"The name or title of the event\")\n",
    "    date: str = Field(description=\"The date of the event in ISO format (YYYY-MM-DD)\")\n",
    "    participants: List[str] = Field(description=\"List of people participating in the event\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=CalendarEvent)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input_text}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | openai_model.with_structured_output(CalendarEvent)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\n",
    "    \"input_text\": \"Alice and Bob are going to a science fair on Friday.\"\n",
    "})\n",
    "\n",
    "# Print the formatted result\n",
    "print(json.dumps(response.dict(), indent=2))\n",
    "\n",
    "# The response is a CalendarEvent object\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.json import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698abd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"\"\"As a specialized research assistant, your task is to perform detailed topic analysis\n",
    "of news item summaries. You will process news items summaries provided as a JSON object according to  \n",
    "the input specification below. You will extract topics of the news item summaries according to the \n",
    "output specification below and return a raw JSON object without any additional formatting or markdown syntax. \n",
    "\n",
    "Input Specification:\n",
    "You will receive an array of JSON objects representing news summaries.\n",
    "Each headline object contains exactly two fields:\n",
    "'id': A unique numeric identifier\n",
    "'summary': The news summmary item\n",
    "\n",
    "Example input:\n",
    "[\n",
    " {{\n",
    "    \"id\": 29,\n",
    "    \"summary\": \" Elon Musk's xAI launched Grok 3, a new family of AI models trained using 100,000 Nvidia H100 GPUs at the Colossus Supercluster; benchmarks show it outperforms competitors like GPT-4o and Claude 3.5 Sonnet in areas such as math, science, and coding.\n",
    "\n",
    " Grok 3 includes advanced features like reasoning models for step-by-step logical problem-solving and a DeepSearch function that synthesizes internet-sourced information into single answers; it is initially available to X Premium+ subscribers, with advanced features under a paid \"SuperGrok\" plan.\n",
    "\n",
    " Former Tesla AI director Andrej Karpathy and others have confirmed Grok 3's strong performance, with Karpathy noting it is comparable to and slightly better than leading AI models from OpenAI and other competitors.\"\n",
    "  }},\n",
    "{{\n",
    "    \"id\": 34,\n",
    "    \"summary\": \" Google Gemini has received a memory upgrade that allows it to recall past conversations and summarize previous chats, enhancing its ability to remember user preferences such as interests and professional details. This feature is currently available only to Google One AI Premium subscribers in English, with broader language support expected soon.\n",
    "\n",
    " Users retain control over their data with options to delete past conversations, prevent chats from being saved, or set them to auto-delete, although discussions can still be used for AI training unless deleted.\n",
    "\n",
    " Similar to OpenAI's ChatGPT persistent memory feature, Gemini's upgrade aims to make chats more practical, though users are advised not to input sensitive information as conversations may be reviewed for quality control.\"\n",
    "  }},\n",
    " {{\n",
    "    \"id\": 47,\n",
    "    \"summary\": \" Major tech companies like OpenAI, Google, and Meta are competing to dominate generative AI, though the path to profitability remains uncertain.  \n",
    "\n",
    " Chinese start-up DeepSeek has introduced a cost-effective way to build powerful AI, disrupting the market and pressuring established players.\n",
    "\n",
    " OpenAI aims to reach 1 billion users, while Meta continues to invest heavily in AI despite market disruptions caused by DeepSeek.\"\n",
    "  }},\n",
    "{{\n",
    "    \"id\": 56,\n",
    "    \"summary\": \"- OpenAI is exploring new measures to protect itself from a potential hostile takeover by Elon Musk.  \n",
    "- The company is in discussions to empower its non-profit board to maintain control as it transitions into a for-profit business model.\"\n",
    "  }},\n",
    " {{\n",
    "    \"id\": 63,\n",
    "    \"summary\": \"- The New York Times has approved the use of select AI tools, such as GitHub Copilot, Google Vertex AI, and their in-house summarization tool Echo, to assist with tasks like content summarization, editing, and enhancing product development, while reinforcing the tools as aids rather than replacements for journalistic work.\n",
    "\n",
    "- Strict guidelines and safeguards have been implemented, including prohibitions on using AI to draft full articles, revise them significantly, or generate images and videos, with a mandatory training video to prevent misuse and protect journalistic integrity.\n",
    "\n",
    "- Some staff members have expressed concerns about AI potentially compromising creativity and accuracy, leading to skepticism about universal adoption, although the guidelines align with standard industry practices.\"\n",
    "  }},\n",
    "]\n",
    "\n",
    "Output Specification:\n",
    "Return a raw JSON object containing 'items', a list of JSON objects, each containing:\n",
    "'id': Matching the input item's id field.\n",
    "'extracted_topics': An array of relevant topic strings\n",
    "Topics should capture:\n",
    "- The main subject matter\n",
    "- Key entities (companies, people, products)\n",
    "- Technical domains, industry sectors, event types\n",
    "\n",
    "Output Example:\n",
    "{{items:\n",
    " [{{\"id\": 29, \"extracted_topics\": ['AI model development', 'xAI Grok capabilities', 'AI advancements']}},\n",
    "  {{\"id\": 34, \"extracted_topics\": ['Google Gemini', 'Interactive AI advancements', 'Digital assistants']}},\n",
    "  {{\"id\": 47, \"extracted_topics\": ['OpenAI', 'Google', 'Meta', 'DeepSeek']}},\n",
    "  {{\"id\": 56, \"extracted_topics\": ['OpenAI', 'non-profit oversight', 'anti-takeover strategies', 'Elon Musk']}},\n",
    "  {{\"id\": 63, \"extracted_topics\": ['New York Times', 'AI in journalism', 'GitHub Copilot', 'Google Vertex AI']}},\n",
    " ]\n",
    "}}\n",
    "\n",
    "Detailed Guidelines:\n",
    "The output must strictly adhere to the output specification.\n",
    "Do not return markdown, return a raw JSON string.\n",
    "For each input item, output a valid JSON object for each news item in the exact schema provided.\n",
    "Extract 3-6 relevant topics per news item.\n",
    "Avoid duplicate or redundant topics.\n",
    "Use topics which are as specific as possible.\n",
    "Please analyze the following news items and provide topic classifications according to these specifications:\n",
    "\"\"\"\n",
    "\n",
    "input_text = \\\n",
    "[\n",
    "  {\n",
    "    \"id\": 0,\n",
    "    \"summary\": \"4 days left to save up to $325 at TechCrunch Sessions: AI\\n* TechCrunch Sessions: AI will take place on June 5 at UC Berkeley's Zellerbach Hall, featuring speakers like Twelve Labs CEO Jae Lee, CapitalG partner Jill Chase, and Khosla Ventures partner Kanu Gulati.  One session will focus on how small companies can stay relevant in the rapidly changing AI space.\\n* The event will include main stage talks, breakout sessions, and demos of the latest AI advancements.  It's aimed at AI leaders, VCs, and tech enthusiasts.\\n* A discount of up to $325 on select tickets is available until March 2 at 11:59 p.m. PT.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"summary\": \"57% of enterprise employees input confidential data into AI tools, survey reveals\\n* 57% of enterprise employees at companies with 5,000+ staff admitted to inputting confidential company data into publicly available generative AI tools like ChatGPT, according to a TELUS Digital Experience survey of 1,000 US-based employees.\\n* 68% of surveyed employees use personal AI accounts for work, indicating a rise in \\\"shadow AI\\\" practices that bypass IT and security oversight, increasing data exposure and compliance violation risks.\\n* While 29% of respondents confirmed their organizations have AI guidelines, enforcement is weak, with only 24% receiving AI training and 42% reporting no consequences for not following guidelines.  TELUS Digital Fuel iX general manager Bret Kinsella emphasized the need for secure, company-approved AI solutions to address the security risks associated with employees using personal AI accounts for work tasks.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"summary\": \"A New Machine Learning Approach Answers What-If Questions\\n* Causal ML, an emerging machine learning technique, helps managers make better decisions by analyzing potential outcomes of different choices, unlike traditional ML which relies on correlations and may provide flawed insights for decision-making.\\n* Causal ML allows for exploring \\\"what-if\\\" scenarios, considering various factors and their influence on outcomes, such as determining the optimal R&D budget by considering its impact on revenue alongside other economic variables.\\n* While traditional ML remains suitable for predictions like stock prices or customer preferences, Causal ML is valuable for exploring cause-and-effect relationships and informing actions in various business functions like product development, finance, and marketing.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 3,\n",
    "    \"summary\": \"A major AI company filed accounts months late and pointed the finger at its Big Four auditor\\n* Supermicro filed delayed financial reports, blaming former auditor EY's resignation over concerns about financial reporting governance and senior management integrity.\\n* EY dropped Supermicro as a client in October 2024 after raising these concerns, prompting an internal review and the hiring of a new accounting firm, BDO.\\n* BDO found no material issues with Supermicro's financials, and the company's stock rebounded after the filings were submitted.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 4,\n",
    "    \"summary\": \"AI CAPTCHA Fails Are the Internets New Comedy Show!\\n* AI struggles with CAPTCHA challenges, particularly image-based ones, often misidentifying objects or failing to interpret blurred text.  Examples include AI misidentifying a painted bicycle symbol as a real bicycle and failing to transcribe blurred text.\\n* While AI can solve some simpler text-based CAPTCHAs, the increasing complexity of CAPTCHAs, including puzzle-based ones, poses a significant challenge even for advanced AI models.\\n* Dedicated CAPTCHA-solving tools, unlike AI, are specifically designed to bypass CAPTCHAs and are more effective for this purpose.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 5,\n",
    "    \"summary\": \"AI Overview jokes\\n* Google's AI Overview feature is producing humorous and inaccurate results for certain queries, including questions about elements ending in \\\"um\\\" and the kosher status of tripe.  The inaccuracies seem to stem from the LLMs' difficulty with spelling and counting, as well as a \\\"lossy-compressed summary of the internet.\\\"\\n* Users report Google is proactively suggesting these flawed queries in the search dropdown, raising questions about whether a human or an algorithm selected them as examples of AI Overview's capabilities.\\n*  Several users provided additional examples of inaccurate or nonsensical responses from Google's AI Overview, ranging from incorrect information about surnames of African origin to illogical hyphenation advice and flawed explanations of linguistic history.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 6,\n",
    "    \"summary\": \"AI startup Bridgetown Research raises $19 million in latest funding\\n* Bridgetown Research, a Seattle-based AI startup, raised $19 million in Series A funding, led by Lightspeed Venture Partners and Accel, with participation from a research university.  The funding round values the company at $250 million.\\n* Unlike many AI companies focused on LLMs, Bridgetown Research develops AI agents that collect and analyze proprietary data from experts and customer surveys to provide insights for strategic decision-making.\\n* The company plans to use the funding to expand the capabilities of its AI agents and broaden access to sector-specific intelligence through partnerships.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 7,\n",
    "    \"summary\": \"AI-Powered Ransomware Attacks\\n* AI is being used to enhance ransomware attacks, automating processes like vulnerability analysis, malware deployment, and lateral movement within networks, making them more sophisticated and harder to detect.\\n* AI-powered phishing attacks are becoming more targeted and convincing, leveraging publicly available data to create personalized messages and dynamically adjusting content based on recipient behavior.\\n* Defending against AI-powered ransomware requires a multi-layered approach including AI-driven security systems, firewalls, updated anti-malware software, intrusion detection systems, end-point detection and response tools, employee training, and robust incident response plans with regular data backups.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 8,\n",
    "    \"summary\": \"Akool unleashes enhancements to its AI human 3D avatars connected to LLMs\\n* Akool Inc. enhanced its AI-driven 3D human avatars to connect with large language models (LLMs), enabling dynamic conversational experiences.  The avatars can display emotions, movements, and hand gestures, creating a lifelike interaction similar to a video call.\\n* Akool offers two avatar types: talking avatars for scripted messages and streaming avatars for real-time conversations, suitable for customer service and guidance.  Users can customize LLMs or integrate with existing models like OpenAI's.\\n* Akool CEO Jiajun Lu highlighted the avatars' success in customer service and language education, citing the improved user experience from interacting with a lifelike figure. He also sees potential in government and healthcare.  Low latency and full-body motion capabilities are key differentiators for Akool's technology.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 9,\n",
    "    \"summary\": \"Alibaba And DeepSeek Intensify AI Showdown, Challenge OpenAI Market Dominance\\n* Chinese AI startup DeepSeek has reopened its core programming interface after a three-week suspension.\\n* DeepSeek had previously suspended service due to capacity issues, according to Bloomberg.\\n* DeepSeek's reopening intensifies the AI competition with OpenAI and other major players.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 11,\n",
    "    \"summary\": \"Amazon's $25 Billion Robotics Push Targets Cost Savings, AI Growth And Temu Competition: Report\\n* Amazon has committed up to $25 billion to its retail network, including robotics, aiming for cost savings and AI growth.\\n* The investment is partly driven by competition from Temu and the increasing costs of artificial intelligence.\\n* The robotics investment has the potential to generate near-term savings for Amazon.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 12,\n",
    "    \"summary\": \"Amazons subscription-based Alexa+ looks highly capableand questionable\\n* Amazon is launching Alexa+, a more conversational and capable version of its voice assistant, powered by large language models.  It will be free for Prime members and $20/month for non-Prime subscribers.\\n* Alexa+ will initially be available on Echo Show 8, 10, 15, and 21 smart displays.  Amazon demonstrated features like personalized recipe recommendations, ticket price monitoring, and seamless integration with other Amazon services like Amazon Music and Fire TV.\\n* This upgrade aims to revitalize Amazon's voice assistant business, which has struggled to be profitable, especially in the face of competition from generative AI chatbots.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 13,\n",
    "    \"summary\": \"Anthropic's Claude 3.7 Sonnet reportedly cost a few tens of millions of dollars to train, similar to Claude 3.5 and cheaper than GPT-4, which cost over $100M\\n* Anthropic's latest AI model, Claude 3.7 Sonnet, reportedly cost \\\"a few tens of millions of dollars\\\" to train, using less than 10^26 FLOPs of computing power.\\n* This cost is comparable to Claude 3.5 and significantly lower than the reported training costs of OpenAI's GPT-4 (over $100 million) and Google's Gemini Ultra (close to $200 million).\\n* While current costs are relatively low, Anthropic CEO Dario Amodei predicts future AI model training will cost billions of dollars.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 14,\n",
    "    \"summary\": \"Apple AI tool transcribed the word 'racist' as 'Trump'\\n* Apple's speech-to-text tool incorrectly transcribed \\\"racist\\\" as \\\"Trump,\\\" a problem the company claims is due to difficulty distinguishing words with \\\"r.\\\"  A fix is being rolled out.\\n* Experts dispute Apple's explanation, citing the distinct phonetic differences and vast training data used for such models.  One expert suggested potential software manipulation, while a former Apple employee called it a \\\"serious prank.\\\"\\n* This follows another incident where Apple's AI-generated news summaries displayed false information, leading to suspension of the feature.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 15,\n",
    "    \"summary\": \"Artificial Intelligence (AI) and the Metaverse Intellectual Property (IP), Standards and Policies Training Course (ONLINE EVENT: March 11, 2025 & ON-DEMAND)\\n* A training course titled \\\"Artificial Intelligence (AI) and the Metaverse: Intellectual Property (IP) and standards and policies\\\" has been launched by ResearchAndMarkets.com, covering legal and commercial aspects of AI and Metaverse technologies.\\n* The course addresses intellectual property issues arising from AI and Metaverse use, AI standards' role in policy development, and the latest UK and EU legislation.  It includes a practical workshop on negotiating IP clauses.\\n*  The speakers include Mark Weston, a partner at Hill Dickinson LLP specializing in commercial, IP, and IT law, and Henry Rivero, founder of Riveroconsult with expertise in TV and digital media.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 16,\n",
    "    \"summary\": \"Balancing Power With Caution: AIs Impact On Breast Cancer\\n* AI is showing promise in breast cancer care, particularly in mammography, potentially increasing detection rates by 20% without increasing false positives.  However, access to AI-enhanced mammography is currently uneven, and research is ongoing to determine if AI can match the effectiveness of dual radiologist readings.\\n* While AI can accelerate analysis and personalize treatment, challenges remain, including biased datasets and cost barriers for patients.  Advocacy for insurance coverage and diverse participation in clinical trials are needed to ensure equitable access.\\n*  The human element in healthcare must be preserved alongside AI advancements. Technology should complement, not replace, compassionate patient care.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 17,\n",
    "    \"summary\": \"Big bang Nvidia Q4 earnings today; here's what you need to watch out for\\n* Nvidia's Q4 FY2025 earnings are projected at $38.32 billion in revenue and $21.08 billion in net income, representing significant year-over-year growth driven by increasing demand for AI infrastructure.\\n*  Analysts are optimistic about Nvidia's performance, with a majority giving \\\"buy\\\" ratings and an average price target of $175.  The company's success is attributed to the rising demand for its data center chips, particularly in the AI sector.\\n*  Investors are also keenly awaiting Nvidia's FY2026 guidance, with projected revenue of approximately $42 billion.  The performance and guidance will be key indicators of the overall AI market's trajectory.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 18,\n",
    "    \"summary\": \"Billionaire Ray Dalio Says AI Risks 'Totalitarian Control Or Anarchy' As It Could Reshape World In Next 5 Years: Here Are AI-Linked ETFs For Investors To Consider\\n* Billionaire Ray Dalio warns that the development of artificial intelligence (AI) could lead to totalitarian control or anarchy in the next five years.\\n* Dalio shared his concerns during a recent podcast interview with Tucker Carlson.\\n* He emphasized the unpredictable nature of AI's development and its potential societal impact. \\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 19,\n",
    "    \"summary\": \"Bluesky Dubs AI Video of Trump Sucking Elon Musk's Toes 'Non-Consensual Explicit Material'\\n* Bluesky removed an AI-generated video depicting Trump engaging in explicit acts with Elon Musk, citing it as \\\"non-consensual explicit material.\\\"\\n* The video, shared by journalist Marisa Kabas, was originally displayed on hacked TV screens within the Department of Housing and Urban Development (HUD) as a protest.\\n* After Kabas appealed the removal, citing the video's political context, Bluesky reinstated it, acknowledging their moderators initially missed the newsworthy context.\\n\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 20,\n",
    "    \"summary\": \"Cash torrent pouring into Nvidia slows  despite booming Blackwell adoptionMay we all have problems like annual revenue growth dropping from 126 to 114 percentSystems11 hrs|6\\n* Nvidia's fiscal year 2025 revenue reached $130 billion (114% growth), slightly lower than the previous year's 126% growth, with Q4 2025 revenue at $39.3 billion, including $11 billion from Blackwell GPUs.  Profits for FY 2025 were $72.9 billion (145% growth).\\n*  The company forecasts Q1 2026 revenue around $43 billion, driven by anticipated demand for AI infrastructure, particularly large GPU clusters, and the shift towards widespread inferencing deployments.\\n*  While facing geopolitical pressures like export controls and potential tariffs, Nvidia remains optimistic about growth, citing strong demand for Blackwell accelerators, NVLink interconnects, and networking products, and partnerships like the one with Cisco for Spectrum-X.\\n\"\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "class TopicSpec(BaseModel):\n",
    "    \"\"\"TopicSpec class for structured output of story topics\"\"\"\n",
    "    id: int = Field(description=\"The id of the story\")\n",
    "    extracted_topics: List[str] = Field(\n",
    "        description=\"List of topics covered in the story\")\n",
    "\n",
    "\n",
    "class TopicSpecList(BaseModel):\n",
    "    \"\"\"List of TopicSpec class for structured output\"\"\"\n",
    "    items: List[TopicSpec] = Field(description=\"List of TopicSpec\")\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', request_timeout=60, verbose=True)\n",
    "langchain.verbose = True\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", input_prompt),\n",
    "        (\"user\", \"{input_text}\")\n",
    "    ])\n",
    "input_dict = {\"input_text\": input_text}\n",
    "\n",
    "chain = prompt_template | model.with_structured_output(TopicSpecList)\n",
    "response = chain.invoke(input_dict)\n",
    "resp = (response.content[8:-3])\n",
    "    \n",
    "resp_dict = json.loads(resp.replace(\"'\", '\"'))\n",
    "\n",
    "# Validate with TopicSpecList\n",
    "try:\n",
    "    validated_response = TopicSpecList(**resp_dict)\n",
    "    print(validated_response)\n",
    "except ValidationError as e:\n",
    "    print(f\"Validation Error: {e.json()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccfbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = (response.content[8:-3])\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# Convert the string to a dictionary\n",
    "resp = (response.content[8:-3])\n",
    "    \n",
    "resp_dict = json.loads(resp.replace(\"'\", '\"'))\n",
    "\n",
    "# Validate with TopicSpecList\n",
    "try:\n",
    "    validated_response = TopicSpecList(**resp_dict)\n",
    "    print(validated_response)\n",
    "except ValidationError as e:\n",
    "    print(f\"Validation Error: {e.json()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df4d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainewsbot",
   "language": "python",
   "name": "ainewsbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
