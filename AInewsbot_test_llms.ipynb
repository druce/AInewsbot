{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb072688",
   "metadata": {},
   "source": [
    "# Test LLM calling\n",
    "- test different ways of calling LLMS, native API, LangChain, sync/async\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74032f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to selectively re-import as needed\n",
    "import sys\n",
    "# del sys.modules['ainb_llm']\n",
    "# del sys.modules['ainb_const']\n",
    "# del sys.modules['ainb_utilities']\n",
    "# del sys.modules['ainb_webscrape']\n",
    "# del sys.modules['AInewsbot_langgraph']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562be45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# import dotenv\n",
    "# import subprocess\n",
    "\n",
    "from collections import Counter\n",
    "import json\n",
    "import uuid\n",
    "import re\n",
    "# import operator\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, PromptTemplate,\n",
    "                                    SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.errors import NodeInterrupt\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import bs4\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type\n",
    ")\n",
    "\n",
    "import asyncio\n",
    "from asyncio import Semaphore\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "\n",
    "# import pyperclip\n",
    "# import shlex\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "import google.generativeai as genai\n",
    "\n",
    "import anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, TypedDict, Annotated, Any\n",
    "\n",
    "import httpx\n",
    "\n",
    "import trafilatura   # web scrape uses this to get clean news stories w/o a lot of js and boilerplate\n",
    "\n",
    "from ainb_const import (\n",
    "                        \n",
    "                        REWRITE_PROMPT, FINAL_SUMMARY_PROMPT,\n",
    "                        SCREENSHOT_DIR, SUMMARIZE_SYSTEM_PROMPT, SUMMARIZE_USER_PROMPT\n",
    "                       )\n",
    "\n",
    "from ainb_utilities import log\n",
    "\n",
    "from AInewsbot_langgraph import (newscatcher_sources, fn_initialize, fn_download_sources, fn_extract_urls,\n",
    "                                 fn_verify_download, fn_extract_newscatcher, fn_filter_urls, fn_topic_clusters,\n",
    "                                 fn_topic_analysis, fn_download_pages, fn_summarize_pages, fn_propose_cats,\n",
    "                                 fn_compose_summary, fn_rewrite_summary, fn_is_revision_complete, fn_send_mail\n",
    "                                )\n",
    "\n",
    "\n",
    "import podcastfy\n",
    "from podcastfy.client import generate_podcast, process_content\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from IPython.display import Audio, display, Markdown\n",
    "\n",
    "import pdb\n",
    "\n",
    "# need this to run async in jupyter since it already has an asyncio event loop running\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Activate global verbose logging\n",
    "set_debug(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59ba13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python            3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 14:21:42) [Clang 18.1.8 ]\n",
      "LangChain         0.3.18\n",
      "OpenAI            1.63.1\n",
      "Anthropic         0.45.2\n",
      "trafilatura       2.0.0\n",
      "numpy             1.26.4\n",
      "pandas            2.2.3\n",
      "sklearn           1.6.1\n",
      "umap              0.5.7\n",
      "podcastfy         0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python            {sys.version}\")\n",
    "print(f\"LangChain         {langchain.__version__}\")\n",
    "print(f\"OpenAI            {openai.__version__}\")\n",
    "print(f\"Anthropic         {anthropic.__version__}\")\n",
    "# print(f\"smtplib           {smtplib.sys.version}\")\n",
    "print(f\"trafilatura       {trafilatura.__version__}\")\n",
    "# print(f\"bs4               {bs4.__version__}\")\n",
    "print(f\"numpy             {np.__version__}\")\n",
    "print(f\"pandas            {pd.__version__}\")\n",
    "print(f\"sklearn           {sklearn.__version__}\")\n",
    "print(f\"umap              {umap.__version__}\")\n",
    "print(f\"podcastfy         {podcastfy.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78ebdfab",
   "metadata": {},
   "outputs": [],
   "source": [
    " LOWCOST_MODEL, MODEL, HIGHCOST_MODEL = 'gpt-4o-mini', 'gpt-4o-2024-11-20', 'o3-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5ed3b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:41:15,673 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ascoltami. Sei bellissima. Sei perfetta e ti amo.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 39, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_709714d124', 'finish_reason': 'stop', 'logprobs': None}, id='run-6e140a92-1a92-47d5-bca1-ffe5d3ec5eb7-0', usage_metadata={'input_tokens': 39, 'output_tokens': 17, 'total_tokens': 56, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a basic LLM call with langchain\\\n",
    "openai_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "response = openai_model.invoke([\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian\"),\n",
    "    HumanMessage(content='Listen to me. You are beautiful. You are perfect and I love you.'),\n",
    "])\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa52d702-271a-4caf-96dc-e8a47700fc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 17,\n",
       "  'prompt_tokens': 39,\n",
       "  'total_tokens': 56,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_709714d124',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8451135-7b09-4b78-9d7e-dab8d43cdbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 39,\n",
       " 'output_tokens': 17,\n",
       " 'total_tokens': 56,\n",
       " 'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       " 'output_token_details': {'audio': 0, 'reasoning': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata\n",
    "# no rate limit info like tokens remaining, available in headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27c0bd0f-b529-4786-b325-2eae57835ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:41:19,917 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "babbage-002\n",
      "chatgpt-4o-latest\n",
      "dall-e-2\n",
      "dall-e-3\n",
      "davinci-002\n",
      "gpt-3.5-turbo\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-3.5-turbo-1106\n",
      "gpt-3.5-turbo-16k\n",
      "gpt-3.5-turbo-16k-0613\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "gpt-4\n",
      "gpt-4-0125-preview\n",
      "gpt-4-0613\n",
      "gpt-4-1106-preview\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4-turbo-preview\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-realtime-preview\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "o1\n",
      "o1-2024-12-17\n",
      "o1-mini\n",
      "o1-mini-2024-09-12\n",
      "o1-preview\n",
      "o1-preview-2024-09-12\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "omni-moderation-2024-09-26\n",
      "omni-moderation-latest\n",
      "text-embedding-3-large\n",
      "text-embedding-3-small\n",
      "text-embedding-ada-002\n",
      "tts-1\n",
      "tts-1-1106\n",
      "tts-1-hd\n",
      "tts-1-hd-1106\n",
      "whisper-1\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "# Retrieve the list of available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print out the model IDs\n",
    "models = [model.id for model in models.data]\n",
    "models.sort()\n",
    "print(\"\\n\".join(models))\n",
    "# yay, we got o3-mini API access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9357cdc3-c02a-499f-9ef2-c8d725173ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini Models:\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-latest\n",
      "Description: The original Gemini 1.0 Pro model. This model will be discontinued on February 15th, 2025. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro\n",
      "Description: The best model for scaling across a wide range of tasks\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-pro\n",
      "Description: The best model for scaling across a wide range of tasks\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-001\n",
      "Description: The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 Pro will be discontinued on February 15th, 2025. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "-----------------------\n",
      "Name: models/gemini-1.0-pro-vision-latest\n",
      "Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-pro-vision\n",
      "Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-001\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro-002\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-pro\n",
      "Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-001\n",
      "Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-001-tuning\n",
      "Description: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createTunedModel']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash\n",
      "Description: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-002\n",
      "Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n",
      "Generation Methods: ['generateContent', 'countTokens', 'createCachedContent']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b\n",
      "Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-001\n",
      "Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-latest\n",
      "Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n",
      "Generation Methods: ['createCachedContent', 'generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-exp-0827\n",
      "Description: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-1.5-flash-8b-exp-0924\n",
      "Description: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-exp\n",
      "Description: Gemini 2.0 Flash Experimental\n",
      "Generation Methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash\n",
      "Description: Gemini 2.0 Flash\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-001\n",
      "Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite-preview\n",
      "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-lite-preview-02-05\n",
      "Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-pro-exp\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-pro-exp-02-05\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-exp-1206\n",
      "Description: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp-01-21\n",
      "Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp\n",
      "Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n",
      "Name: models/gemini-2.0-flash-thinking-exp-1219\n",
      "Description: Gemini 2.0 Flash Thinking Experimental\n",
      "Generation Methods: ['generateContent', 'countTokens']\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def list_gemini_models():\n",
    "    try:\n",
    "        # Configure the library\n",
    "        genai.configure()\n",
    "\n",
    "        # List available models\n",
    "        models = genai.list_models()\n",
    "\n",
    "        print(\"Available Gemini Models:\")\n",
    "        print(\"-----------------------\")\n",
    "        for m in models:\n",
    "            if \"gemini\" in m.name.lower():\n",
    "                print(f\"Name: {m.name}\")\n",
    "                print(f\"Description: {m.description}\")\n",
    "                print(f\"Generation Methods: {m.supported_generation_methods}\")\n",
    "                print(\"-----------------------\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "list_gemini_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a82d1c0b-342e-49db-b311-f4a141e73db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei bellissima. Sei perfetto/a e ti amo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try Gemini\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "# GEMINI_MODEL = \"gemini-1.5-pro\"  # or \"flash\" depending on the desired model\n",
    "GEMINI_MODEL = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "gmodel = ChatGoogleGenerativeAI(\n",
    "    model=GEMINI_MODEL,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = gmodel.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "893686e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:44:02,210 - httpx - INFO - HTTP Request: GET https://api.anthropic.com/v1/models \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Anthropic Models:\n",
      "- claude-3-7-sonnet-20250219\n",
      "  Description: Claude 3.7 Sonnet\n",
      "  Created: 2025-02-19 00:00:00+00:00\n",
      "\n",
      "- claude-3-5-sonnet-20241022\n",
      "  Description: Claude 3.5 Sonnet (New)\n",
      "  Created: 2024-10-22 00:00:00+00:00\n",
      "\n",
      "- claude-3-5-haiku-20241022\n",
      "  Description: Claude 3.5 Haiku\n",
      "  Created: 2024-10-22 00:00:00+00:00\n",
      "\n",
      "- claude-3-5-sonnet-20240620\n",
      "  Description: Claude 3.5 Sonnet (Old)\n",
      "  Created: 2024-06-20 00:00:00+00:00\n",
      "\n",
      "- claude-3-haiku-20240307\n",
      "  Description: Claude 3 Haiku\n",
      "  Created: 2024-03-07 00:00:00+00:00\n",
      "\n",
      "- claude-3-opus-20240229\n",
      "  Description: Claude 3 Opus\n",
      "  Created: 2024-02-29 00:00:00+00:00\n",
      "\n",
      "- claude-3-sonnet-20240229\n",
      "  Description: Claude 3 Sonnet\n",
      "  Created: 2024-02-29 00:00:00+00:00\n",
      "\n",
      "- claude-2.1\n",
      "  Description: Claude 2.1\n",
      "  Created: 2023-11-21 00:00:00+00:00\n",
      "\n",
      "- claude-2.0\n",
      "  Description: Claude 2.0\n",
      "  Created: 2023-07-11 00:00:00+00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Anthropic client with your API key\n",
    "client = Anthropic(api_key=os.environ.get(\"CLAUDE_API_KEY\"))\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "\n",
    "# Print model information\n",
    "print(\"Available Anthropic Models:\")\n",
    "for model in models.data:\n",
    "    print(f\"- {model.id}\")\n",
    "    print(f\"  Description: {model.display_name}\")\n",
    "    print(f\"  Created: {model.created_at}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ceaee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:44:52,353 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei bellissima. Sei perfetta e ti amo.\n"
     ]
    }
   ],
   "source": [
    "# anthropic\n",
    "\n",
    "# Initialize the ChatAnthropic model\n",
    "claude_model = ChatAnthropic(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    anthropic_api_key=os.environ[\"CLAUDE_API_KEY\"],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Create the messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the following from English into Italian without explanation or comment.\"),\n",
    "    HumanMessage(content=\"Listen to me. You are beautiful. You are perfect and I love you.\"),\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "response = claude_model.invoke(messages)\n",
    "\n",
    "# Print the response\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79ff6dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:44:56,816 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ciao! Come posso aiutarti oggi?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a langchain template\n",
    "system_template = \"You are a translator. Translate the following from English into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt_template | openai_model | parser\n",
    "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6382e44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:06,782 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écoute-moi. Tu es parfaite. Tu es belle et je t'aime.\n",
      "Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:08,198 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:08,850 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:09,822 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hör mir zu. Du bist makellos. Du bist wunderschön und ich liebe dich.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:10,606 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escúchame. Eres perfecta. Eres hermosa y te amo.\n",
      "Escúchame. Eres perfecto/perfecta. Eres precioso/preciosa y te amo/quiero.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:11,812 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escúchame. Eres perfecta. Eres preciosa y te amo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:12,671 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei incredibile. Sei magnifico e ti amo.\n",
      "Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:13,760 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:14,562 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figyelj rám. Elbűvölő vagy. Lélegzetelállító vagy, és szeretlek.\n",
      "Hallgass rám. Elragadó vagy. Lenyűgöző vagy, és szeretlek.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:16,081 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figyelj rám. Elragadó vagy. Gyönyörű vagy és szeretlek.\n",
      "\n",
      "\n",
      "Elapsed seconds: 10.459992\n"
     ]
    }
   ],
   "source": [
    "# time multiple templates (single-threaded)\n",
    "prompt_inputs = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "system_template = 'You are a translator. Translate the following text from English into {language}. Provide only the translation, no other information:'\n",
    "user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "ochain = prompt_template | openai_model | parser\n",
    "gchain = prompt_template | gmodel | parser\n",
    "cchain = prompt_template | claude_model | parser\n",
    "\n",
    "start_time = datetime.now()\n",
    "for tpl in prompt_inputs:\n",
    "    for chain in [ochain, gchain, cchain]:\n",
    "        response = \"\"\n",
    "        #     print()\n",
    "        #     print(prompt_template.format(**tpl))\n",
    "        # stream tokens as they are generated\n",
    "        for r in chain.stream(tpl):\n",
    "            print(r, end=\"\")\n",
    "            response += r\n",
    "        print()\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe2d99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queuing French translations...\n",
      "Queuing German translations...\n",
      "Queuing Spanish translations...\n",
      "Queuing Italian translations...\n",
      "Queuing Hungarian translations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:23,365 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:23,371 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:23,376 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:23,450 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:23,474 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:23,478 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:23,508 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:23,515 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:23,522 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:23,708 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai: Écoute-moi. Tu es parfaite. Tu es belle et je t'aime.\n",
      "\n",
      "google: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n",
      "\n",
      "claude: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n",
      "openai: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n",
      "google: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n",
      "\n",
      "claude: Hör mir zu. Du bist makellos. Du bist wunderschön und ich liebe dich.\n",
      "\n",
      "openai: Escúchame. Eres perfecta. Eres preciosa y te amo.\n",
      "\n",
      "google: Escúchame. Eres perfecto/a. Eres precioso/a y te amo/quiero.\n",
      "\n",
      "\n",
      "claude: Escúchame. Eres perfecta. Eres preciosa y te amo.\n",
      "\n",
      "openai: Ascoltami. Sei fantastico. Sei magnifico e ti amo.\n",
      "\n",
      "google: Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "\n",
      "\n",
      "claude: Ascoltami. Sei fantastico. Sei magnifico e ti amo.\n",
      "\n",
      "openai: Hallgass rám. Elképesztő vagy. Lélegzetelállító vagy, és szeretlek.\n",
      "\n",
      "google: Hallgass rám. Elbűvölő vagy. Lenyűgöző vagy, és szeretlek.\n",
      "\n",
      "\n",
      "claude: Figyelj rám. Elbűvölő vagy. Gyönyörű vagy és szeretlek.\n",
      "\n",
      "\n",
      "Elapsed seconds: 1.735194\n"
     ]
    }
   ],
   "source": [
    "# Retry decorator with exponential backoff\n",
    "# if I run google too fast I get a 429 error, need to update something in gcp probably\n",
    "\n",
    "def should_retry_exception(exception):\n",
    "    \"\"\"Determine if the exception should trigger a retry. (always retry)\"\"\"\n",
    "    print(type(exception))\n",
    "    print(exception)\n",
    "    return True\n",
    "\n",
    "\n",
    "# @retry(\n",
    "#     stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "#     wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "#     retry=retry_if_exception_type(should_retry_exception),\n",
    "#     before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    "# )\n",
    "async def process_translation(chain, inputs, name):\n",
    "    response = \"\"\n",
    "    async for chunk in chain.astream(inputs):\n",
    "        response += chunk\n",
    "    return response, name\n",
    "\n",
    "\n",
    "async def main():\n",
    "    prompt_inputs = [\n",
    "        {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "        {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "        {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "        {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "        {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "    ]\n",
    "\n",
    "    system_template = 'You are a translator. Translate the following text into {language}. Provide only the translation, no other information:'\n",
    "    user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", user_template)\n",
    "    ])\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "    ochain = prompt_template | openai_model | parser\n",
    "    gchain = prompt_template | gmodel | parser\n",
    "    cchain = prompt_template | claude_model | parser\n",
    "    chains = {'openai': ochain, 'google': gchain, 'claude': cchain}\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    tasks = []\n",
    "    for tpl in prompt_inputs:\n",
    "        print(f\"Queuing {tpl['language']} translations...\")\n",
    "        for name, chain in chains.items():\n",
    "            task = asyncio.create_task(process_translation(chain, tpl, name))\n",
    "            tasks.append(task)\n",
    "\n",
    "    try:\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        for response, name in responses:\n",
    "            print(f\"{name}: {response}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during translation: {str(e)}\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    difference = end_time - start_time\n",
    "    total_seconds = difference.total_seconds()\n",
    "    print(f\"\\nElapsed seconds: {total_seconds:.6f}\")\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "659782d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:45:58,303 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:58,392 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:58,418 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:58,444 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:58,448 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:58,458 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:58,604 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:58,705 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:58,800 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-02-25 10:45:58,876 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai: Écoute-moi. Tu es parfaite. Tu es belle et je t'aime.\n",
      "google: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "\n",
      "claude: Écoute-moi. Tu es parfait(e). Tu es magnifique et je t'aime.\n",
      "openai: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "google: Hör mir zu. Du bist makellos. Du bist exquisit und ich liebe dich.\n",
      "\n",
      "claude: Hör mir zu. Du bist makellos. Du bist wunderschön und ich liebe dich.\n",
      "openai: Escúchame. Eres perfecto. Eres preciosa y te amo.\n",
      "google: Escúchame. Eres perfecto/perfecta. Eres precioso/preciosa y te amo/quiero.\n",
      "\n",
      "claude: Escúchame. Eres perfecta. Eres preciosa y te amo.\n",
      "openai: Ascoltami. Sei fantastico. Sei magnifico e ti amo.\n",
      "google: Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "\n",
      "claude: Ascoltami. Sei fantastico/a. Sei magnifico/a e ti amo.\n",
      "openai: Hallgass rám. Elbűvölő vagy. Lélegzetelállító vagy és szeretlek.\n",
      "google: Hallgass rám. Elbűvölő vagy. Lenyűgöző vagy, és szeretlek.\n",
      "\n",
      "claude: Figyelj rám. Elragadó vagy. Gyönyörű vagy és szeretlek.\n",
      "\n",
      "\n",
      "Elapsed seconds: 1.224319\n"
     ]
    }
   ],
   "source": [
    "# same but use ainvoke, no stream\n",
    "# Rate limit settings\n",
    "# CALLS_PER_MINUTE = 60  # Adjust based on your quota\n",
    "# MAX_CONCURRENT = 5     # Maximum concurrent requests\n",
    "# sem = Semaphore(MAX_CONCURRENT) # semaphore for controlling concurrent requests\n",
    "\n",
    "# @sleep_and_retry\n",
    "# @limits(calls=CALLS_PER_MINUTE, period=60)\n",
    "@retry(\n",
    "    stop=stop_after_attempt(8),  # Maximum 8 attempts\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=128),  # Wait 2^x * multiplier seconds between retries\n",
    "    retry=retry_if_exception_type(should_retry_exception),\n",
    "    before_sleep=lambda retry_state: print(f\"Retrying after {retry_state.outcome.exception()}, attempt {retry_state.attempt_number}\")\n",
    ")\n",
    "async def async_langchain(chain, input_dict, name=\"\"):\n",
    "#     async with sem:\n",
    "        response = await chain.ainvoke(input_dict)\n",
    "        return response, name\n",
    "\n",
    "prompt_templates = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "\n",
    "chains = {'openai': ochain, \n",
    "          'google': gchain, \n",
    "          'claude': cchain}\n",
    "\n",
    "start_time = datetime.now()\n",
    "tasks = []\n",
    "for d in prompt_templates:\n",
    "    for name, chain in chains.items():\n",
    "        task = asyncio.create_task(async_langchain(chain, d, name))\n",
    "        tasks.append(task)\n",
    "responses = await asyncio.gather(*tasks)\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "for response, name in responses:\n",
    "    print(f\"{name}: {response}\")\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "203ddd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:46:31,875 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the Python script that parses a matrix from a string like \"[1,2],[3,4],[5,6]\", computes its transpose, and prints it in the same format:\n",
      "\n",
      "------------------------------------------------\n",
      "#!/usr/bin/env python3\n",
      "\n",
      "import re\n",
      "\n",
      "def parse_matrix(matrix_str):\n",
      "    # Use regex to extract the content inside each pair of square brackets.\n",
      "    # E.g. \"[1,2],[3,4],[5,6]\" generates groups \"1,2\", \"3,4\", \"5,6\"\n",
      "    pattern = r'\\[([^\\]]+)\\]'\n",
      "    rows = re.findall(pattern, matrix_str)\n",
      "    matrix = []\n",
      "    for row in rows:\n",
      "        # Split the row by comma and convert to int.\n",
      "        # Also, strip any accidental spaces.\n",
      "        numbers = [int(item.strip()) for item in row.split(',')]\n",
      "        matrix.append(numbers)\n",
      "    return matrix\n",
      "\n",
      "def format_matrix(matrix):\n",
      "    # Format each row as \"[elem1,elem2,...]\" and join them with commas.\n",
      "    formatted_rows = []\n",
      "    for row in matrix:\n",
      "        row_str = \",\".join(str(num) for num in row)\n",
      "        formatted_rows.append(f'[{row_str}]')\n",
      "    return \",\".join(formatted_rows)\n",
      "\n",
      "def transpose_matrix(matrix):\n",
      "    # Use zip with unpacking (*) to transpose the matrix.\n",
      "    # zip() returns tuples; convert each tuple to a list if needed.\n",
      "    transposed = list(map(list, zip(*matrix)))\n",
      "    return transposed\n",
      "\n",
      "def main():\n",
      "    # The matrix string can be read from input or set directly.\n",
      "    # For demonstration, we set it directly as below:\n",
      "    input_str = input(\"Enter matrix string (e.g. [1,2],[3,4],[5,6]): \").strip()\n",
      "    \n",
      "    # Parse the matrix\n",
      "    matrix = parse_matrix(input_str)\n",
      "    \n",
      "    # Compute the transpose\n",
      "    transposed = transpose_matrix(matrix)\n",
      "    \n",
      "    # Format the transposed matrix to string in the same format\n",
      "    output_str = format_matrix(transposed)\n",
      "    \n",
      "    # Print out the result\n",
      "    print(output_str)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "------------------------------------------------\n",
      "\n",
      "How It Works:\n",
      "\n",
      "1. The function parse_matrix uses a regular expression to extract each row from the input string.\n",
      "2. The transpose_matrix function uses zip(*) to transpose the matrix.\n",
      "3. The format_matrix function converts the matrix back into the desired string format.\n",
      "4. The main() function ties everything together, reading input and printing the transpose.\n",
      "\n",
      "You can run this script and provide an input string like \"[1,2],[3,4],[5,6]\", and it will output \"[1,3,5],[2,4,6]\".\n"
     ]
    }
   ],
   "source": [
    "# test o3-mini\n",
    "client = OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    reasoning_effort = \"low\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You will act as an expert Python developer.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c736089a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 10:46:45,856 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is one way to accomplish this. In the script, we first parse the input string into a list of lists (each inner list represents a row of the matrix), then compute the transpose using Python’s built-in zip function, and finally format the transposed matrix back into the input string format.\n",
      "\n",
      "You can save this script into a file (e.g., transpose_matrix.py) and run it.\n",
      "\n",
      "------------------------------------------------------------\n",
      "#!/usr/bin/env python3\n",
      "\n",
      "def parse_matrix(matrix_str):\n",
      "    \"\"\"\n",
      "    Parse a matrix from a string of the format '[1,2],[3,4],[5,6]'\n",
      "    into a list of lists.\n",
      "    \"\"\"\n",
      "    # Split the string using the nuanced '],[' separator.\n",
      "    # First, remove any surrounding whitespace and strip off\n",
      "    # any leading '[' or trailing ']' in case input is bracketed.\n",
      "    matrix_str = matrix_str.strip()\n",
      "    if matrix_str.startswith('[') and matrix_str.endswith(']'):\n",
      "        matrix_str = matrix_str[1:-1]\n",
      "    # Now split on the sequence \"],[\".\n",
      "    rows_str = matrix_str.split(\"],[\")\n",
      "    matrix = []\n",
      "    for row in rows_str:\n",
      "        # Remove any stray brackets just in case.\n",
      "        row = row.strip(\"[]\")\n",
      "        # Split by comma and remove any extra whitespace from each element.\n",
      "        elements = [elem.strip() for elem in row.split(\",\") if elem.strip()]\n",
      "        matrix.append(elements)\n",
      "    return matrix\n",
      "\n",
      "def format_matrix(matrix):\n",
      "    \"\"\"\n",
      "    Format a list of lists into a string in the format '[a,b],[c,d],...'\n",
      "    \"\"\"\n",
      "    formatted_rows = []\n",
      "    for row in matrix:\n",
      "        formatted_row = \"[\" + \",\".join(row) + \"]\"\n",
      "        formatted_rows.append(formatted_row)\n",
      "    return \",\".join(formatted_rows)\n",
      "\n",
      "def main():\n",
      "    # Read input from the user.\n",
      "    # Expected format example: [1,2],[3,4],[5,6]\n",
      "    matrix_input = input(\"Enter matrix (format: '[1,2],[3,4],[5,6]'): \").strip()\n",
      "    if not matrix_input:\n",
      "        print(\"No matrix input provided.\")\n",
      "        return\n",
      "\n",
      "    # Parse the input string into a list of rows.\n",
      "    matrix = parse_matrix(matrix_input)\n",
      "\n",
      "    # Compute the transpose using zip. Note that each row is a list of strings.\n",
      "    # The zip(*matrix) returns tuples for each column.\n",
      "    transposed = list(zip(*matrix))\n",
      "    # Convert each tuple into a list so we can format it easily.\n",
      "    transposed = [list(col) for col in transposed]\n",
      "\n",
      "    # Format the transposed matrix back to the required string format.\n",
      "    result = format_matrix(transposed)\n",
      "    print(result)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "------------------------------------------------------------\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. The function parse_matrix takes the input string and:\n",
      "   - Removes any outer brackets if present.\n",
      "   - Splits the string on \"],[\" to separate the rows.\n",
      "   - Strips extra characters and spaces, then splits each row by commas.\n",
      "2. The main function computes the transpose with zip(*matrix), then converts the resulting tuples to lists.\n",
      "3. Finally, format_matrix constructs the resulting string by joining each row in the desired format.\n",
      "\n",
      "When you run the script and input:\n",
      "\n",
      "  [1,2],[3,4],[5,6]\n",
      "\n",
      "It will output:\n",
      "\n",
      "  [1,3,5],[2,4,6]\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You will act as an expert Python developer.\"),\n",
    "     (\"user\", \"{input}\")]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "openai_model = ChatOpenAI(model=\"o3-mini\", reasoning_effort=\"low\")\n",
    "ochain = prompt_template | openai_model | parser\n",
    "response = ochain.invoke(\"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e0db6-a2f1-4faa-b0e0-ebdf5a638e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use the metadata to saturate the OpenAI API and use as many tokens per second as available\n",
    "# but not supported by langchain across multiple models so exponential backoff seems to be the best alternative\n",
    "\n",
    "apikey = os.environ.get(\"OPENAI_API_KEY\")\n",
    "url = \"https://api.openai.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"OpenAI-Beta\": \"assistants=v2\",\n",
    "    \"Authorization\": f\"Bearer {apikey}\"\n",
    "}\n",
    "body = {\n",
    "    \"model\": \"gpt-4o-2024-08-06\", \"max_tokens\": 25, \"top_p\": 0.8,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": [{\"type\": \"text\", \"text\": \"What is the airspeed velocity of an unladen swallow\"}]\n",
    "        }\n",
    "    ]}\n",
    "\n",
    "try:\n",
    "    response = httpx.post(url, headers=headers, json=body)\n",
    "    response_json = response.json()\n",
    "    print(json.dumps(response_json, indent=3))\n",
    "\n",
    "    # Extract headers starting with 'x-' and load them into a dictionary\n",
    "    x_headers = {k: v for k, v in response.headers.items() if k.lower().startswith('x-')}\n",
    "\n",
    "    # Convert time values into seconds\n",
    "    time_multipliers = {'h': 3600, 'm': 60, 's': 1, 'ms': 0.001}\n",
    "    rate_headers = ['x-ratelimit-limit-requests', 'x-ratelimit-limit-tokens',\n",
    "                    'x-ratelimit-remaining-requests', 'x-ratelimit-remaining-tokens',\n",
    "                    'x-ratelimit-reset-requests', 'x-ratelimit-reset-tokens']\n",
    "    for key in rate_headers:\n",
    "        if key in x_headers:\n",
    "            if 'reset' in key:\n",
    "                total_time = 0\n",
    "                for time_part in re.findall(r'(\\d+)([hms]+)', x_headers[key]):\n",
    "                    total_time += int(time_part[0]) * time_multipliers[time_part[1]]\n",
    "                x_headers[key] = total_time\n",
    "            else:\n",
    "                x_headers[key] = int(x_headers[key])\n",
    "\n",
    "    # Print the headers\n",
    "    print(\"\\nHeaders starting with 'x-':\")\n",
    "    for key, value in x_headers.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f51a7b3-5aae-4021-8a5e-55e213d95757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured response from openai using pydantic and openai api\n",
    "client = OpenAI()\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information in JSON format.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "\n",
    "print(json.dumps(json.loads(event.json()), indent=2))\n",
    "\n",
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95dad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using langchain and with_structured_output\n",
    "formatted_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "system_prompt = f\"\"\"You are a precise calendar event extractor. Your task is to extract event details from natural language and format them as structured data.\n",
    "\n",
    "TASK REQUIREMENTS:\n",
    "1. Extract exactly three pieces of information:\n",
    "   - Event name (be specific and descriptive)\n",
    "   - Event date (convert relative dates to YYYY-MM-DD format)\n",
    "   - List of all participants mentioned\n",
    "\n",
    "RULES:\n",
    "- Convert relative dates using today's date ({formatted_date}) as reference\n",
    "- Include a descriptive event name even if only implied\n",
    "- List ALL participants mentioned, even in passing\n",
    "- Never include participants who aren't explicitly mentioned\n",
    "- If any required information is missing, make reasonable assumptions based on context\n",
    "\n",
    "Example Input: \"Alice and Bob are going to a science fair on Friday\"\n",
    "\n",
    "Example Output:\n",
    "{{{{\n",
    "    \"name\": \"Science Fair\",\n",
    "    \"date\": \"2024-02-02\",\n",
    "    \"participants\": [\"Alice\", \"Bob\"]\n",
    "}}}}\n",
    "\n",
    "FORMAT INSTRUCTIONS:\n",
    "The output should be a JSON object with the following schema:\n",
    "{{{{\n",
    "    \"name\": string,       // The name or title of the event\n",
    "    \"date\": string,       // The date in YYYY-MM-DD format\n",
    "    \"participants\": [     // Array of participant names\n",
    "        string,\n",
    "        ...\n",
    "    ]\n",
    "}}}}\n",
    "\"\"\"\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str = Field(description=\"The name or title of the event\")\n",
    "    date: str = Field(description=\"The date of the event in ISO format (YYYY-MM-DD)\")\n",
    "    participants: List[str] = Field(description=\"List of people participating in the event\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=CalendarEvent)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input_text}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | openai_model.with_structured_output(CalendarEvent)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\n",
    "    \"input_text\": \"Alice and Bob are going to a science fair on Friday.\"\n",
    "})\n",
    "\n",
    "# Print the formatted result\n",
    "print(json.dumps(response.dict(), indent=2))\n",
    "\n",
    "# The response is a CalendarEvent object\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b2e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainewsbot",
   "language": "python",
   "name": "ainewsbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
