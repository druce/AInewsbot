{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd15f6b8-26b9-468c-8427-9de836b240fd",
   "metadata": {},
   "source": [
    "AInewsbot.ipynb\n",
    "\n",
    "- Automate collecting daily AI news\n",
    "- Open URLs of news sites specififed in `sources` dict (sources.yaml) using Selenium and Firefox\n",
    "- Save HTML of each URL in htmldata directory\n",
    "- Extract URLs from all files, create a pandas dataframe with url, title, src\n",
    "- Use ChatGPT to filter only AI-related headlines by sending a prompt and formatted table of headlines\n",
    "- Use SQLite to filter headlines previously seen \n",
    "- OPENAI_API_KEY should be in the environment or in a .env file\n",
    "  \n",
    "Alternative manual workflow to get HTML files if necessary\n",
    "- Use Chrome, open e.g. Tech News bookmark folder, right-click and open all bookmarks in new window\n",
    "- on Google News, make sure switch to AI tab\n",
    "- on Google News, Feedly, Reddit, scroll to additional pages as desired\n",
    "- Use SingleFile extension, 'save all tabs'\n",
    "- Move files to htmldata directory\n",
    "- Run lower part of notebook to process the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986568e",
   "metadata": {},
   "source": [
    "1. initialize\n",
    "2. fetch web pages\n",
    "3. parse news story urls from web pages\n",
    "4. filter headlines by relevance, not previously seen\n",
    "5. perform topic analysis on headlines, and ordering by topic\n",
    "6. summarize individual pages as bullet points\n",
    "7. from bullet points, extract top 10 most common themes and stories of the day in order of importance\n",
    "8. topic analysis of bullet points, categorize bullet points as belonging to particular themes\n",
    "9. for each theme, make a summary and links. Here we want to iterate to improve summaries per specific criteria.\n",
    "10. combine themes and send."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9663d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# del sys.modules['ainb_const']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7195880-36be-4987-b71f-86590c934d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import yaml\n",
    "import dotenv\n",
    "import sqlite3\n",
    "import unicodedata\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "import markdown\n",
    "\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from ainb_const import (DOWNLOAD_DIR, LOWCOST_MODEL, MODEL, CANONICAL_TOPICS,\n",
    "                        SOURCECONFIG, FILTER_PROMPT, TOPIC_PROMPT,\n",
    "                        SUMMARIZE_SYSTEM_PROMPT, SUMMARIZE_USER_PROMPT, FINAL_SUMMARY_PROMPT, TOP_CATEGORIES_PROMPT,\n",
    "                        MAX_INPUT_TOKENS, MAX_OUTPUT_TOKENS, MAX_RETRIES, TEMPERATURE)\n",
    "from ainb_utilities import (log, delete_files, filter_unseen_urls_db, insert_article, \n",
    "                            nearest_neighbor_sort, agglomerative_cluster_sort, traveling_salesman_sort_scipy,\n",
    "                            unicode_to_ascii, send_gmail)\n",
    "from ainb_webscrape import (get_driver, quit_drivers, launch_drivers, get_file, get_url, parse_file, \n",
    "                            get_og_tags, get_path_from_url, trimmed_href, process_source_queue_factory, \n",
    "                            process_url_queue_factory, DRIVERS)\n",
    "from ainb_llm import paginate_df, process_pages, fetch_pages, fetch_openai, fetch_all_summaries, fetch_openai_summary, trunc_tokens\n",
    "\n",
    "\n",
    "import asyncio\n",
    "# need this to run async in jupyter since it already has an asyncio event loop running\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427af54",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2950cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API module\n",
    "client = OpenAI()\n",
    "\n",
    "# Or can use REST API directly\n",
    "API_URL = 'https://api.openai.com/v1/chat/completions'\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {os.getenv(\"OPENAI_API_KEY\")}',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2324ab9-f769-4a6f-aa16-a30a001ede78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 15:08:55,829 - AInewsbot - INFO - Load 17 sources from sources.yaml\n",
      "2024-07-09 15:08:55,830 - AInewsbot - INFO - Ars Technica -> https://arstechnica.com/ -> Ars Technica.html\n",
      "2024-07-09 15:08:55,830 - AInewsbot - INFO - Bloomberg Tech -> https://www.bloomberg.com/technology -> Bloomberg Technology - Bloomberg.html\n",
      "2024-07-09 15:08:55,831 - AInewsbot - INFO - Business Insider -> https://www.businessinsider.com/tech -> Tech - Business Insider.html\n",
      "2024-07-09 15:08:55,831 - AInewsbot - INFO - FT Tech -> https://www.ft.com/technology -> Technology.html\n",
      "2024-07-09 15:08:55,831 - AInewsbot - INFO - Feedly AI -> https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ -> Discover and Add New Feedly AI Feeds.html\n",
      "2024-07-09 15:08:55,832 - AInewsbot - INFO - Google News -> https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US%3Aen -> Google News - Technology - Artificial intelligence.html\n",
      "2024-07-09 15:08:55,832 - AInewsbot - INFO - Hacker News -> https://news.ycombinator.com/ -> Hacker News Page 1.html\n",
      "2024-07-09 15:08:55,833 - AInewsbot - INFO - Hacker News 2 -> https://news.ycombinator.com/?p=2 -> Hacker News Page 2.html\n",
      "2024-07-09 15:08:55,833 - AInewsbot - INFO - HackerNoon -> https://hackernoon.com/ -> HackerNoon - read, write and learn about any technology.html\n",
      "2024-07-09 15:08:55,833 - AInewsbot - INFO - NYT Tech -> https://www.nytimes.com/section/technology -> Technology - The New York Times.html\n",
      "2024-07-09 15:08:55,833 - AInewsbot - INFO - Reddit -> https://www.reddit.com/r/ChatGPT+ChatGPTCoding+MacOS+MachineLearning+OpenAI+ProgrammerHumor+Windows10+battlestations+buildapc+cordcutters+dataisbeautiful+gadgets+hardware+linux+msp+programming+realtech+software+talesfromtechsupport+tech+technews+technology+techsupportgore+windows/top/?sort=top&t=day -> top scoring links _ multi.html\n",
      "2024-07-09 15:08:55,834 - AInewsbot - INFO - Techmeme -> https://www.techmeme.com/river -> Techmeme.html\n",
      "2024-07-09 15:08:55,834 - AInewsbot - INFO - The Register -> https://www.theregister.com/ -> The Register_ Enterprise Technology News and Analysis.html\n",
      "2024-07-09 15:08:55,834 - AInewsbot - INFO - The Verge -> https://www.theverge.com/ai-artificial-intelligence -> Artificial Intelligence - The Verge.html\n",
      "2024-07-09 15:08:55,835 - AInewsbot - INFO - VentureBeat -> https://venturebeat.com/category/ai/ -> AI News _ VentureBeat.html\n",
      "2024-07-09 15:08:55,835 - AInewsbot - INFO - WSJ Tech -> https://www.wsj.com/tech -> Technology - WSJ.com.html\n",
      "2024-07-09 15:08:55,835 - AInewsbot - INFO - WaPo Tech -> https://www.washingtonpost.com/business/technology/ -> Technology - The Washington Post.html\n",
      "2024-07-09 15:08:55,836 - AInewsbot - INFO - Mapped 17 source page titles to sources\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  load sources to scrape from sources.yaml\n",
    "with open(SOURCECONFIG, \"r\") as stream:\n",
    "    try:\n",
    "        sources = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "log(f\"Load {len(sources)} sources from {SOURCECONFIG}\")\n",
    "\n",
    "# make a reverse dict to map output file titles to source names\n",
    "sources_reverse = {}\n",
    "for k, v in sources.items():\n",
    "    log(f\"{k} -> {v['url']} -> {v['title']}.html\")\n",
    "    v['sourcename'] = k\n",
    "    # map filename (title) to source name\n",
    "    sources_reverse[v['title']] = k\n",
    "\n",
    "log(f\"Mapped {len(sources_reverse)} source page titles to sources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85998de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ars Technica': {'include': ['^https://arstechnica.com/(\\\\w+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'Ars Technica',\n",
       "  'url': 'https://arstechnica.com/',\n",
       "  'sourcename': 'Ars Technica'},\n",
       " 'Bloomberg Tech': {'include': ['^https://www.bloomberg.com/news/'],\n",
       "  'title': 'Bloomberg Technology - Bloomberg',\n",
       "  'url': 'https://www.bloomberg.com/technology',\n",
       "  'sourcename': 'Bloomberg Tech'},\n",
       " 'Business Insider': {'exclude': ['^https://www.insider.com',\n",
       "   '^https://www.passionfroot.me'],\n",
       "  'title': 'Tech - Business Insider',\n",
       "  'url': 'https://www.businessinsider.com/tech',\n",
       "  'sourcename': 'Business Insider'},\n",
       " 'FT Tech': {'include': ['https://www.ft.com/content/'],\n",
       "  'title': 'Technology',\n",
       "  'url': 'https://www.ft.com/technology',\n",
       "  'sourcename': 'FT Tech'},\n",
       " 'Feedly AI': {'exclude': ['^https://feedly.com',\n",
       "   '^https://s1.feedly.com',\n",
       "   '^https://blog.feedly.com'],\n",
       "  'scroll': 5,\n",
       "  'initial_sleep': 30,\n",
       "  'title': 'Discover and Add New Feedly AI Feeds',\n",
       "  'url': 'https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ',\n",
       "  'sourcename': 'Feedly AI'},\n",
       " 'Google News': {'click': '//*[@aria-label=\"Artificial intelligence\"]',\n",
       "  'include': ['^https://news.google.com/articles/'],\n",
       "  'scroll': 2,\n",
       "  'title': 'Google News - Technology - Artificial intelligence',\n",
       "  'url': 'https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US%3Aen',\n",
       "  'sourcename': 'Google News'},\n",
       " 'Hacker News': {'exclude': ['https://news.ycombinator.com/',\n",
       "   'https://www.ycombinator.com/'],\n",
       "  'title': 'Hacker News Page 1',\n",
       "  'url': 'https://news.ycombinator.com/',\n",
       "  'minlength': 5,\n",
       "  'sourcename': 'Hacker News'},\n",
       " 'Hacker News 2': {'exclude': ['https://news.ycombinator.com',\n",
       "   'https://www.ycombinator.com'],\n",
       "  'title': 'Hacker News Page 2',\n",
       "  'url': 'https://news.ycombinator.com/?p=2',\n",
       "  'sourcename': 'Hacker News 2'},\n",
       " 'HackerNoon': {'exclude': ['^https://hackernoon.com/$',\n",
       "   '^https://hackernoon.com/c$',\n",
       "   '^https://hackernoon.com/coins$',\n",
       "   '^https://hackernoon.com/companies$',\n",
       "   '^https://hackernoon.com/gallery$',\n",
       "   '^https://hackernoon.com/how-to-gain-followers-and-newsletter-subs-directly-on-hackernoon$',\n",
       "   '^https://hackernoon.com/login$',\n",
       "   '^https://hackernoon.com/reader-boot$',\n",
       "   '^https://hackernoon.com/sitemap.xml$',\n",
       "   '^https://hackernoon.com/startups$',\n",
       "   '^https://hackernoon.com/techbeat$',\n",
       "   '^https://hackernoon.com/why-i-write-on-hacker-noon-nl28335q$',\n",
       "   '^https://hackernoon.com/writer-signup$'],\n",
       "  'title': 'HackerNoon - read, write and learn about any technology',\n",
       "  'url': 'https://hackernoon.com/',\n",
       "  'sourcename': 'HackerNoon'},\n",
       " 'NYT Tech': {'include': ['^https://www.nytimes.com/(\\\\d+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'Technology - The New York Times',\n",
       "  'url': 'https://www.nytimes.com/section/technology',\n",
       "  'sourcename': 'NYT Tech'},\n",
       " 'Reddit': {'exclude': ['^https://chat.reddit.com/',\n",
       "   '^https://i.redd.it/',\n",
       "   '^https://redditblog.com/',\n",
       "   '^https://www.redditinc.com/',\n",
       "   '^https://www.reddithelp.com/',\n",
       "   '^https://itunes.apple.com/',\n",
       "   '^https://play.google.com/'],\n",
       "  'scroll': 2,\n",
       "  'minlength': 8,\n",
       "  'title': 'top scoring links _ multi',\n",
       "  'url': 'https://www.reddit.com/r/ChatGPT+ChatGPTCoding+MacOS+MachineLearning+OpenAI+ProgrammerHumor+Windows10+battlestations+buildapc+cordcutters+dataisbeautiful+gadgets+hardware+linux+msp+programming+realtech+software+talesfromtechsupport+tech+technews+technology+techsupportgore+windows/top/?sort=top&t=day',\n",
       "  'sourcename': 'Reddit'},\n",
       " 'Techmeme': {'exclude': ['^https://www.techmeme.com',\n",
       "   '^https://twitter.com/',\n",
       "   '^https://www.threads.net',\n",
       "   '^https://www.linkedin.com',\n",
       "   '^https://mastodon.social',\n",
       "   '^https://bsky.app'],\n",
       "  'title': 'Techmeme',\n",
       "  'url': 'https://www.techmeme.com/river',\n",
       "  'sourcename': 'Techmeme'},\n",
       " 'The Register': {'include': ['^https://www.theregister.com/(\\\\d+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'The Register_ Enterprise Technology News and Analysis',\n",
       "  'url': 'https://www.theregister.com/',\n",
       "  'sourcename': 'The Register'},\n",
       " 'The Verge': {'include': ['^https://www.theverge.com/(\\\\d+)/(\\\\d+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'Artificial Intelligence - The Verge',\n",
       "  'url': 'https://www.theverge.com/ai-artificial-intelligence',\n",
       "  'sourcename': 'The Verge'},\n",
       " 'VentureBeat': {'title': 'AI News _ VentureBeat',\n",
       "  'url': 'https://venturebeat.com/category/ai/',\n",
       "  'sourcename': 'VentureBeat'},\n",
       " 'WSJ Tech': {'include': ['^https://www.wsj.com/articles/'],\n",
       "  'title': 'Technology - WSJ.com',\n",
       "  'url': 'https://www.wsj.com/tech',\n",
       "  'sourcename': 'WSJ Tech'},\n",
       " 'WaPo Tech': {'include': ['https://www.washingtonpost.com/(\\\\w+)/(\\\\d+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'Technology - The Washington Post',\n",
       "  'url': 'https://www.washingtonpost.com/business/technology/',\n",
       "  'sourcename': 'WaPo Tech'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562f83b1-b4b8-4355-8b8b-4fe028ea2af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ars Technica': 'Ars Technica',\n",
       " 'Bloomberg Technology - Bloomberg': 'Bloomberg Tech',\n",
       " 'Tech - Business Insider': 'Business Insider',\n",
       " 'Technology': 'FT Tech',\n",
       " 'Discover and Add New Feedly AI Feeds': 'Feedly AI',\n",
       " 'Google News - Technology - Artificial intelligence': 'Google News',\n",
       " 'Hacker News Page 1': 'Hacker News',\n",
       " 'Hacker News Page 2': 'Hacker News 2',\n",
       " 'HackerNoon - read, write and learn about any technology': 'HackerNoon',\n",
       " 'Technology - The New York Times': 'NYT Tech',\n",
       " 'top scoring links _ multi': 'Reddit',\n",
       " 'Techmeme': 'Techmeme',\n",
       " 'The Register_ Enterprise Technology News and Analysis': 'The Register',\n",
       " 'Artificial Intelligence - The Verge': 'The Verge',\n",
       " 'AI News _ VentureBeat': 'VentureBeat',\n",
       " 'Technology - WSJ.com': 'WSJ Tech',\n",
       " 'Technology - The Washington Post': 'WaPo Tech'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources_reverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2549cf13-0de6-47d2-b7a9-544b5099ff11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 15:09:04,191 - AInewsbot - INFO - 17\n",
      "2024-07-09 15:09:04,193 - AInewsbot - INFO - htmldata/Technology - The Washington Post (07_09_2024 11_21_39 AM).html\n",
      "2024-07-09 15:09:04,193 - AInewsbot - INFO - htmldata/Technology - WSJ.com (07_09_2024 11_21_34 AM).html\n",
      "2024-07-09 15:09:04,194 - AInewsbot - INFO - htmldata/AI News _ VentureBeat (07_09_2024 11_21_32 AM).html\n",
      "2024-07-09 15:09:04,194 - AInewsbot - INFO - htmldata/Discover and Add New Feedly AI Feeds (07_09_2024 11_21_28 AM).html\n",
      "2024-07-09 15:09:04,195 - AInewsbot - INFO - htmldata/top scoring links _ multi (07_09_2024 11_21_22 AM).html\n",
      "2024-07-09 15:09:04,195 - AInewsbot - INFO - htmldata/Artificial Intelligence - The Verge (07_09_2024 11_21_21 AM).html\n",
      "2024-07-09 15:09:04,196 - AInewsbot - INFO - htmldata/The Register_ Enterprise Technology News and Analysis (07_09_2024 11_21_10 AM).html\n",
      "2024-07-09 15:09:04,196 - AInewsbot - INFO - htmldata/Techmeme (07_09_2024 11_21_00 AM).html\n",
      "2024-07-09 15:09:04,197 - AInewsbot - INFO - htmldata/Technology - The New York Times (07_09_2024 11_20_49 AM).html\n",
      "2024-07-09 15:09:04,197 - AInewsbot - INFO - htmldata/HackerNoon - read, write and learn about any technology (07_09_2024 11_20_48 AM).html\n",
      "2024-07-09 15:09:04,197 - AInewsbot - INFO - htmldata/Google News - Technology - Artificial intelligence (07_09_2024 11_20_38 AM).html\n",
      "2024-07-09 15:09:04,198 - AInewsbot - INFO - htmldata/Hacker News Page 2 (07_09_2024 11_20_37 AM).html\n",
      "2024-07-09 15:09:04,198 - AInewsbot - INFO - htmldata/Hacker News Page 1 (07_09_2024 11_20_26 AM).html\n",
      "2024-07-09 15:09:04,198 - AInewsbot - INFO - htmldata/Technology (07_09_2024 11_20_16 AM).html\n",
      "2024-07-09 15:09:04,199 - AInewsbot - INFO - htmldata/Ars Technica (07_09_2024 11_20_04 AM).html\n",
      "2024-07-09 15:09:04,199 - AInewsbot - INFO - htmldata/Bloomberg Technology - Bloomberg (07_09_2024 11_20_04 AM).html\n",
      "2024-07-09 15:09:04,199 - AInewsbot - INFO - htmldata/Tech - Business Insider (07_09_2024 11_20_04 AM).html\n"
     ]
    }
   ],
   "source": [
    "# determine files already in htmldata directory\n",
    "# List all paths in the directory matching today's date\n",
    "nfiles = 50\n",
    "files = [os.path.join(DOWNLOAD_DIR, file)\n",
    "         for file in os.listdir(DOWNLOAD_DIR)]\n",
    "# Get the current date\n",
    "today = datetime.now()\n",
    "year, month, day = today.year, today.month, today.day\n",
    "datestr = datetime.now().strftime(\"%m_%d_%Y\")\n",
    "\n",
    "# filter files only\n",
    "files = [file for file in files if os.path.isfile(file)]\n",
    "\n",
    "# Sort files by modification time and take top 50\n",
    "files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "file = files[:nfiles]\n",
    "\n",
    "# filter files by with today's date ending in .html\n",
    "files = [\n",
    "    file for file in files if datestr in file and file.endswith(\".html\")]\n",
    "log(len(files))\n",
    "for file in files:\n",
    "    log(file)\n",
    "\n",
    "saved_pages = []\n",
    "for file in files:\n",
    "    filename = os.path.basename(file)\n",
    "    # locate date like '01_14_2024' in filename\n",
    "    position = filename.find(\" (\" + datestr)\n",
    "    basename = filename[:position]\n",
    "    # match to source name\n",
    "    sourcename = sources_reverse.get(basename)\n",
    "    if sourcename is None:\n",
    "        log(f\"Skipping {basename}, no sourcename metadata\")\n",
    "        continue\n",
    "    sources[sourcename]['latest'] = file\n",
    "    saved_pages.append((sourcename, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e703b",
   "metadata": {},
   "source": [
    "# Fetch and save source pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb94c3-464b-43d7-a156-f965c4e6e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch HTML files from sources\n",
    "\n",
    "# empty download directory\n",
    "delete_files(DOWNLOAD_DIR)\n",
    "\n",
    "# save each file specified from sources\n",
    "num_browsers = 3\n",
    "log(f\"Saving HTML files using {num_browsers} browsers\")\n",
    "\n",
    "# Create a queue for multiprocessing and populate it \n",
    "queue = multiprocessing.Queue()\n",
    "for item in sources.values():\n",
    "    queue.put(item)\n",
    "    \n",
    "# Function to take the queue and pop entries off and process until none are left\n",
    "# lets you create an array of functions with different args\n",
    "callable = process_source_queue_factory(queue)\n",
    "\n",
    "saved_pages = launch_drivers(num_browsers, callable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5aa21a-7387-4e76-a17e-6b22544dd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(f\"Saved {len(saved_pages)} pages\")\n",
    "\n",
    "print(len(saved_pages))\n",
    "for sourcename, page in saved_pages:\n",
    "    sources[sourcename]['latest'] = page\n",
    "    log(\"{sourcename} -> {page}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec5ab0",
   "metadata": {},
   "source": [
    "# Extract news URLs from saved pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4d797e-1d72-46a5-9242-1877c5754bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 15:09:15,875 - AInewsbot - INFO - Parsing html files\n",
      "2024-07-09 15:09:15,877 - AInewsbot - INFO - WaPo Tech -> htmldata/Technology - The Washington Post (07_09_2024 11_21_39 AM).html\n",
      "2024-07-09 15:09:15,878 - AInewsbot - INFO - parse loop - WaPo Tech\n",
      "2024-07-09 15:09:15,907 - AInewsbot - INFO - parse_file - found 160 raw links\n",
      "2024-07-09 15:09:15,911 - AInewsbot - INFO - parse_file - found 20 filtered links\n",
      "2024-07-09 15:09:15,912 - AInewsbot - INFO - parse loop - 20 links found\n",
      "2024-07-09 15:09:15,912 - AInewsbot - INFO - WSJ Tech -> htmldata/Technology - WSJ.com (07_09_2024 11_21_34 AM).html\n",
      "2024-07-09 15:09:15,912 - AInewsbot - INFO - parse loop - WSJ Tech\n",
      "2024-07-09 15:09:15,954 - AInewsbot - INFO - parse_file - found 512 raw links\n",
      "2024-07-09 15:09:15,961 - AInewsbot - INFO - parse_file - found 8 filtered links\n",
      "2024-07-09 15:09:15,961 - AInewsbot - INFO - parse loop - 8 links found\n",
      "2024-07-09 15:09:15,961 - AInewsbot - INFO - VentureBeat -> htmldata/AI News _ VentureBeat (07_09_2024 11_21_32 AM).html\n",
      "2024-07-09 15:09:15,961 - AInewsbot - INFO - parse loop - VentureBeat\n",
      "2024-07-09 15:09:15,977 - AInewsbot - INFO - parse_file - found 326 raw links\n",
      "2024-07-09 15:09:15,981 - AInewsbot - INFO - parse_file - found 44 filtered links\n",
      "2024-07-09 15:09:15,981 - AInewsbot - INFO - parse loop - 44 links found\n",
      "2024-07-09 15:09:15,982 - AInewsbot - INFO - Feedly AI -> htmldata/Discover and Add New Feedly AI Feeds (07_09_2024 11_21_28 AM).html\n",
      "2024-07-09 15:09:15,982 - AInewsbot - INFO - parse loop - Feedly AI\n",
      "2024-07-09 15:09:16,009 - AInewsbot - INFO - parse_file - found 256 raw links\n",
      "2024-07-09 15:09:16,013 - AInewsbot - INFO - parse_file - found 65 filtered links\n",
      "2024-07-09 15:09:16,013 - AInewsbot - INFO - parse loop - 65 links found\n",
      "2024-07-09 15:09:16,013 - AInewsbot - INFO - Reddit -> htmldata/top scoring links _ multi (07_09_2024 11_21_22 AM).html\n",
      "2024-07-09 15:09:16,014 - AInewsbot - INFO - parse loop - Reddit\n",
      "2024-07-09 15:09:16,149 - AInewsbot - INFO - parse_file - found 567 raw links\n",
      "2024-07-09 15:09:16,159 - AInewsbot - INFO - parse_file - found 373 filtered links\n",
      "2024-07-09 15:09:16,160 - AInewsbot - INFO - parse loop - 373 links found\n",
      "2024-07-09 15:09:16,160 - AInewsbot - INFO - The Verge -> htmldata/Artificial Intelligence - The Verge (07_09_2024 11_21_21 AM).html\n",
      "2024-07-09 15:09:16,160 - AInewsbot - INFO - parse loop - The Verge\n",
      "2024-07-09 15:09:16,185 - AInewsbot - INFO - parse_file - found 318 raw links\n",
      "2024-07-09 15:09:16,189 - AInewsbot - INFO - parse_file - found 34 filtered links\n",
      "2024-07-09 15:09:16,189 - AInewsbot - INFO - parse loop - 34 links found\n",
      "2024-07-09 15:09:16,190 - AInewsbot - INFO - The Register -> htmldata/The Register_ Enterprise Technology News and Analysis (07_09_2024 11_21_10 AM).html\n",
      "2024-07-09 15:09:16,190 - AInewsbot - INFO - parse loop - The Register\n",
      "2024-07-09 15:09:16,205 - AInewsbot - INFO - parse_file - found 202 raw links\n",
      "2024-07-09 15:09:16,209 - AInewsbot - INFO - parse_file - found 90 filtered links\n",
      "2024-07-09 15:09:16,209 - AInewsbot - INFO - parse loop - 90 links found\n",
      "2024-07-09 15:09:16,209 - AInewsbot - INFO - Techmeme -> htmldata/Techmeme (07_09_2024 11_21_00 AM).html\n",
      "2024-07-09 15:09:16,210 - AInewsbot - INFO - parse loop - Techmeme\n",
      "2024-07-09 15:09:16,218 - AInewsbot - INFO - parse_file - found 186 raw links\n",
      "2024-07-09 15:09:16,221 - AInewsbot - INFO - parse_file - found 63 filtered links\n",
      "2024-07-09 15:09:16,221 - AInewsbot - INFO - parse loop - 63 links found\n",
      "2024-07-09 15:09:16,221 - AInewsbot - INFO - NYT Tech -> htmldata/Technology - The New York Times (07_09_2024 11_20_49 AM).html\n",
      "2024-07-09 15:09:16,221 - AInewsbot - INFO - parse loop - NYT Tech\n",
      "2024-07-09 15:09:16,232 - AInewsbot - INFO - parse_file - found 75 raw links\n",
      "2024-07-09 15:09:16,234 - AInewsbot - INFO - parse_file - found 19 filtered links\n",
      "2024-07-09 15:09:16,234 - AInewsbot - INFO - parse loop - 19 links found\n",
      "2024-07-09 15:09:16,234 - AInewsbot - INFO - HackerNoon -> htmldata/HackerNoon - read, write and learn about any technology (07_09_2024 11_20_48 AM).html\n",
      "2024-07-09 15:09:16,234 - AInewsbot - INFO - parse loop - HackerNoon\n",
      "2024-07-09 15:09:16,286 - AInewsbot - INFO - parse_file - found 569 raw links\n",
      "2024-07-09 15:09:16,294 - AInewsbot - INFO - parse_file - found 92 filtered links\n",
      "2024-07-09 15:09:16,294 - AInewsbot - INFO - parse loop - 92 links found\n",
      "2024-07-09 15:09:16,294 - AInewsbot - INFO - Google News -> htmldata/Google News - Technology - Artificial intelligence (07_09_2024 11_20_38 AM).html\n",
      "2024-07-09 15:09:16,294 - AInewsbot - INFO - parse loop - Google News\n",
      "2024-07-09 15:09:16,585 - AInewsbot - INFO - parse_file - found 988 raw links\n",
      "2024-07-09 15:09:16,592 - AInewsbot - INFO - parse_file - found 423 filtered links\n",
      "2024-07-09 15:09:16,592 - AInewsbot - INFO - parse loop - 423 links found\n",
      "2024-07-09 15:09:16,592 - AInewsbot - INFO - Hacker News 2 -> htmldata/Hacker News Page 2 (07_09_2024 11_20_37 AM).html\n",
      "2024-07-09 15:09:16,593 - AInewsbot - INFO - parse loop - Hacker News 2\n",
      "2024-07-09 15:09:16,603 - AInewsbot - INFO - parse_file - found 257 raw links\n",
      "2024-07-09 15:09:16,606 - AInewsbot - INFO - parse_file - found 25 filtered links\n",
      "2024-07-09 15:09:16,606 - AInewsbot - INFO - parse loop - 25 links found\n",
      "2024-07-09 15:09:16,606 - AInewsbot - INFO - Hacker News -> htmldata/Hacker News Page 1 (07_09_2024 11_20_26 AM).html\n",
      "2024-07-09 15:09:16,606 - AInewsbot - INFO - parse loop - Hacker News\n",
      "2024-07-09 15:09:16,618 - AInewsbot - INFO - parse_file - found 261 raw links\n",
      "2024-07-09 15:09:16,620 - AInewsbot - INFO - parse_file - found 25 filtered links\n",
      "2024-07-09 15:09:16,621 - AInewsbot - INFO - parse loop - 25 links found\n",
      "2024-07-09 15:09:16,621 - AInewsbot - INFO - FT Tech -> htmldata/Technology (07_09_2024 11_20_16 AM).html\n",
      "2024-07-09 15:09:16,621 - AInewsbot - INFO - parse loop - FT Tech\n",
      "2024-07-09 15:09:16,648 - AInewsbot - INFO - parse_file - found 460 raw links\n",
      "2024-07-09 15:09:16,653 - AInewsbot - INFO - parse_file - found 103 filtered links\n",
      "2024-07-09 15:09:16,653 - AInewsbot - INFO - parse loop - 103 links found\n",
      "2024-07-09 15:09:16,653 - AInewsbot - INFO - Ars Technica -> htmldata/Ars Technica (07_09_2024 11_20_04 AM).html\n",
      "2024-07-09 15:09:16,654 - AInewsbot - INFO - parse loop - Ars Technica\n",
      "2024-07-09 15:09:16,668 - AInewsbot - INFO - parse_file - found 252 raw links\n",
      "2024-07-09 15:09:16,670 - AInewsbot - INFO - parse_file - found 31 filtered links\n",
      "2024-07-09 15:09:16,670 - AInewsbot - INFO - parse loop - 31 links found\n",
      "2024-07-09 15:09:16,670 - AInewsbot - INFO - Bloomberg Tech -> htmldata/Bloomberg Technology - Bloomberg (07_09_2024 11_20_04 AM).html\n",
      "2024-07-09 15:09:16,671 - AInewsbot - INFO - parse loop - Bloomberg Tech\n",
      "2024-07-09 15:09:16,689 - AInewsbot - INFO - parse_file - found 294 raw links\n",
      "2024-07-09 15:09:16,693 - AInewsbot - INFO - parse_file - found 51 filtered links\n",
      "2024-07-09 15:09:16,693 - AInewsbot - INFO - parse loop - 51 links found\n",
      "2024-07-09 15:09:16,693 - AInewsbot - INFO - Business Insider -> htmldata/Tech - Business Insider (07_09_2024 11_20_04 AM).html\n",
      "2024-07-09 15:09:16,693 - AInewsbot - INFO - parse loop - Business Insider\n",
      "2024-07-09 15:09:16,714 - AInewsbot - INFO - parse_file - found 310 raw links\n",
      "2024-07-09 15:09:16,718 - AInewsbot - INFO - parse_file - found 50 filtered links\n",
      "2024-07-09 15:09:16,718 - AInewsbot - INFO - parse loop - 50 links found\n",
      "2024-07-09 15:09:16,718 - AInewsbot - INFO - parse loop - found 1516 links\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>src</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>What we know about microdosing candy illnesses...</td>\n",
       "      <td>https://arstechnica.com/science/2024/07/author...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Why 1994’sLair of Squidwas the weirdest pack-i...</td>\n",
       "      <td>https://arstechnica.com/google/2024/07/how-i-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Paul Sutter walks us through the future of cli...</td>\n",
       "      <td>https://arstechnica.com/science/2022/04/paul-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Alaska’s top-heavy glaciers are approaching an...</td>\n",
       "      <td>https://arstechnica.com/science/2024/07/alaska...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ars Technica</td>\n",
       "      <td>Egalitarian oddity found in the Neolithic</td>\n",
       "      <td>https://arstechnica.com/science/2024/07/egalit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           src                                              title  \\\n",
       "0   0  Ars Technica  What we know about microdosing candy illnesses...   \n",
       "1   1  Ars Technica  Why 1994’sLair of Squidwas the weirdest pack-i...   \n",
       "2   2  Ars Technica  Paul Sutter walks us through the future of cli...   \n",
       "3   3  Ars Technica  Alaska’s top-heavy glaciers are approaching an...   \n",
       "4   4  Ars Technica          Egalitarian oddity found in the Neolithic   \n",
       "\n",
       "                                                 url  \n",
       "0  https://arstechnica.com/science/2024/07/author...  \n",
       "1  https://arstechnica.com/google/2024/07/how-i-f...  \n",
       "2  https://arstechnica.com/science/2022/04/paul-s...  \n",
       "3  https://arstechnica.com/science/2024/07/alaska...  \n",
       "4  https://arstechnica.com/science/2024/07/egalit...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse news URLs and titles from downloaded HTML files\n",
    "log(\"Parsing html files\")\n",
    "all_urls = []\n",
    "for sourcename, filename in saved_pages:\n",
    "    log(sourcename +' -> ' + filename)\n",
    "    log(f\"{sourcename}\", \"parse loop\")\n",
    "    links = parse_file(sources[sourcename])\n",
    "    log(f\"{len(links)} links found\", \"parse loop\")\n",
    "    all_urls.extend(links)\n",
    "\n",
    "log(f\"found {len(all_urls)} links\", \"parse loop\")\n",
    "\n",
    "# make a pandas dataframe of all the links found\n",
    "orig_df = (\n",
    "    pd.DataFrame(all_urls)\n",
    "    .groupby(\"url\")\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    .sort_values(\"src\")[[\"src\", \"title\", \"url\"]]\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index(drop=False)\n",
    "    .rename(columns={\"index\": \"id\"})\n",
    ")\n",
    "orig_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27972b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracts all links from history where isAI=1\n",
    "# # useful for training dimensionality reduction\n",
    "# conn = sqlite3.connect('articles.db')\n",
    "# c = conn.cursor()\n",
    "# #  and timestamp > '2024-07-01' \n",
    "# query = \"select * from news_articles where isAI=1 order by id\"\n",
    "# ai_history_df = pd.read_sql_query(query, conn)\n",
    "# ai_history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df42fdb-fed6-4e10-a130-17f22f9a97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # clean up sqlite database if you want to rerun the job from a given point\n",
    "# conn.execute(f\"delete from news_articles where timestamp > '2024-07-08 19:15'\")\n",
    "# # conn.execute(f\"delete from news_articles where id > 220230\")\n",
    "# # Committing the changes\n",
    "# conn.commit()\n",
    "\n",
    "# # Close the connection\n",
    "# conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bbfb65",
   "metadata": {},
   "source": [
    "# Filter URLs to new AI headlines only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f760895-915f-499e-814d-dea389c9a3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 15:18:14,142 - AInewsbot - INFO - Existing URLs: 125015\n",
      "2024-07-09 15:18:14,170 - AInewsbot - INFO - New URLs: 576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "576"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter urls we've already seen in previous runs and saved in SQLite\n",
    "filtered_df = filter_unseen_urls_db(orig_df, before_date='2024-07-09 06:00:00')\n",
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a988ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use chatgpt to filter AI-related headlines using a prompt to OpenAI\n",
    "print(FILTER_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pages that fit in a reasonably sized (MAXPAGELEN or MAX_INPUT_TOKENS) prompt\n",
    "pages = paginate_df(filtered_df)\n",
    "log(f\"Paginated {len(pages)} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39235332-cb60-4c13-8e8a-0e63c91bd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use REST API directly. OpenAI python API doesn't support concurrent requests from a single client\n",
    "# this runs fast with async aiohttp and on gpt-3.5 (15 seconds vs 2 minutes synchronously with gpt-4o)\n",
    "# the old API supported submitting multiple payloads in a single completion request\n",
    "# current API supports a slow 'batch' submission https://platform.openai.com/docs/guides/rate-limits/usage-tiers\n",
    "# there is a more complex example here - https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py\n",
    "\n",
    "log(\"start classify\")\n",
    "enriched_urls = asyncio.run(fetch_pages(pages, prompt=FILTER_PROMPT))\n",
    "log(\"end classify\")\n",
    "\n",
    "enriched_df = pd.DataFrame(enriched_urls)\n",
    "print(len(enriched_df))\n",
    "log(\"isAI\", len(enriched_df.loc[enriched_df[\"isAI\"]]))\n",
    "log(\"not isAI\", len(enriched_df.loc[~enriched_df[\"isAI\"]]))\n",
    "enriched_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36ce69-2da0-4c85-a98f-760c7b7c6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge returned df with isAI column into original df on id column\n",
    "merged_df = pd.merge(filtered_df, enriched_df, on=\"id\", how=\"outer\")\n",
    "merged_df['date'] = datetime.now().date()\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286de30-81e7-4bab-a7fa-9477f27b6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be empty, shouldn't get back rows that don't match to existing\n",
    "log(f\"Unmatched response rows: {len(merged_df.loc[merged_df['src'].isna()])}\")\n",
    "# should be empty, should get back all rows from orig\n",
    "log(f\"Unmatched source rows: {len(merged_df.loc[merged_df['isAI'].isna()])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ead65-a4f6-4b25-97a1-e14aec1d2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update SQLite database with all seen URLs\n",
    "conn = sqlite3.connect('articles.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for row in merged_df.itertuples():\n",
    "    insert_article(conn, cursor, row.src, row.title,\n",
    "                   row.url, row.isAI, row.date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5548bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep headlines that are related to AI\n",
    "AIdf = merged_df.loc[merged_df[\"isAI\"]==1] \\\n",
    "    .reset_index(drop=True)  \\\n",
    "    .reset_index()  \\\n",
    "    .drop(columns=[\"id\"])  \\\n",
    "    .rename(columns={'index': 'id'})\n",
    "\n",
    "log(f\"Found {len(AIdf)} AI headlines\")\n",
    "AIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323512b-727a-4bcb-b74e-0577044e757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map title to ascii characters to avoid some dupes with e.g. different quote symbols\n",
    "\n",
    "def unicode_to_ascii(input_string):\n",
    "    # Normalize the Unicode string to NFKD form\n",
    "    normalized_string = unicodedata.normalize('NFKD', input_string)\n",
    "    \n",
    "    # Encode to ASCII bytes, ignoring characters that cannot be converted\n",
    "    ascii_bytes = normalized_string.encode('ascii', 'ignore')\n",
    "    \n",
    "    # Convert bytes back to a string\n",
    "    ascii_string = ascii_bytes.decode('ascii')\n",
    "    \n",
    "    return ascii_string\n",
    "\n",
    "AIdf['title'] = AIdf['title'].apply(unicode_to_ascii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28a38b-d93d-43d8-9e24-c6a107538f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedupe identical headlines\n",
    "AIdf['title_clean'] = AIdf['title'].map(lambda s: \"\".join(s.split()))\n",
    "AIdf = AIdf.sort_values(\"src\") \\\n",
    "    .groupby(\"title_clean\") \\\n",
    "    .first() \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .drop(columns=['id']) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'id'})\n",
    "\n",
    "log(f\"Found {len(AIdf)} unique AI headlines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60816eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map google news headlines to redirect, kind of unnecessary\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "redirect_dict = {}\n",
    "for row in AIdf.itertuples():\n",
    "    parsed_url = urlparse(row.url)\n",
    "    netloc = parsed_url.netloc\n",
    "    if netloc == 'news.google.com':\n",
    "        print(netloc, end=\" -> \")        \n",
    "        response = requests.get(row.url, allow_redirects=False)\n",
    "        # The URL to which it would have redirected\n",
    "        redirect_url = response.headers.get('Location')\n",
    "        redirect_dict[row.url] = redirect_url\n",
    "        parsed_url2 = urlparse(redirect_url)\n",
    "        netloc2 = parsed_url2.netloc\n",
    "        if netloc2 == 'news.google.com':\n",
    "            print(netloc2, end=\" -> \")\n",
    "            response = requests.get(redirect_url, allow_redirects=False)\n",
    "        # The URL to which it would have redirected\n",
    "            redirect_url = response.headers.get('Location')\n",
    "            redirect_dict[row.url] = redirect_url\n",
    "        print(redirect_url)\n",
    "\n",
    "AIdf['url'] = AIdf['url'].apply(lambda url: redirect_dict.get(url, url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9078abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a733fe",
   "metadata": {},
   "source": [
    "# Topic analysis\n",
    "Here we are trying to identify the top topics of the day, to help make a nice summary. \n",
    "\n",
    "1st approach - do dimensionality reduction on the headline embeddings with UMAP and cluster with DBSCAN.\n",
    "\n",
    "2nd approach\n",
    " - extract topics from headline using a prompt\n",
    " - human canonicalizes topics\n",
    " - assign headlines to topics using a prompt\n",
    " \n",
    " The final summary is pretty inconsistent, would be nice to give chatgpt a prompt that would say, summarize these bullet points using this categorization.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to extract top topics \n",
    "print(TOPIC_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topics\n",
    "pages = paginate_df(AIdf)\n",
    "\n",
    "# apply this prompt to AI headlines\n",
    "log(\"start topic extraction\")\n",
    "response = asyncio.run(fetch_pages(pages, prompt=TOPIC_PROMPT))\n",
    "log(\"end topic extraction\")\n",
    "\n",
    "topic_df = pd.DataFrame(response)\n",
    "print(len(topic_df))\n",
    "topic_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = [item for row in topic_df.itertuples() for item in row.topics]\n",
    "item_counts = Counter(all_topics)\n",
    "for x in item_counts.most_common():\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evergreen topics to hopefully map healdines to canonical standardized topics\n",
    "# review extracted topics and add\n",
    "CANONICAL_TOPICS = [\n",
    "    \"Policy and regulation\",\n",
    "    \"AI economic impacts\",\n",
    "    \"Robots\",\n",
    "    \"Autonomous vehicles\",\n",
    "    \"AI job market\",\n",
    "    \"LLMs\",\n",
    "    \"Healthcare\",\n",
    "    \"Fintech\",\n",
    "    \"Education\",\n",
    "    \"Entertainment\",\n",
    "    \"Startup funding\",\n",
    "    \"IPOs\",\n",
    "    \"Ethical issues\",\n",
    "    \"Legal issues\",\n",
    "    \"Cybersecurity\",\n",
    "    \"AI doom\",\n",
    "    'Stocks',\n",
    "    'Climate',\n",
    "    'Scams',\n",
    "    'Privacy',\n",
    "    'Intellectual Property',\n",
    "    'Code assistants',\n",
    "    'Customer service',\n",
    "    'Reinforcement Learning',\n",
    "    'Open Source',\n",
    "    'Language Models',\n",
    "    'China',\n",
    "    'Military',\n",
    "    'Semiconductor chips',\n",
    "    'Sustainability',\n",
    "    'Agriculture',\n",
    "    'Gen AI',\n",
    "    'Testing',\n",
    "    \n",
    "    'Nvidia',\n",
    "    'Google',\n",
    "    'OpenAI',\n",
    "    'Meta',\n",
    "    'Apple',\n",
    "    'Microsoft',\n",
    "    'Salesforce',\n",
    "    'Uber',\n",
    "    'AMD',\n",
    "    'Netflix',\n",
    "    'Disney',\n",
    "    'Amazon',\n",
    "    'Cloudflare',\n",
    "    'Anthropic',\n",
    "    'Cohere',\n",
    "    'Baidu',\n",
    "    'Big Tech',\n",
    "    'Samsung',\n",
    "    'Tesla',\n",
    "    \n",
    "    'ChatGPT',\n",
    "    'WhatsApp',\n",
    "    'Gemini',\n",
    "    'Claude',\n",
    "    'Copilot',\n",
    "    \n",
    "    'Elon Musk',\n",
    "    'Bill Gates',\n",
    "    'Sam Altman',\n",
    "    'Mustafa Suleyman',\n",
    "    'Sundar Pichai',\n",
    "    'Yann LeCun',\n",
    "    'Geoffrey Hinton',\n",
    "    'Mark Zuckerberg',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86688034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could try it with new cats or new cats + evergreen\n",
    "# but probably look at new cats and human in the loop should add good new cats today to evergreen list\n",
    "# new_cats = list(json.loads(response.choices[0].message.content).values())[0]\n",
    "# categories = sorted(list(set(new_cats + evergreen)))\n",
    "categories = sorted(CANONICAL_TOPICS)\n",
    "categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def categorize_story(headline, categories, session, \n",
    "                           model=LOWCOST_MODEL,\n",
    "                           temperature=0.5,\n",
    "                           max_retries=MAX_RETRIES):\n",
    "    \n",
    "    retlist = []\n",
    "    if type(categories) is not list:\n",
    "        categories = [categories]\n",
    "    for topic in categories:\n",
    "        cat_prompt = f\"\"\"You are a news topic categorizaton assistant. I will provide a headline \n",
    "and a topic. You will respond with a JSON object {{'response': 1}} if the news headline matches \n",
    "the news topic and {{'response': 0}} if it does not. Check carefully and only return {{'response': 1}}\n",
    "if the headline closely matches the topic. If the headline is not a close match or if unsure, \n",
    "return {{'response': 0}}\n",
    "Headline:\n",
    "{headline}\n",
    "Topic:\n",
    "{topic}\n",
    "\"\"\"\n",
    "        for i in range(max_retries):\n",
    "            try:\n",
    "                messages=[\n",
    "                          {\"role\": \"user\", \"content\": cat_prompt\n",
    "                          }]\n",
    "\n",
    "                payload = {\"model\":  model,\n",
    "                           \"response_format\": {\"type\": \"json_object\"},\n",
    "                           \"messages\": messages,\n",
    "                           \"temperature\": temperature\n",
    "                           }\n",
    "                response = await fetch_openai(session, payload)\n",
    "                response_dict = json.loads(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "                response_val = response_dict['response']\n",
    "                if response_val == 1:\n",
    "                    retlist.append(topic)\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                log(f\"Error: {exc}\")\n",
    "\n",
    "            \n",
    "    return retlist\n",
    "        \n",
    "\n",
    "h = \"Utility stocks are Wall Streets secret backdoor to AI\"\n",
    "catdict = dict()\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    for i, row in enumerate(AIdf.itertuples()):\n",
    "        tasks = []\n",
    "        log(f\"Categorizing headline {row.id+1} of {len(AIdf)}\")\n",
    "        h = row.title\n",
    "        log(h)\n",
    "        for c in categories:\n",
    "            task = asyncio.create_task(categorize_story(h, c, session))\n",
    "            tasks.append(task)\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        catdict[row.id] = [item for sublist in responses for item in sublist]\n",
    "        log(str(catdict[row.id]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "catdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c42045",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf = AIdf.drop(columns=[\"assigned_topics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df[\"assigned_topics\"] = topic_df[\"id\"].apply(lambda id: catdict.get(id, []))\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ee805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_topics(row):\n",
    "    topics = [x.title() for x in row.topics if x.lower() not in {\"ai\", \"artificial intelligence\"}]\n",
    "    assigned_topics = [x.title() for x in row.assigned_topics]\n",
    "    combined = sorted(list(set(topics + assigned_topics)))\n",
    "    combined = [s.replace(\"Ai\", \"AI\") for s in combined]\n",
    "    combined = [s.replace(\"Genai\", \"Gen AI\") for s in combined]\n",
    "    \n",
    "    return \", \".join(combined)\n",
    "\n",
    "topic_df[\"clean_topics\"] = topic_df.apply(clean_topics, axis=1)\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa602bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge returned df into original df\n",
    "merged_df = pd.merge(AIdf, topic_df[[\"id\", \"topic_str\"]], on=\"id\", how=\"outer\")\n",
    "merged_df['title_topic_str'] = merged_df.apply(lambda row: f'{row.title} (Topics: {row.topic_str})', axis=1)\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f269ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf = merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6748df0",
   "metadata": {},
   "source": [
    "### Semantic sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fa9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use embeddings to sort headlines by semantical similarity\n",
    "log(f\"Fetching embeddings for {len(AIdf)} headlines\")\n",
    "embedding_model = 'text-embedding-3-large'\n",
    "response = client.embeddings.create(input=AIdf['title_topic_str'].tolist(),\n",
    "                                    model=embedding_model)\n",
    "embedding_df = pd.DataFrame([e.model_dump()['embedding'] for e in response.data])\n",
    "\n",
    "sorted_indices = agglomerative_cluster_sort(embedding_df)\n",
    "AIdf = AIdf.iloc[sorted_indices] \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .reset_index() \\\n",
    "    .drop(columns=[\"id\"]) \\\n",
    "    .rename(columns={'index': 'id'})\n",
    "\n",
    "# sort embedding_df to match\n",
    "embedding_df = embedding_df[sorted_indices]\n",
    "\n",
    "AIdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17941fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    display(AIdf[[\"title\"]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa44e52",
   "metadata": {},
   "source": [
    "### Cluster with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = 'text-embedding-3-large'\n",
    "# chunksize = 1000\n",
    "# e_results = []\n",
    "# for start in range(0, len(ai_history_df), chunksize):\n",
    "#     tempdf = ai_history_df.iloc[start:start+chunksize]\n",
    "#     templist = tempdf['title'].tolist()\n",
    "#     log(f\"Fetching embeddings for {len(templist)} headlines starting at row {start}\")\n",
    "#     response = client.embeddings.create(input=templist,\n",
    "#                                         model=embedding_model)\n",
    "#     e_results.append(pd.DataFrame([e.model_dump()['embedding'] for e in response.data]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical_embedding_df = pd.concat(e_results)\n",
    "# historical_embedding_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2053391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical_embedding_df.to_pickle('historical_embedding_df.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cea3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(f\"Fetching embeddings for {len(AIdf)} headlines\")\n",
    "embedding_model = 'text-embedding-3-large'\n",
    "response = client.embeddings.create(input=AIdf['title'].tolist(),\n",
    "                                    model=embedding_model)\n",
    "embedding_df = pd.DataFrame([e.model_dump()['embedding'] for e in response.data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c19196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducer = umap.UMAP(n_components=30)  # Reducing to 30 dimensions\n",
    "# reduced_data = reducer.fit_transform(historical_embedding_df)\n",
    "# with open('umap_model.pkl', 'wb') as file:\n",
    "#     pickle.dump(reducer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f535eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"umap_model.pkl\", 'rb') as file:\n",
    "    # Load the model from the file\n",
    "    reducer = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b9f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = reducer.transform(embedding_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c4ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(reduced_data).any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab237f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a Clustering Algorithm (e.g., K-Means)\n",
    "kmeans = KMeans(n_clusters=20)  \n",
    "clusters = kmeans.fit_predict(reduced_data)\n",
    "\n",
    "# Evaluate the Clustering\n",
    "silhouette_avg = silhouette_score(reduced_data, clusters)\n",
    "print(f'Silhouette Score: {silhouette_avg}')\n",
    "\n",
    "# Visualization with UMAP (optional)\n",
    "# reducer_2d = umap.UMAP(n_components=2)  # Reducing to 2 dimensions for visualization\n",
    "# reduced_data_2d = reducer_2d.fit_transform(embedding_df)\n",
    "\n",
    "# plt.scatter(reduced_data_2d[:, 0], reduced_data_2d[:, 1], c=clusters, cmap='viridis', s=5)\n",
    "# plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "# plt.title('UMAP Projection of the News Headlines Clusters')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.5, min_samples=3)  # Adjust eps and min_samples as needed\n",
    "clusters = dbscan.fit_predict(reduced_data)\n",
    "\n",
    "AIdf['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca92189",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    for i in range(30):\n",
    "        tmpdf = AIdf.loc[AIdf['cluster']==i][[\"id\", \"title\"]]\n",
    "        if len(tmpdf) ==0:\n",
    "            break\n",
    "        display(tmpdf)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cc4ee",
   "metadata": {},
   "source": [
    "# Save and email headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12047601-67d0-4e60-b1e7-79d38349b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_str = \"\"\n",
    "for row in AIdf.itertuples():\n",
    "    log(f\"[{row.Index}. {row.title} - {row.src}]({row.url})\")\n",
    "    html_str += f'{row.Index}.<a href=\"{row.url}\">{row.title} - {row.src}</a><br />\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save headlines\n",
    "with open('headlines.html', 'w') as f:\n",
    "    f.write(html_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send mail\n",
    "log(\"Sending headlines email\")\n",
    "subject = f'AI headlines {datetime.now().strftime(\"%H:%M:%S\")}'\n",
    "send_gmail(subject, html_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0768a",
   "metadata": {},
   "source": [
    "# Save individual pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch pages\n",
    "# Create a queue for multiprocessing and populate it \n",
    "log(\"Queuing URLs for scraping\")\n",
    "\n",
    "queue = multiprocessing.Queue()\n",
    "for row in AIdf.itertuples():\n",
    "    queue.put((row.id, row.url, row.title))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe930e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape urls in queue asynchronously\n",
    "num_browsers = 4\n",
    "\n",
    "callable = process_url_queue_factory(queue)\n",
    "\n",
    "log(f\"fetching {len(AIdf)} pages using {num_browsers} browsers\")\n",
    "saved_pages = launch_drivers(num_browsers, callable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = pd.DataFrame(saved_pages)\n",
    "pages_df.columns = ['id', 'url', 'title', 'path']\n",
    "pages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56202217",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf = pd.merge(AIdf, pages_df[[\"id\", \"path\"]], on='id', how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ada9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5793763",
   "metadata": {},
   "source": [
    "# Summarize individual pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SUMMARIZE_SYSTEM_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ccf2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SUMMARIZE_USER_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are fetching all at once, could be 200 summaries, so we are firing off 200 REST requests at once\n",
    "# This seems like a bad idea, could loop through and fire off e.g. 10 at a time, or use queues and workers (seems pointless)\n",
    "# But it works and runs fast on 3.5 and if ChatGPT doesn't like it they could throttle it\n",
    "\n",
    "log(\"Starting summarize\")\n",
    "responses = await fetch_all_summaries(AIdf)\n",
    "log(f\"Received {len(responses)} summaries\")\n",
    "print(responses[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring summaries into dict\n",
    "response_dict = {}\n",
    "for i, response in responses:\n",
    "    try:\n",
    "        response_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        response_dict[i] = response_str\n",
    "    except Exception as exc:\n",
    "        print(exc)\n",
    "        \n",
    "len(response_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91263777",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ''\n",
    "\n",
    "for i, row in enumerate(AIdf.itertuples()):\n",
    "    mdstr = f\"[{i+1}. {row.title} - {row.src}]({row.url})  \\n\\n {row.topic_str} \\n\\n{response_dict[row.id]} \\n\\n\"\n",
    "    display(Markdown(mdstr))\n",
    "    markdown_str += mdstr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe07a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Markdown to HTML\n",
    "html_str = markdown.markdown(markdown_str, extensions=['extra'])\n",
    "# display(HTML(html_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165758da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bullets\n",
    "with open('bullets.md', 'w') as f:\n",
    "    f.write(markdown_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Sending bullet points email\")\n",
    "subject = f'AI news bullets {datetime.now().strftime(\"%H:%M:%S\")}'\n",
    "send_gmail(subject, html_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dffcde",
   "metadata": {},
   "source": [
    "# Ask ChatGPT for top categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOP_CATEGORIES_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "              {\"role\": \"user\", \"content\": TOP_CATEGORIES_PROMPT + markdown_str\n",
    "              }],\n",
    "    n=1,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22965c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(json.loads(response.choices[0].message.content).values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e426c2c",
   "metadata": {},
   "source": [
    "# Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_str = ''\n",
    "for i, row in enumerate(AIdf.itertuples()):\n",
    "    mdstr = f\"[{i+1}. {row.title} - {row.src}]({row.url})  \\n\\n\"\n",
    "    if 0 < len(catdict[row.id]) < 11 :\n",
    "        topicstr = \", \".join(catdict[row.id])\n",
    "        mdstr += f\"Topics: {topicstr}\\n\\n\"\n",
    "    mdstr += f\"{response_dict[row.id]} \\n\\n\"\n",
    "    display(Markdown(mdstr))\n",
    "    markdown_str += mdstr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_SUMMARY_PROMPT = f\"\"\"You are a summarization assistant. I will provide a list of today's news articlds about AI\n",
    "and summary bullet points in markdown format. Bullet points will have a title and URL, a list of topics discussed, \n",
    "and a bullet-point summary of the article. You are tasked with identifying and summarizing the key themes,\n",
    "common facts, and recurring elements. Your goal is to create a concise summary containing about 20 of the most \n",
    "frequently mentioned topics and developments.\n",
    "\n",
    "\n",
    "Example Input Bullet Points:\n",
    "\n",
    "[2. Sentient closes $85M seed round for open-source AI](https://cointelegraph.com/news/sentient-85-million-round-open-source-ai)\n",
    "\n",
    "AI startup funding, New AI products\n",
    "\n",
    "- Sentient secured $85 million in a seed funding round led by Peter Thiel's Founders Fund, Pantera Capital, and Framework Ventures for their open-source AI platform.\n",
    "- The startup aims to incentivize AI developers with its blockchain protocol and incentive mechanism, allowing for the evolution of open artificial general intelligence.\n",
    "- The tech industry is witnessing a rise in decentralized AI startups combining blockchain\n",
    "\n",
    "Examples of important stories:\n",
    "\n",
    "Major investments and funding rounds\n",
    "Key technological advancements or breakthroughs\n",
    "Frequently mentioned companies, organizations, or figures\n",
    "Notable statements by AI leaders\n",
    "Any other recurring themes or notable patterns\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Read the summary bullet points closely.\n",
    "Use only information provided in them and provide the most common facts without commentary or elaboration.\n",
    "Write in the professional but engaging, narrative style of a tech reporter for a national publication.\n",
    "Be balanced, professional, informative, providing accurate, clear, concise summaries in a respectful neutral tone.\n",
    "Focus on the most common elements across the bullet points and group similar items together.\n",
    "Headers must be as short and simple as possible: use \"Health Care\" and not \"AI developments in Health Care\" or \"AI in Health Care\"\n",
    "Ensure that you provide at least one link from the provided text for each item in the summary.\n",
    "You must include at least 10 and no more than 25 items in the summary.\n",
    "\n",
    "Example Output Format:\n",
    "\n",
    "# Today's AI News\n",
    "\n",
    "### Security and Privacy:\n",
    "- ChatGPT Mac app had a security flaw exposing user conversations in plain text. ([Macworld](https://www.macworld.com/article/2386267/chatgpt-mac-sandboxing-security-flaw-apple-intelligence.html))\n",
    "- Brazil suspended Meta from using Instagram and Facebook posts for AI training over privacy concerns. ([BBC](https://www.bbc.com/news/articles/c7291l3nvwv))\n",
    "\n",
    "### Health Care:\n",
    "- AI can predict Alzheimer's disease with 70% accuracy up to seven years in advance. ([Decrypt](https://decrypt.co/238449/ai-alzheimers-detection-70-percent-accurate-study))\n",
    "- New AI system detects 13 cancers with 98% accuracy, revolutionizing cancer diagnosis. ([India Express](https://news.google.com/articles/CBMiiAFodHRwczovL2luZGlhbmV4cHJl))\n",
    "\n",
    "Bullet Points to Summarize:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0726c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "              {\"role\": \"user\", \"content\": FINAL_SUMMARY_PROMPT + markdown_str\n",
    "              }],\n",
    "    n=1,   \n",
    "    temperature=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dacf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_str = response.choices[0].message.content\n",
    "response_str = response_str.replace(\"$\", \"\\\\$\")\n",
    "display(Markdown(response_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044beddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Sending full summary email \")\n",
    "subject = f'AI news summary {datetime.now().strftime(\"%H:%M:%S\")}'\n",
    "final_html_str = markdown.markdown(response_str, extensions=['extra'])\n",
    "display(HTML(final_html_str))\n",
    "send_gmail(subject, final_html_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a637f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742b179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainewsbot",
   "language": "python",
   "name": "ainewsbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
