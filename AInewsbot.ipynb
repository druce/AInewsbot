{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd15f6b8-26b9-468c-8427-9de836b240fd",
   "metadata": {},
   "source": [
    "# AInewsbot.ipynb - Automate collecting daily AI news\n",
    "\n",
    "1. initial scrape of front pages of tech sites\n",
    "  - Open URLs of news sites specififed in `sources` dict (sources.yaml) using Selenium and Firefox\n",
    "  - Save HTML of each URL in htmldata directory  \n",
    "  - Extract URLs from all files, create a pandas dataframe with url, title, src\n",
    "\n",
    "2. Filter and clean to AI-related headlines not seen before\n",
    "  - Use ChatGPT prompt to filter only AI-related headlines by sending a prompt and formatted table of headlines\n",
    "  - Use SQLite to filter headlines previously seen \n",
    "  - remove duplicate URLs and headlines\n",
    "  - ensure there are pretty source names for each news site\n",
    "\n",
    "3. Topic analysis, make a list of topics for each headline\n",
    "  - using a prompt, check each headline against a number of evergreen AI topics, e.g. deepfakes, regulation, AI in education\n",
    "  - extract free from topics from each headline\n",
    "  - combine topics into topic list for each headline\n",
    "  - cluster headlines using dimensionality-reduced embeddings and DBSCAN; ask chatgpt to name each cluster\n",
    "  - sort headlines by doing a traveling salesman shortest traversal in embedding space\n",
    "\n",
    "4. Summarize individual news story pages in 3 bullets using a prompt\n",
    "\n",
    "5. create a large markdown file with all bullet points and topics\n",
    "\n",
    "6. give the markdown file to ChatGPT and ask it to make a list of most popular and import topics of the day\n",
    "\n",
    "7. human should make a list of the day's topics, combining the chatgpt response to the quesion and cluster topics and \n",
    "\n",
    "8. Put summaries in vector store along with metadata. For each topic, retrieve all associated stories and have chatgpt write a digest of those stories in the given format.\n",
    "\n",
    "9. assemble stories into first draft of newsletter for rewriting as necessary\n",
    "\n",
    "todo:\n",
    "\n",
    "use langgraph for final editing workfow\n",
    "1. prompt to edit final copy for dupes, combine similar sections, copy edit\n",
    "2. have a reviewer prompt check if there are any bullet points to move to a different section \n",
    "3. if so have an editor prompt remove them , return to 2. until nothing left to move dupes left\n",
    "4. have a reviewer check each section, identify bullet points that are similar to other bullet points in the section and have identical links. rewrite combining so there is no duplication. \n",
    "5. identify sections that are short or similar to other sections and suggest sections that should be combined them\n",
    "6. have an editor prompt merge short sections, return to 4, until no orphan sections left\n",
    "7. maybe final copy-edit prompt\n",
    "\n",
    "Original, alternative manual workflow to get HTML files if necessary\n",
    "- Use Chrome, open e.g. Tech News bookmark folder, right-click and open all bookmarks in new window\n",
    "- on Google News, make sure switch to AI tab\n",
    "- on Google News, Feedly, Reddit, scroll to additional pages as desired\n",
    "- Use SingleFile extension, 'save all tabs'\n",
    "- Move files to htmldata directory\n",
    "- Run lower part of notebook to process the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9663d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# del sys.modules['ainb_utilities']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7195880-36be-4987-b71f-86590c934d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import yaml\n",
    "import dotenv\n",
    "import sqlite3\n",
    "import unicodedata\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# import bs4\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import trafilatura\n",
    "\n",
    "import multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "import markdown\n",
    "\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "from ainb_const import (DOWNLOAD_DIR, LOWCOST_MODEL, MODEL, CANONICAL_TOPICS,\n",
    "                        SOURCECONFIG, FILTER_PROMPT, TOPIC_PROMPT,\n",
    "                        SUMMARIZE_SYSTEM_PROMPT, SUMMARIZE_USER_PROMPT, FINAL_SUMMARY_PROMPT, TOP_CATEGORIES_PROMPT,\n",
    "                        MAX_INPUT_TOKENS, MAX_OUTPUT_TOKENS, MAX_RETRIES, TEMPERATURE)\n",
    "from ainb_utilities import (log, delete_files, filter_unseen_urls_db, insert_article, \n",
    "                            nearest_neighbor_sort, agglomerative_cluster_sort, traveling_salesman_sort_scipy,\n",
    "                            unicode_to_ascii, send_gmail)\n",
    "from ainb_webscrape import (get_driver, quit_drivers, launch_drivers, get_file, get_url, parse_file, \n",
    "                            get_og_tags, get_path_from_url, trimmed_href, process_source_queue_factory, \n",
    "                            process_url_queue_factory, get_google_news_redirects)\n",
    "from ainb_llm import (paginate_df, process_pages, fetch_pages, fetch_openai, fetch_all_summaries, \n",
    "                      fetch_openai_summary, trunc_tokens, categorize_headline)\n",
    "\n",
    "\n",
    "import asyncio\n",
    "# need this to run async in jupyter since it already has an asyncio event loop running\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3427af54",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5625c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "before_date = '2024-07-21 09:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2950cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API module\n",
    "client = OpenAI()\n",
    "\n",
    "# Or can use REST API directly\n",
    "API_URL = 'https://api.openai.com/v1/chat/completions'\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {os.getenv(\"OPENAI_API_KEY\")}',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2324ab9-f769-4a6f-aa16-a30a001ede78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 15:07:13,850 - AInewsbot - INFO - Load 17 sources from sources.yaml\n",
      "2024-07-21 15:07:13,851 - AInewsbot - INFO - Ars Technica -> https://arstechnica.com/ -> Ars Technica.html\n",
      "2024-07-21 15:07:13,851 - AInewsbot - INFO - Bloomberg Tech -> https://www.bloomberg.com/technology -> Bloomberg Technology - Bloomberg.html\n",
      "2024-07-21 15:07:13,851 - AInewsbot - INFO - Business Insider -> https://www.businessinsider.com/tech -> Tech - Business Insider.html\n",
      "2024-07-21 15:07:13,852 - AInewsbot - INFO - FT Tech -> https://www.ft.com/technology -> Technology.html\n",
      "2024-07-21 15:07:13,852 - AInewsbot - INFO - Feedly AI -> https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ -> Discover and Add New Feedly AI Feeds.html\n",
      "2024-07-21 15:07:13,853 - AInewsbot - INFO - Google News -> https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US%3Aen -> Google News - Technology - Artificial intelligence.html\n",
      "2024-07-21 15:07:13,853 - AInewsbot - INFO - Hacker News -> https://news.ycombinator.com/ -> Hacker News Page 1.html\n",
      "2024-07-21 15:07:13,853 - AInewsbot - INFO - Hacker News 2 -> https://news.ycombinator.com/?p=2 -> Hacker News Page 2.html\n",
      "2024-07-21 15:07:13,854 - AInewsbot - INFO - HackerNoon -> https://hackernoon.com/ -> HackerNoon - read, write and learn about any technology.html\n",
      "2024-07-21 15:07:13,854 - AInewsbot - INFO - NYT Tech -> https://www.nytimes.com/section/technology -> Technology - The New York Times.html\n",
      "2024-07-21 15:07:13,854 - AInewsbot - INFO - Reddit -> https://www.reddit.com/r/ChatGPT+ChatGPTCoding+MacOS+MachineLearning+OpenAI+ProgrammerHumor+Windows10+battlestations+buildapc+cordcutters+dataisbeautiful+gadgets+hardware+linux+msp+programming+realtech+software+talesfromtechsupport+tech+technews+technology+techsupportgore+windows/top/?sort=top&t=day -> top scoring links _ multi.html\n",
      "2024-07-21 15:07:13,854 - AInewsbot - INFO - Techmeme -> https://www.techmeme.com/river -> Techmeme.html\n",
      "2024-07-21 15:07:13,855 - AInewsbot - INFO - The Register -> https://www.theregister.com/ -> The Register_ Enterprise Technology News and Analysis.html\n",
      "2024-07-21 15:07:13,855 - AInewsbot - INFO - The Verge -> https://www.theverge.com/ai-artificial-intelligence -> Artificial Intelligence - The Verge.html\n",
      "2024-07-21 15:07:13,855 - AInewsbot - INFO - VentureBeat -> https://venturebeat.com/category/ai/ -> AI News _ VentureBeat.html\n",
      "2024-07-21 15:07:13,856 - AInewsbot - INFO - WSJ Tech -> https://www.wsj.com/tech -> Technology - WSJ.com.html\n",
      "2024-07-21 15:07:13,856 - AInewsbot - INFO - WaPo Tech -> https://www.washingtonpost.com/business/technology/ -> Technology - The Washington Post.html\n",
      "2024-07-21 15:07:13,857 - AInewsbot - INFO - Mapped 17 source page titles to sources\n"
     ]
    }
   ],
   "source": [
    "#  load sources to scrape from sources.yaml\n",
    "with open(SOURCECONFIG, \"r\") as stream:\n",
    "    try:\n",
    "        sources = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "log(f\"Load {len(sources)} sources from {SOURCECONFIG}\")\n",
    "\n",
    "# make a reverse dict to map output file titles to source names\n",
    "sources_reverse = {}\n",
    "for k, v in sources.items():\n",
    "    log(f\"{k} -> {v['url']} -> {v['title']}.html\")\n",
    "    v['sourcename'] = k\n",
    "    # map filename (title) to source name\n",
    "    sources_reverse[v['title']] = k\n",
    "\n",
    "log(f\"Mapped {len(sources_reverse)} source page titles to sources\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85998de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ars Technica': {'include': ['^https://arstechnica.com/(\\\\w+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'Ars Technica',\n",
       "  'url': 'https://arstechnica.com/',\n",
       "  'sourcename': 'Ars Technica'},\n",
       " 'Bloomberg Tech': {'include': ['^https://www.bloomberg.com/news/'],\n",
       "  'title': 'Bloomberg Technology - Bloomberg',\n",
       "  'url': 'https://www.bloomberg.com/technology',\n",
       "  'sourcename': 'Bloomberg Tech'},\n",
       " 'Business Insider': {'exclude': ['^https://www.insider.com',\n",
       "   '^https://www.passionfroot.me'],\n",
       "  'title': 'Tech - Business Insider',\n",
       "  'url': 'https://www.businessinsider.com/tech',\n",
       "  'sourcename': 'Business Insider'},\n",
       " 'FT Tech': {'include': ['https://www.ft.com/content/'],\n",
       "  'title': 'Technology',\n",
       "  'url': 'https://www.ft.com/technology',\n",
       "  'sourcename': 'FT Tech'},\n",
       " 'Feedly AI': {'exclude': ['^https://feedly.com',\n",
       "   '^https://s1.feedly.com',\n",
       "   '^https://blog.feedly.com'],\n",
       "  'scroll': 5,\n",
       "  'initial_sleep': 30,\n",
       "  'title': 'Discover and Add New Feedly AI Feeds',\n",
       "  'url': 'https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ',\n",
       "  'sourcename': 'Feedly AI'},\n",
       " 'Google News': {'click': '//*[@aria-label=\"Artificial intelligence\"]',\n",
       "  'include': ['^https://news.google.com/articles/'],\n",
       "  'scroll': 2,\n",
       "  'title': 'Google News - Technology - Artificial intelligence',\n",
       "  'url': 'https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US%3Aen',\n",
       "  'sourcename': 'Google News'},\n",
       " 'Hacker News': {'exclude': ['https://news.ycombinator.com/',\n",
       "   'https://www.ycombinator.com/'],\n",
       "  'title': 'Hacker News Page 1',\n",
       "  'url': 'https://news.ycombinator.com/',\n",
       "  'minlength': 5,\n",
       "  'sourcename': 'Hacker News'},\n",
       " 'Hacker News 2': {'exclude': ['https://news.ycombinator.com',\n",
       "   'https://www.ycombinator.com'],\n",
       "  'title': 'Hacker News Page 2',\n",
       "  'url': 'https://news.ycombinator.com/?p=2',\n",
       "  'sourcename': 'Hacker News 2'},\n",
       " 'HackerNoon': {'exclude': ['^https://hackernoon.com/$',\n",
       "   '^https://hackernoon.com/c$',\n",
       "   '^https://hackernoon.com/coins$',\n",
       "   '^https://hackernoon.com/companies$',\n",
       "   '^https://hackernoon.com/gallery$',\n",
       "   '^https://hackernoon.com/how-to-gain-followers-and-newsletter-subs-directly-on-hackernoon$',\n",
       "   '^https://hackernoon.com/login$',\n",
       "   '^https://hackernoon.com/reader-boot$',\n",
       "   '^https://hackernoon.com/sitemap.xml$',\n",
       "   '^https://hackernoon.com/startups$',\n",
       "   '^https://hackernoon.com/techbeat$',\n",
       "   '^https://hackernoon.com/why-i-write-on-hacker-noon-nl28335q$',\n",
       "   '^https://hackernoon.com/writer-signup$'],\n",
       "  'title': 'HackerNoon - read, write and learn about any technology',\n",
       "  'url': 'https://hackernoon.com/',\n",
       "  'sourcename': 'HackerNoon'},\n",
       " 'NYT Tech': {'include': ['^https://www.nytimes.com/(\\\\d+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'Technology - The New York Times',\n",
       "  'url': 'https://www.nytimes.com/section/technology',\n",
       "  'sourcename': 'NYT Tech'},\n",
       " 'Reddit': {'exclude': ['^https://chat.reddit.com/',\n",
       "   '^https://i.redd.it/',\n",
       "   '^https://redditblog.com/',\n",
       "   '^https://www.redditinc.com/',\n",
       "   '^https://www.reddithelp.com/',\n",
       "   '^https://itunes.apple.com/',\n",
       "   '^https://play.google.com/'],\n",
       "  'scroll': 2,\n",
       "  'minlength': 8,\n",
       "  'title': 'top scoring links _ multi',\n",
       "  'url': 'https://www.reddit.com/r/ChatGPT+ChatGPTCoding+MacOS+MachineLearning+OpenAI+ProgrammerHumor+Windows10+battlestations+buildapc+cordcutters+dataisbeautiful+gadgets+hardware+linux+msp+programming+realtech+software+talesfromtechsupport+tech+technews+technology+techsupportgore+windows/top/?sort=top&t=day',\n",
       "  'sourcename': 'Reddit'},\n",
       " 'Techmeme': {'exclude': ['^https://www.techmeme.com',\n",
       "   '^https://twitter.com/',\n",
       "   '^https://www.threads.net',\n",
       "   '^https://www.linkedin.com',\n",
       "   '^https://mastodon.social',\n",
       "   '^https://bsky.app'],\n",
       "  'title': 'Techmeme',\n",
       "  'url': 'https://www.techmeme.com/river',\n",
       "  'sourcename': 'Techmeme'},\n",
       " 'The Register': {'include': ['^https://www.theregister.com/(\\\\d+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'The Register_ Enterprise Technology News and Analysis',\n",
       "  'url': 'https://www.theregister.com/',\n",
       "  'sourcename': 'The Register'},\n",
       " 'The Verge': {'include': ['^https://www.theverge.com/(\\\\d+)/(\\\\d+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'Artificial Intelligence - The Verge',\n",
       "  'url': 'https://www.theverge.com/ai-artificial-intelligence',\n",
       "  'sourcename': 'The Verge'},\n",
       " 'VentureBeat': {'title': 'AI News _ VentureBeat',\n",
       "  'url': 'https://venturebeat.com/category/ai/',\n",
       "  'sourcename': 'VentureBeat'},\n",
       " 'WSJ Tech': {'include': ['^https://www.wsj.com/articles/'],\n",
       "  'title': 'Technology - WSJ.com',\n",
       "  'url': 'https://www.wsj.com/tech',\n",
       "  'sourcename': 'WSJ Tech'},\n",
       " 'WaPo Tech': {'include': ['https://www.washingtonpost.com/(\\\\w+)/(\\\\d+)/(\\\\d+)/(\\\\d+)/'],\n",
       "  'title': 'Technology - The Washington Post',\n",
       "  'url': 'https://www.washingtonpost.com/business/technology/',\n",
       "  'sourcename': 'WaPo Tech'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "562f83b1-b4b8-4355-8b8b-4fe028ea2af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ars Technica': 'Ars Technica',\n",
       " 'Bloomberg Technology - Bloomberg': 'Bloomberg Tech',\n",
       " 'Tech - Business Insider': 'Business Insider',\n",
       " 'Technology': 'FT Tech',\n",
       " 'Discover and Add New Feedly AI Feeds': 'Feedly AI',\n",
       " 'Google News - Technology - Artificial intelligence': 'Google News',\n",
       " 'Hacker News Page 1': 'Hacker News',\n",
       " 'Hacker News Page 2': 'Hacker News 2',\n",
       " 'HackerNoon - read, write and learn about any technology': 'HackerNoon',\n",
       " 'Technology - The New York Times': 'NYT Tech',\n",
       " 'top scoring links _ multi': 'Reddit',\n",
       " 'Techmeme': 'Techmeme',\n",
       " 'The Register_ Enterprise Technology News and Analysis': 'The Register',\n",
       " 'Artificial Intelligence - The Verge': 'The Verge',\n",
       " 'AI News _ VentureBeat': 'VentureBeat',\n",
       " 'Technology - WSJ.com': 'WSJ Tech',\n",
       " 'Technology - The Washington Post': 'WaPo Tech'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources_reverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2549cf13-0de6-47d2-b7a9-544b5099ff11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 15:07:20,372 - AInewsbot - INFO - 0\n"
     ]
    }
   ],
   "source": [
    "# determine files already in htmldata directory\n",
    "# List all paths in the directory matching today's date\n",
    "nfiles = 50\n",
    "files = [os.path.join(DOWNLOAD_DIR, file)\n",
    "         for file in os.listdir(DOWNLOAD_DIR)]\n",
    "# Get the current date\n",
    "today = datetime.now()\n",
    "year, month, day = today.year, today.month, today.day\n",
    "datestr = datetime.now().strftime(\"%m_%d_%Y\")\n",
    "\n",
    "# filter files only\n",
    "files = [file for file in files if os.path.isfile(file)]\n",
    "\n",
    "# Sort files by modification time and take top 50\n",
    "files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "file = files[:nfiles]\n",
    "\n",
    "# filter files by with today's date ending in .html\n",
    "files = [\n",
    "    file for file in files if datestr in file and file.endswith(\".html\")]\n",
    "log(len(files))\n",
    "for file in files:\n",
    "    log(file)\n",
    "\n",
    "saved_pages = []\n",
    "for file in files:\n",
    "    filename = os.path.basename(file)\n",
    "    # locate date like '01_14_2024' in filename\n",
    "    position = filename.find(\" (\" + datestr)\n",
    "    basename = filename[:position]\n",
    "    # match to source name\n",
    "    sourcename = sources_reverse.get(basename)\n",
    "    if sourcename is None:\n",
    "        log(f\"Skipping {basename}, no sourcename metadata\")\n",
    "        continue\n",
    "    sources[sourcename]['latest'] = file\n",
    "    saved_pages.append((sourcename, file))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1e48ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 15:07:54,204 - AInewsbot - INFO - 0 files found\n"
     ]
    }
   ],
   "source": [
    "log(f\"{len(files)} files found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e703b",
   "metadata": {},
   "source": [
    "# Fetch and save source pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e0cd934",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'multithreading'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultithreading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool, cpu_count\n\u001b[1;32m      2\u001b[0m cpu_count()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'multithreading'"
     ]
    }
   ],
   "source": [
    "from multithreading import Pool, cpu_count\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d12e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(Pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce443dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 15:11:32,692 - AInewsbot - INFO - Saving HTML files using 4 browsers\n",
      "2024-07-21 15:11:32,699 - AInewsbot - INFO - get_driver - 67021 Initializing webdriver\n",
      "2024-07-21 15:11:32,701 - AInewsbot - INFO - get_driver - 67021 Initializing webdriver\n",
      "2024-07-21 15:11:32,702 - AInewsbot - INFO - get_driver - 67021 Initializing webdriver\n",
      "2024-07-21 15:11:32,702 - AInewsbot - INFO - get_driver - 67021 Initializing webdriver\n",
      "2024-07-21 15:11:53,275 - AInewsbot - INFO - get_driver - Initialized webdriver profile\n",
      "2024-07-21 15:11:53,276 - AInewsbot - INFO - get_driver - Initialized webdriver profile\n",
      "2024-07-21 15:11:53,276 - AInewsbot - INFO - get_driver - Initialized webdriver profile\n",
      "2024-07-21 15:11:53,276 - AInewsbot - INFO - get_driver - Initialized webdriver profile\n",
      "2024-07-21 15:11:53,277 - AInewsbot - INFO - get_driver - Initialized webdriver service\n",
      "2024-07-21 15:11:53,277 - AInewsbot - INFO - get_driver - Initialized webdriver service\n",
      "2024-07-21 15:11:53,278 - AInewsbot - INFO - get_driver - Initialized webdriver service\n",
      "2024-07-21 15:11:53,278 - AInewsbot - INFO - get_driver - Initialized webdriver service\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# initialize drivers\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mnum_browsers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Initialize drivers in parallel\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     drivers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_driver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_browsers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.9/concurrent/futures/_base.py:609\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.9/concurrent/futures/_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.9/concurrent/futures/_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.9/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/projects/AInewsbot/ainb_webscrape.py:181\u001b[0m, in \u001b[0;36mget_driver\u001b[0;34m(geckodriver_path, firefox_profile_path)\u001b[0m\n\u001b[1;32m    178\u001b[0m log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized webdriver service\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# Set up the Firefox driver\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFirefox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mservice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized webdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m driver\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.9/site-packages/selenium/webdriver/firefox/webdriver.py:59\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[1;32m     57\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m \u001b[43mDriverFinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mservice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     62\u001b[0m executor \u001b[38;5;241m=\u001b[39m FirefoxRemoteConnection(\n\u001b[1;32m     63\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[1;32m     64\u001b[0m     keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[1;32m     65\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.9/site-packages/selenium/webdriver/common/driver_finder.py:43\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[0;34m(service, options)\u001b[0m\n\u001b[1;32m     40\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using Selenium Manager.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate or obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.9/pathlib.py:1082\u001b[0m, in \u001b[0;36mPath.__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m Path:\n\u001b[1;32m   1081\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m WindowsPath \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PosixPath\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_parts\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flavour\u001b[38;5;241m.\u001b[39mis_supported:\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1085\u001b[0m                               \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.9/pathlib.py:707\u001b[0m, in \u001b[0;36mPurePath._from_parts\u001b[0;34m(cls, args, init)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_parts\u001b[39m(\u001b[38;5;28mcls\u001b[39m, args, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# We need to call _parse_args on the instance, so as to get the\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# right flavour.\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 707\u001b[0m     drv, root, parts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drv \u001b[38;5;241m=\u001b[39m drv\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root \u001b[38;5;241m=\u001b[39m root\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ainewsbot/lib/python3.9/pathlib.py:691\u001b[0m, in \u001b[0;36mPurePath._parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    689\u001b[0m     parts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39m_parts\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;66;03m# Force-cast str subclasses to str (issue #21127)\u001b[39;00m\n\u001b[1;32m    694\u001b[0m         parts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m(a))\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not int"
     ]
    }
   ],
   "source": [
    "# Fetch HTML files from sources\n",
    "\n",
    "# empty download directory\n",
    "delete_files(DOWNLOAD_DIR)\n",
    "\n",
    "# save each file specified from sources\n",
    "num_browsers = 4\n",
    "log(f\"Saving HTML files using {num_browsers} browsers\")\n",
    "\n",
    "# initialize drivers\n",
    "with ThreadPoolExecutor(max_workers=num_browsers) as executor:\n",
    "    # Initialize drivers in parallel\n",
    "    drivers = list(executor.map(get_driver, range(num_browsers)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b67da748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 15:12:21,468 - AInewsbot - INFO - get_driver - 67021 Initializing webdriver\n",
      "2024-07-21 15:12:34,118 - AInewsbot - INFO - get_driver - Initialized webdriver profile\n",
      "2024-07-21 15:12:34,119 - AInewsbot - INFO - get_driver - Initialized webdriver service\n",
      "2024-07-21 15:13:14,893 - AInewsbot - INFO - get_driver - Initialized webdriver\n"
     ]
    }
   ],
   "source": [
    "d = get_driver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8379c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for driver in drivers:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40fb94c3-464b-43d7-a156-f965c4e6e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 15:16:05,824 - AInewsbot - INFO - Saving HTML files using 3 browsers\n",
      "2024-07-21 15:16:05,848 - AInewsbot - INFO - get_driver - 67021 Initializing webdriver\n",
      "2024-07-21 15:16:05,850 - AInewsbot - INFO - get_driver - 67021 Initializing webdriver\n",
      "2024-07-21 15:16:05,851 - AInewsbot - INFO - get_driver - 67021 Initializing webdriver\n",
      "2024-07-21 15:16:22,670 - AInewsbot - INFO - get_driver - Initialized webdriver profile\n",
      "2024-07-21 15:16:22,671 - AInewsbot - INFO - get_driver - Initialized webdriver profile\n",
      "2024-07-21 15:16:22,671 - AInewsbot - INFO - get_driver - Initialized webdriver profile\n",
      "2024-07-21 15:16:22,672 - AInewsbot - INFO - get_driver - Initialized webdriver service\n",
      "2024-07-21 15:16:22,672 - AInewsbot - INFO - get_driver - Initialized webdriver service\n",
      "2024-07-21 15:16:22,672 - AInewsbot - INFO - get_driver - Initialized webdriver service\n",
      "2024-07-21 15:17:29,609 - AInewsbot - INFO - get_driver - Initialized webdriver\n",
      "2024-07-21 15:17:29,703 - AInewsbot - INFO - Processing Ars Technica\n",
      "2024-07-21 15:17:29,729 - AInewsbot - INFO - get_files(Ars Technica) - starting get_files https://arstechnica.com/\n",
      "2024-07-21 15:17:30,003 - AInewsbot - INFO - get_driver - Initialized webdriver\n",
      "2024-07-21 15:17:30,089 - AInewsbot - INFO - Processing Bloomberg Tech\n",
      "2024-07-21 15:17:30,093 - AInewsbot - INFO - get_files(Bloomberg Technology - Bloomberg) - starting get_files https://www.bloomberg.com/technology\n",
      "2024-07-21 15:17:31,007 - AInewsbot - INFO - get_driver - Initialized webdriver\n",
      "2024-07-21 15:17:31,014 - AInewsbot - INFO - Processing Business Insider\n",
      "2024-07-21 15:17:31,015 - AInewsbot - INFO - get_files(Tech - Business Insider) - starting get_files https://www.businessinsider.com/tech\n",
      "2024-07-21 15:17:42,522 - AInewsbot - INFO - get_files(Ars Technica) - Saving Ars Technica (07_21_2024 03_17_42 PM).html as utf-8\n",
      "2024-07-21 15:17:42,531 - AInewsbot - INFO - Processing FT Tech\n",
      "2024-07-21 15:17:42,531 - AInewsbot - INFO - get_files(Technology) - starting get_files https://www.ft.com/technology\n",
      "2024-07-21 15:17:43,281 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:17:43,281 - AInewsbot - INFO - get_files(Tech - Business Insider) - Saving Tech - Business Insider (07_21_2024 03_17_43 PM).html as utf-8\n",
      "2024-07-21 15:17:43,282 - AInewsbot - INFO - Processing Feedly AI\n",
      "2024-07-21 15:17:43,283 - AInewsbot - INFO - get_files(Discover and Add New Feedly AI Feeds) - starting get_files https://feedly.com/i/aiFeeds?options=eyJsYXllcnMiOlt7InBhcnRzIjpbeyJpZCI6Im5scC9mL3RvcGljLzMwMDAifV0sInNlYXJjaEhpbnQiOiJ0ZWNobm9sb2d5IiwidHlwZSI6Im1hdGNoZXMiLCJzYWxpZW5jZSI6ImFib3V0In1dLCJidW5kbGVzIjpbeyJ0eXBlIjoic3RyZWFtIiwiaWQiOiJ1c2VyLzYyZWViYjlmLTcxNTEtNGY5YS1hOGM3LTlhNTdiODIwNTMwOC9jYXRlZ29yeS9HYWRnZXRzIn1dfQ\n",
      "2024-07-21 15:17:43,797 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:17:43,799 - AInewsbot - INFO - get_files(Bloomberg Technology - Bloomberg) - Saving Bloomberg Technology - Bloomberg (07_21_2024 03_17_43 PM).html as utf-8\n",
      "2024-07-21 15:17:43,800 - AInewsbot - INFO - Processing Google News\n",
      "2024-07-21 15:17:43,801 - AInewsbot - INFO - get_files(Google News - Technology - Artificial intelligence) - starting get_files https://news.google.com/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVhZU0FtVnVHZ0pWVXlnQVAB?hl=en-US&gl=US&ceid=US%3Aen\n",
      "2024-07-21 15:17:53,152 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:17:53,153 - AInewsbot - INFO - get_files(Technology) - Saving Technology (07_21_2024 03_17_53 PM).html as utf-8\n",
      "2024-07-21 15:17:53,156 - AInewsbot - INFO - Processing Hacker News\n",
      "2024-07-21 15:17:53,157 - AInewsbot - INFO - get_files(Hacker News Page 1) - starting get_files https://news.ycombinator.com/\n",
      "2024-07-21 15:17:55,109 - AInewsbot - INFO - get_files(Google News - Technology - Artificial intelligence) - Attempting to click on //*[@aria-label=\"Artificial intelligence\"]\n",
      "2024-07-21 15:17:55,401 - AInewsbot - INFO - get_files - Clicked\n",
      "2024-07-21 15:17:55,407 - AInewsbot - INFO - get_files(Google News - Technology - Artificial intelligence) - Loading additional infinite scroll items\n",
      "2024-07-21 15:18:03,750 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:18:03,751 - AInewsbot - INFO - get_files(Hacker News Page 1) - Saving Hacker News Page 1 (07_21_2024 03_18_03 PM).html as utf-8\n",
      "2024-07-21 15:18:03,753 - AInewsbot - INFO - Processing Hacker News 2\n",
      "2024-07-21 15:18:03,753 - AInewsbot - INFO - get_files(Hacker News Page 2) - starting get_files https://news.ycombinator.com/?p=2\n",
      "2024-07-21 15:18:05,431 - AInewsbot - INFO - get_files(Google News - Technology - Artificial intelligence) - Loading additional infinite scroll items\n",
      "2024-07-21 15:18:13,957 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:18:13,958 - AInewsbot - INFO - get_files(Hacker News Page 2) - Saving Hacker News Page 2 (07_21_2024 03_18_13 PM).html as utf-8\n",
      "2024-07-21 15:18:13,962 - AInewsbot - INFO - Processing HackerNoon\n",
      "2024-07-21 15:18:13,965 - AInewsbot - INFO - get_files(HackerNoon - read, write and learn about any technology) - starting get_files https://hackernoon.com/\n",
      "2024-07-21 15:18:14,029 - AInewsbot - INFO - get_files(Discover and Add New Feedly AI Feeds) - Loading additional infinite scroll items\n",
      "2024-07-21 15:18:15,529 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 15:18:15,529 - AInewsbot - INFO - get_files(Google News - Technology - Artificial intelligence) - Saving Google News - Technology - Artificial intelligence (07_21_2024 03_18_15 PM).html as utf-8\n",
      "2024-07-21 15:18:15,534 - AInewsbot - INFO - Processing NYT Tech\n",
      "2024-07-21 15:18:15,535 - AInewsbot - INFO - get_files(Technology - The New York Times) - starting get_files https://www.nytimes.com/section/technology\n",
      "2024-07-21 15:18:24,039 - AInewsbot - INFO - get_files(Discover and Add New Feedly AI Feeds) - Loading additional infinite scroll items\n",
      "2024-07-21 15:18:24,751 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:18:24,752 - AInewsbot - INFO - get_files(HackerNoon - read, write and learn about any technology) - Saving HackerNoon - read, write and learn about any technology (07_21_2024 03_18_24 PM).html as utf-8\n",
      "2024-07-21 15:18:24,755 - AInewsbot - INFO - Processing Reddit\n",
      "2024-07-21 15:18:24,756 - AInewsbot - INFO - get_files(top scoring links _ multi) - starting get_files https://www.reddit.com/r/ChatGPT+ChatGPTCoding+MacOS+MachineLearning+OpenAI+ProgrammerHumor+Windows10+battlestations+buildapc+cordcutters+dataisbeautiful+gadgets+hardware+linux+msp+programming+realtech+software+talesfromtechsupport+tech+technews+technology+techsupportgore+windows/top/?sort=top&t=day\n",
      "2024-07-21 15:18:26,255 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:18:26,256 - AInewsbot - INFO - get_files(Technology - The New York Times) - Saving Technology - The New York Times (07_21_2024 03_18_26 PM).html as utf-8\n",
      "2024-07-21 15:18:26,259 - AInewsbot - INFO - Processing Techmeme\n",
      "2024-07-21 15:18:26,259 - AInewsbot - INFO - get_files(Techmeme) - starting get_files https://www.techmeme.com/river\n",
      "2024-07-21 15:18:34,045 - AInewsbot - INFO - get_files(Discover and Add New Feedly AI Feeds) - Loading additional infinite scroll items\n",
      "2024-07-21 15:18:37,238 - AInewsbot - INFO - get_files(top scoring links _ multi) - Loading additional infinite scroll items\n",
      "2024-07-21 15:18:37,365 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:18:37,366 - AInewsbot - INFO - get_files(Techmeme) - Saving Techmeme (07_21_2024 03_18_37 PM).html as utf-8\n",
      "2024-07-21 15:18:37,367 - AInewsbot - INFO - Processing The Register\n",
      "2024-07-21 15:18:37,368 - AInewsbot - INFO - get_files(The Register_ Enterprise Technology News and Analysis) - starting get_files https://www.theregister.com/\n",
      "2024-07-21 15:18:44,073 - AInewsbot - INFO - get_files(Discover and Add New Feedly AI Feeds) - Loading additional infinite scroll items\n",
      "2024-07-21 15:18:47,250 - AInewsbot - INFO - get_files(top scoring links _ multi) - Loading additional infinite scroll items\n",
      "2024-07-21 15:18:47,712 - AInewsbot - INFO - get_files(The Register_ Enterprise Technology News and Analysis) - Saving The Register_ Enterprise Technology News and Analysis (07_21_2024 03_18_47 PM).html as utf-8\n",
      "2024-07-21 15:18:47,713 - AInewsbot - INFO - Processing The Verge\n",
      "2024-07-21 15:18:47,713 - AInewsbot - INFO - get_files(Artificial Intelligence - The Verge) - starting get_files https://www.theverge.com/ai-artificial-intelligence\n",
      "2024-07-21 15:18:54,083 - AInewsbot - INFO - get_files(Discover and Add New Feedly AI Feeds) - Loading additional infinite scroll items\n",
      "2024-07-21 15:18:57,316 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:18:57,316 - AInewsbot - INFO - get_files(top scoring links _ multi) - Saving top scoring links _ multi (07_21_2024 03_18_57 PM).html as utf-8\n",
      "2024-07-21 15:18:57,320 - AInewsbot - INFO - Processing VentureBeat\n",
      "2024-07-21 15:18:57,321 - AInewsbot - INFO - get_files(AI News _ VentureBeat) - starting get_files https://venturebeat.com/category/ai/\n",
      "2024-07-21 15:18:58,145 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:18:58,147 - AInewsbot - INFO - get_files(Artificial Intelligence - The Verge) - Saving Artificial Intelligence - The Verge (07_21_2024 03_18_58 PM).html as utf-8\n",
      "2024-07-21 15:18:58,149 - AInewsbot - INFO - Processing WSJ Tech\n",
      "2024-07-21 15:18:58,150 - AInewsbot - INFO - get_files(Technology - WSJ.com) - starting get_files https://www.wsj.com/tech\n",
      "2024-07-21 15:19:04,112 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:19:04,112 - AInewsbot - INFO - get_files(Discover and Add New Feedly AI Feeds) - Saving Discover and Add New Feedly AI Feeds (07_21_2024 03_19_04 PM).html as utf-8\n",
      "2024-07-21 15:19:04,114 - AInewsbot - INFO - Processing WaPo Tech\n",
      "2024-07-21 15:19:04,114 - AInewsbot - INFO - get_files(Technology - The Washington Post) - starting get_files https://www.washingtonpost.com/business/technology/\n",
      "2024-07-21 15:19:09,164 - AInewsbot - INFO - get_files(AI News _ VentureBeat) - Saving AI News _ VentureBeat (07_21_2024 03_19_09 PM).html as UTF-8\n",
      "2024-07-21 15:19:09,165 - AInewsbot - INFO - Quit webdriver\n",
      "2024-07-21 15:19:09,770 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 15:19:09,771 - AInewsbot - INFO - get_files(Technology - WSJ.com) - Saving Technology - WSJ.com (07_21_2024 03_19_09 PM).html as utf-8\n",
      "2024-07-21 15:19:09,772 - AInewsbot - INFO - Quit webdriver\n",
      "2024-07-21 15:19:15,171 - AInewsbot - INFO - Message: Unable to locate element: //meta[@http-equiv='Content-Type']; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "RemoteError@chrome://remote/content/shared/RemoteError.sys.mjs:8:8\n",
      "WebDriverError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:192:5\n",
      "NoSuchElementError@chrome://remote/content/shared/webdriver/Errors.sys.mjs:510:5\n",
      "dom.find/</<@chrome://remote/content/shared/DOM.sys.mjs:136:16\n",
      "\n",
      "2024-07-21 15:19:15,172 - AInewsbot - INFO - get_files(Technology - The Washington Post) - Saving Technology - The Washington Post (07_21_2024 03_19_15 PM).html as utf-8\n",
      "2024-07-21 15:19:15,175 - AInewsbot - INFO - Quit webdriver\n"
     ]
    }
   ],
   "source": [
    "# Fetch HTML files from sources\n",
    "\n",
    "# empty download directory\n",
    "delete_files(DOWNLOAD_DIR)\n",
    "\n",
    "# save each file specified from sources\n",
    "num_browsers = 3\n",
    "log(f\"Saving HTML files using {num_browsers} browsers\")\n",
    "\n",
    "# Create a queue for multiprocessing and populate it \n",
    "queue = multiprocessing.Queue()\n",
    "for item in sources.values():\n",
    "    queue.put(item)\n",
    "    \n",
    "# Function to take the queue and pop entries off and process until none are left\n",
    "# lets you create an array of functions with different args\n",
    "callable = process_source_queue_factory(queue)\n",
    "\n",
    "saved_pages = launch_drivers(num_browsers, callable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5aa21a-7387-4e76-a17e-6b22544dd13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(f\"Saved {len(saved_pages)} pages\")\n",
    "\n",
    "print(len(saved_pages))\n",
    "for sourcename, page in saved_pages:\n",
    "    sources[sourcename]['latest'] = page\n",
    "    log(f\"{sourcename} -> {page}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec5ab0",
   "metadata": {},
   "source": [
    "# Extract news URLs from saved pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d797e-1d72-46a5-9242-1877c5754bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse news URLs and titles from downloaded HTML files\n",
    "log(\"Parsing html files\")\n",
    "all_urls = []\n",
    "for sourcename, filename in saved_pages:\n",
    "    log(sourcename +' -> ' + filename)\n",
    "    log(f\"{sourcename}\", \"parse loop\")\n",
    "    links = parse_file(sources[sourcename])\n",
    "    log(f\"{len(links)} links found\", \"parse loop\")\n",
    "    all_urls.extend(links)\n",
    "\n",
    "log(f\"found {len(all_urls)} links\", \"parse loop\")\n",
    "\n",
    "# make a pandas dataframe of all the links found\n",
    "orig_df = (\n",
    "    pd.DataFrame(all_urls)\n",
    "    .groupby(\"url\")\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    .sort_values(\"src\")[[\"src\", \"title\", \"url\"]]\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index(drop=False)\n",
    "    .rename(columns={\"index\": \"id\"})\n",
    ")\n",
    "orig_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27972b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracts all links from history where isAI=1\n",
    "# # useful for training dimensionality reduction\n",
    "# conn = sqlite3.connect('articles.db')\n",
    "# c = conn.cursor()\n",
    "# #  and timestamp > '2024-07-01' \n",
    "# query = \"select * from news_articles where isAI=1 order by id\"\n",
    "# ai_history_df = pd.read_sql_query(query, conn)\n",
    "# ai_history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bbfb65",
   "metadata": {},
   "source": [
    "# Filter URLs to new AI headlines only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f760895-915f-499e-814d-dea389c9a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter urls we've already seen in previous runs and saved in SQLite\n",
    "filtered_df = filter_unseen_urls_db(orig_df, before_date=before_date)\n",
    "len(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a988ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use chatgpt to filter AI-related headlines using a prompt to OpenAI\n",
    "print(FILTER_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pages that fit in a reasonably sized (MAXPAGELEN or MAX_INPUT_TOKENS) prompt\n",
    "pages = paginate_df(filtered_df)\n",
    "log(f\"Paginated {len(pages)} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39235332-cb60-4c13-8e8a-0e63c91bd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use REST API directly. OpenAI python API doesn't support concurrent requests from a single client\n",
    "# this runs fast with async aiohttp and on gpt-3.5 (15 seconds vs 2 minutes synchronously with gpt-4o)\n",
    "# the old API supported submitting multiple payloads in a single completion request\n",
    "# current API supports a slow 'batch' submission https://platform.openai.com/docs/guides/rate-limits/usage-tiers\n",
    "# there is a more complex example here - https://github.com/openai/openai-cookbook/blob/main/examples/api_request_parallel_processor.py\n",
    "\n",
    "log(\"start classify\")\n",
    "enriched_urls = asyncio.run(fetch_pages(pages, prompt=FILTER_PROMPT))\n",
    "log(\"end classify\")\n",
    "\n",
    "enriched_df = pd.DataFrame(enriched_urls)\n",
    "print(len(enriched_df))\n",
    "log(\"isAI\", len(enriched_df.loc[enriched_df[\"isAI\"]]))\n",
    "log(\"not isAI\", len(enriched_df.loc[~enriched_df[\"isAI\"]]))\n",
    "enriched_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36ce69-2da0-4c85-a98f-760c7b7c6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge returned df with isAI column into original df on id column\n",
    "merged_df = pd.merge(filtered_df, enriched_df, on=\"id\", how=\"outer\")\n",
    "merged_df['date'] = datetime.now().date()\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286de30-81e7-4bab-a7fa-9477f27b6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be empty, shouldn't get back rows that don't match to existing\n",
    "log(f\"Unmatched response rows: {len(merged_df.loc[merged_df['src'].isna()])}\")\n",
    "# should be empty, should get back all rows from orig\n",
    "log(f\"Unmatched source rows: {len(merged_df.loc[merged_df['isAI'].isna()])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5548bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep headlines that are related to AI\n",
    "AIdf = merged_df.loc[merged_df[\"isAI\"]==1] \\\n",
    "    .reset_index(drop=True)  \\\n",
    "    .reset_index()  \\\n",
    "    .drop(columns=[\"id\"])  \\\n",
    "    .rename(columns={'index': 'id'})\n",
    "\n",
    "log(f\"Found {len(AIdf)} AI headlines\")\n",
    "\n",
    "AIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5323512b-727a-4bcb-b74e-0577044e757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map title to ascii characters to avoid some dupes with e.g. different quote symbols\n",
    "\n",
    "AIdf['title'] = AIdf['title'].apply(unicode_to_ascii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedupe identical headlines\n",
    "AIdf['title_clean'] = AIdf['title'].map(lambda s: \"\".join(s.split()))\n",
    "AIdf = AIdf.sort_values(\"src\") \\\n",
    "    .groupby(\"title_clean\") \\\n",
    "    .first() \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .drop(columns=['id']) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index': 'id'})\n",
    "\n",
    "log(f\"Found {len(AIdf)} unique AI headlines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0679b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map google news headlines to redirect\n",
    "\n",
    "AIdf = get_google_news_redirects(AIdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79044f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must do this after fixing google actualurl\n",
    "AIdf['hostname']=AIdf['actual_url'].apply(lambda url: urlparse(url).netloc)\n",
    "AIdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c149f47d",
   "metadata": {},
   "source": [
    "### Get site names and update site names based on URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9078abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get site_name\n",
    "conn = sqlite3.connect('articles.db')\n",
    "c = conn.cursor()\n",
    "#  and timestamp > '2024-07-01' \n",
    "query = \"select * from sites\"\n",
    "sites_df = pd.read_sql_query(query, conn)\n",
    "sites_dict = {row.hostname:row.site_name for row in sites_df.itertuples()}\n",
    "\n",
    "sites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ed5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf['site_name'] = AIdf['hostname'].apply(lambda hostname: sites_dict.get(hostname, \"\"))\n",
    "AIdf.loc[AIdf['site_name']==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_site_name(session, row):\n",
    "    cat_prompt = f\"\"\"\n",
    "based on this url and your knowledge of the Web, what is the name of the site? https://{row.hostname}\n",
    "\n",
    "return the response as a json object of the form {{\"url\": \"www.yankodesign.com\", \"site_name\": \"Yanko Design\"}}\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        messages=[\n",
    "                  {\"role\": \"user\", \"content\": cat_prompt\n",
    "                  }]\n",
    "\n",
    "        payload = {\"model\":  LOWCOST_MODEL,\n",
    "                   \"response_format\": {\"type\": \"json_object\"},\n",
    "                   \"messages\": messages,\n",
    "                   \"temperature\": 0\n",
    "                   }\n",
    "        response = await fetch_openai(session, payload)\n",
    "        response_dict = json.loads(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "        return response_dict\n",
    "    except Exception as exc:\n",
    "        print(exc)\n",
    "                \n",
    "tasks = []\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    for row in AIdf.loc[AIdf['site_name']==\"\"].itertuples():\n",
    "        task = asyncio.create_task(get_site_name(session, row))\n",
    "        tasks.append(task)\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update site_dict from responses\n",
    "new_urls = []\n",
    "for r in responses:\n",
    "    if r['url'].startswith('https://'):\n",
    "        r['url'] = r['url'][8:]\n",
    "    new_urls.append(r['url'])\n",
    "    sites_dict[r['url']] = r['site_name']\n",
    "    print(r['url'], r['site_name'])\n",
    "\n",
    "AIdf['site_name'] = AIdf['hostname'].apply(lambda hostname: sites_dict.get(hostname, hostname))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in new_urls:\n",
    "    sqlstr = \"INSERT OR IGNORE INTO sites (hostname, site_name) VALUES (?, ?);\"\n",
    "    print(url, '->', sites_dict[url])\n",
    "    conn.execute(sqlstr, (url, sites_dict[url]))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a46e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update SQLite database with all seen URLs\n",
    "conn = sqlite3.connect('articles.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for row in AIdf.itertuples():\n",
    "    insert_article(conn, cursor, row.src, row.hostname, row.title,\n",
    "                   row.url, row.actual_url, row.isAI, row.date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a733fe",
   "metadata": {},
   "source": [
    "# Topic analysis\n",
    "Try to identify the top topics of the day, to help make a nice summary. \n",
    "\n",
    "1st approach - do dimensionality reduction on the headline embeddings with UMAP and cluster with DBSCAN.\n",
    "\n",
    "2nd approach\n",
    " - extract topics from headline using a prompt\n",
    " - human canonicalizes topics\n",
    " - assign headlines to topics using a prompt\n",
    " \n",
    " The final summary is pretty inconsistent, would be nice to give chatgpt a prompt that would say, summarize these bullet points using this categorization.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e957c4f",
   "metadata": {},
   "source": [
    "### Fit dimensionality reduction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train dimensionality reduction, only need to do this every few months and pickle the model to reflect new topics\n",
    "# # extracts all links from history where isAI=1\n",
    "# conn = sqlite3.connect('articles.db')\n",
    "# c = conn.cursor()\n",
    "# #  and timestamp > '2024-07-01' \n",
    "# query = \"select * from news_articles where isAI=1 order by id desc limit 20000\"\n",
    "# ai_history_df = pd.read_sql_query(query, conn)\n",
    "# len(ai_history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model=\"text-embedding-3-large\"\n",
    "# embedding_df_list = []\n",
    "# pages = paginate_df(ai_history_df, maxpagelen=1000, max_input_tokens=8192)\n",
    "\n",
    "# for p in pages:\n",
    "#     response = client.embeddings.create(input=[obj['title'] for obj in p],\n",
    "#                                         model=embedding_model)\n",
    "#     embedding_df_list.append(pd.DataFrame([e.model_dump()['embedding'] for e in response.data]))\n",
    "\n",
    "# embedding_df = pd.concat(embedding_df_list, axis=0, ignore_index=True)\n",
    "\n",
    "# embedding_df.to_pickle(\"historical_embedding_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab68cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the UMAP reducer\n",
    "# reducer = umap.UMAP(n_components=30)\n",
    "# # Fit the reducer to the data without transforming\n",
    "# reducer.fit(embedding_df)\n",
    "# # Pickle the reducer\n",
    "# with open('reducer.pkl', 'wb') as f:\n",
    "#     pickle.dump(reducer, f)\n",
    "# print(\"UMAP reducer pickled and saved as 'reducer.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to extract top topics \n",
    "print(TOPIC_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b4c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get topics\n",
    "pages = paginate_df(AIdf)\n",
    "\n",
    "# apply this prompt to AI headlines\n",
    "log(\"start topic extraction\")\n",
    "response = asyncio.run(fetch_pages(pages, prompt=TOPIC_PROMPT))\n",
    "log(\"end topic extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = pd.DataFrame(response)\n",
    "topic_df = topic_df.rename(columns={'topics': 'extracted_topics'})\n",
    "print(len(topic_df))\n",
    "topic_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf65ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = [item.lower() for row in topic_df.itertuples() for item in row.extracted_topics]\n",
    "item_counts = Counter(all_topics)\n",
    "filtered_topics = [item for item in item_counts if item_counts[item] >= 2 and item not in {'technology', 'ai', 'artificial intelligence'}]\n",
    "print(len(filtered_topics))\n",
    "sorted(filtered_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497baef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df['extracted_topics'] = topic_df['extracted_topics'].apply(lambda l: [t.title() for t in l if t.lower() in filtered_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evergreen topics to hopefully map healdines to canonical standardized topics\n",
    "# review extracted topics and add\n",
    "# you could try it with new cats or new cats + evergreen\n",
    "# but probably look at new cats and human in the loop should add good new cats today to evergreen list\n",
    "# new_cats = list(json.loads(response.choices[0].message.content).values())[0]\n",
    "# categories = sorted(list(set(new_cats + evergreen)))\n",
    "categories = sorted(CANONICAL_TOPICS)\n",
    "for c in categories:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in filtered_topics if t.lower() not in [u.lower() for u in CANONICAL_TOPICS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f09b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe try to set timeout in categorize_headline\n",
    "catdict = dict()\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    for i, row in enumerate(AIdf.itertuples()):\n",
    "        tasks = []\n",
    "        log(f\"Categorizing headline {row.id+1} of {len(AIdf)}\")\n",
    "        h = row.title\n",
    "        log(h)\n",
    "        for c in categories:\n",
    "            task = asyncio.create_task(categorize_headline(h, c, session))\n",
    "            tasks.append(task)\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        catdict[row.id] = [item for sublist in responses for item in sublist]\n",
    "        log(str(catdict[row.id]))\n",
    "        \n",
    "catdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df['assigned_topics'] = topic_df['id'].apply(lambda id: catdict.get(id, \"\"))\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd40c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcategories = set([c.lower() for c in categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ee805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_topics(row):\n",
    "    topics = [x.title() for x in row.extracted_topics if x.lower() not in {\"technology\", \"ai\", \"artificial intelligence\"}]\n",
    "    assigned_topics = [x.title() for x in row.assigned_topics if x.lower() in lcategories]\n",
    "    combined = sorted(list(set(topics + assigned_topics)))\n",
    "    combined = [s.replace(\"Ai\", \"AI\") for s in combined]\n",
    "    combined = [s.replace(\"Genai\", \"Gen AI\") for s in combined]\n",
    "    combined = [s.replace(\"Openai\", \"OpenAI\") for s in combined]\n",
    "    return combined\n",
    "\n",
    "topic_df[\"topics\"] = topic_df.apply(clean_topics, axis=1)\n",
    "topic_df[\"topic_str\"] = topic_df.apply(lambda row: \", \".join(row.topics), axis=1)\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2034f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee94e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa602bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:  # for idempotency\n",
    "    AIdf = AIdf.drop(columns=[\"title_topic_str\"])\n",
    "except:\n",
    "    pass\n",
    "try:  # for idempotency\n",
    "    AIdf = AIdf.drop(columns=[\"topic_str\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "AIdf = pd.merge(AIdf, topic_df[[\"id\", \"topic_str\"]], on=\"id\", how=\"inner\")\n",
    "AIdf['title_topic_str'] = AIdf.apply(lambda row: f'{row.title} (Topics: {row.topic_str})', axis=1)\n",
    "AIdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    display(AIdf.loc[AIdf[\"topic_str\"]==\"\"][['title']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6748df0",
   "metadata": {},
   "source": [
    "### Semantic sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468fa9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use embeddings to sort headlines by semantical similarity\n",
    "log(f\"Fetching embeddings for {len(AIdf)} headlines\")\n",
    "embedding_model = 'text-embedding-3-large'\n",
    "response = client.embeddings.create(input=AIdf['title_topic_str'].tolist(),\n",
    "                                    model=embedding_model)\n",
    "embedding_df = pd.DataFrame([e.model_dump()['embedding'] for e in response.data])\n",
    "\n",
    "# sort of a traveling salesman sort\n",
    "log(f\"Sort with agglomerative cluster sort\")\n",
    "sorted_indices = agglomerative_cluster_sort(embedding_df)\n",
    "AIdf['sort_order'] = sorted_indices\n",
    "\n",
    "# do dimensionality reduction on embedding_df and cluster analysis\n",
    "log(f\"Perform dimensionality reduction\")\n",
    "with open(\"reducer.pkl\", 'rb') as file:\n",
    "    # Load the model from the file\n",
    "    reducer = pickle.load(file)\n",
    "reduced_data = reducer.transform(embedding_df)\n",
    "log(f\"Cluster with DBSCAN\")\n",
    "dbscan = DBSCAN(eps=0.4, min_samples=3)  # Adjust eps and min_samples as needed\n",
    "AIdf['cluster'] = dbscan.fit_predict(reduced_data)\n",
    "AIdf.loc[AIdf['cluster'] == -1, 'cluster'] = 999\n",
    "    \n",
    "# sort first by clusters found by DBSCAN, then by semantic ordering\n",
    "AIdf = AIdf.sort_values(['cluster', 'sort_order']) \\\n",
    "    .reset_index(drop=True) \\\n",
    "    .reset_index() \\\n",
    "    .drop(columns=[\"id\"]) \\\n",
    "    .rename(columns={'index': 'id'})\n",
    "\n",
    "AIdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca92189",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def write_topic_name(session, topic_list_str, max_retries=3, model=LOWCOST_MODEL):\n",
    "\n",
    "    TOPIC_WRITER_PROMPT = f\"\"\"\n",
    "You are a topic writing assistant. I will provide a list of headlines with extracted topics in parentheses. \n",
    "Your task is to propose a name for a topic that very simply, clearly and accurately captures all the provided \n",
    "headlines in less than 7 words. You will output a JSON object with the key \"topic_title\".\n",
    "\n",
    "Example Input:\n",
    "In the latest issue of Caixins weekly magazine: CATL Bets on 'Skateboard Chassis' and Battery Swaps to Dispell Market Concerns (powered by AI) (Topics: Battery Swaps, Catl, China, Market Concerns, Skateboard Chassis)\n",
    "\n",
    "AI, cheap EVs, future Chevy  the week (Topics: Chevy, Evs)\n",
    "\n",
    "Electric Vehicles and AI: Driving the Consumer & World Forward (Topics: Consumer, Electric Vehicles, Technology)\n",
    "\n",
    "Example Output:\n",
    "{{\"topic_title\": \"Electric Vehicles\"}}\n",
    "\n",
    "Task\n",
    "Propose the name for the overall topic based on the following provided headlines and individual topics:\n",
    "\n",
    "{topic_list_str}\n",
    "\"\"\"\n",
    "\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            messages=[\n",
    "                      {\"role\": \"user\", \"content\": TOPIC_WRITER_PROMPT\n",
    "                      }]\n",
    "\n",
    "            payload = {\"model\":  model,\n",
    "                       \"response_format\": {\"type\": \"json_object\"},\n",
    "                       \"messages\": messages,\n",
    "                       \"temperature\": 0\n",
    "                       }\n",
    "            response = await fetch_openai(session, payload)\n",
    "            response_dict = json.loads(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "            return response_dict\n",
    "\n",
    "            break\n",
    "        except Exception as exc:\n",
    "            log(f\"Error: {exc}\")\n",
    "\n",
    "    return {}\n",
    "        \n",
    "\n",
    "# show clusters\n",
    "cluster_topics = []\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "\n",
    "        for i in range(30):\n",
    "            tmpdf = AIdf.loc[AIdf['cluster']==i][[\"id\", \"title_topic_str\"]]\n",
    "            if len(tmpdf) ==0:\n",
    "                break\n",
    "            display(tmpdf)\n",
    "            title_topic_str_list = (\"\\n\\n\".join(tmpdf['title_topic_str'].to_list()))\n",
    "            cluster_topic = await write_topic_name(session, title_topic_str_list)\n",
    "            cluster_topics.append(cluster_topic)\n",
    "            print(cluster_topic)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126cf7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could extract top words using tfidf, something like \n",
    "# vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "# topics = []\n",
    "# for i in range(n_topics):\n",
    "#     # Get the documents in this cluster\n",
    "#     cluster_docs = [doc for doc, label in zip(documents, cluster_labels) if label == i]\n",
    "#     cluster_metadatas = [meta for meta, label in zip(metadatas, cluster_labels) if label == i]\n",
    "\n",
    "#     # Get the top words for this cluster based on TF-IDF scores\n",
    "#     tfidf_scores = tfidf_matrix[cluster_labels == i].sum(axis=0).A1\n",
    "#     top_word_indices = tfidf_scores.argsort()[-n_words_per_topic:][::-1]\n",
    "#     top_words = [feature_names[index] for index in top_word_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e089e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_topic_list = [obj['topic_title'] for obj in cluster_topics]\n",
    "cluster_topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf['cluster_name'] = AIdf['cluster'].apply(lambda i: cluster_topic_list[i] if i<len(cluster_topic_list) else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cc4ee",
   "metadata": {},
   "source": [
    "# Save and email headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12047601-67d0-4e60-b1e7-79d38349b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_str = \"\"\n",
    "for row in AIdf.itertuples():\n",
    "    log(f\"[{row.Index}. {row.title} - {row.site_name}]({row.actual_url})\")\n",
    "    html_str += f'{row.Index}.<a href=\"{row.actual_url}\">{row.title} - {row.site_name}</a><br />\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save headlines\n",
    "with open('headlines.html', 'w') as f:\n",
    "    f.write(html_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# send mail\n",
    "log(\"Sending headlines email\")\n",
    "subject = f'AI headlines {datetime.now().strftime(\"%H:%M:%S\")}'\n",
    "send_gmail(subject, html_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff0768a",
   "metadata": {},
   "source": [
    "# Save individual pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch pages\n",
    "# Create a queue for multiprocessing and populate it \n",
    "log(\"Queuing URLs for scraping\")\n",
    "\n",
    "queue = multiprocessing.Queue()\n",
    "for row in AIdf.itertuples():\n",
    "    queue.put((row.id, row.actual_url, row.title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe930e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape urls in queue asynchronously\n",
    "num_browsers = 4\n",
    "\n",
    "callable = process_url_queue_factory(queue)\n",
    "\n",
    "log(f\"fetching {len(AIdf)} pages using {num_browsers} browsers\")\n",
    "saved_pages = launch_drivers(num_browsers, callable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = pd.DataFrame(saved_pages)\n",
    "pages_df.columns = ['id', 'actual_url', 'title', 'path']\n",
    "pages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56202217",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf = pd.merge(AIdf, pages_df[[\"id\", \"path\"]], on='id', how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ada9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5793763",
   "metadata": {},
   "source": [
    "# Summarize individual pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SUMMARIZE_SYSTEM_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ccf2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SUMMARIZE_USER_PROMPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d7778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are fetching all at once, could be 200 summaries, so we are firing off 200 REST requests at once\n",
    "# This seems like a bad idea, could loop through and fire off e.g. 10 at a time, or use queues and workers (seems pointless)\n",
    "# But it works and runs fast on 3.5 and if ChatGPT doesn't like it they could throttle it\n",
    "\n",
    "log(\"Starting summarize\")\n",
    "responses = await fetch_all_summaries(AIdf)\n",
    "log(f\"Received {len(responses)} summaries\")\n",
    "print(responses[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring summaries into dict\n",
    "response_dict = {}\n",
    "for i, response in responses:\n",
    "    try:\n",
    "        response_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        response_dict[i] = response_str\n",
    "    except Exception as exc:\n",
    "        print(exc)\n",
    "        \n",
    "len(response_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c077f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf['hostname']=AIdf['actual_url'].apply(lambda url: urlparse(url).netloc)\n",
    "AIdf['site_name'] = AIdf['hostname'].apply(lambda hostname: sites_dict.get(hostname, \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91263777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make text for email and also collect data for vector store\n",
    "markdown_str = ''\n",
    "vectorstore_list = []\n",
    "metadata_list=[]\n",
    "for i, row in enumerate(AIdf.itertuples()):\n",
    "    topics = []\n",
    "    if row.cluster_name:\n",
    "        topics.append(row.cluster_name)\n",
    "    if row.topic_str:\n",
    "        topics.append(row.topic_str)\n",
    "    topic_str = \", \".join(topics)\n",
    "\n",
    "    mdstr = f\"[{i+1}. {row.title} - {row.site_name}]({row.actual_url})  \\n\\n {topic_str}  \\n\\n{response_dict[row.id]} \\n\\n\"\n",
    "    # simpler version for vector store\n",
    "    vectorstore_list.append(f\"[{row.title} - {row.site_name}]({row.actual_url})\\n\\nTopics: {row.topic_str} \\n\\n{response_dict[row.id]}\\n\\n\")\n",
    "    metadata_list.append({'id': row.id, 'title': row.title, 'url': row.actual_url, 'site': row.site_name})\n",
    "    display(Markdown(mdstr))\n",
    "    markdown_str += mdstr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73284fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown( vectorstore_list[16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e03f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata_list[16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document objects with the paragraphs and corresponding metadata\n",
    "docs = [Document(page_content=paragraph, metadata=meta) \n",
    "        for paragraph, meta in zip(vectorstore_list, metadata_list)]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac14923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist_directory = \"/Users/drucev/projects/AInewsbot/chroma_db_openai\"\n",
    "try:\n",
    "    del vectorstore\n",
    "except Exception as e:\n",
    "    log(f\"{e}\")\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(persist_directory)\n",
    "    log(f\"Directory '{persist_directory}' and all its contents have been removed successfully.\")\n",
    "except Exception as e:\n",
    "    log(f\"Remove directory error: {e}\")\n",
    "        \n",
    "embeddings_openAI = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "vectorstore = Chroma.from_documents(docs, embeddings_openAI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311254d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a similarity search\n",
    "query = \"What is the latest with openai?\"\n",
    "results = vectorstore.similarity_search_with_score(query, \n",
    "                                        k=20,\n",
    "                                       )  # k is the number of results to return\n",
    "# Print the results\n",
    "urldict = {}\n",
    "for doc, score in results:\n",
    "    if urldict.get(doc.metadata['url']):\n",
    "        continue\n",
    "    urldict[doc.metadata['url']] = 1\n",
    "    if score < 1.25:\n",
    "        print(f\"Score:   {score}\")\n",
    "        print(f\"Content: {doc.page_content}\\n\")\n",
    "        print(f\"Metadata: {doc.metadata}\\n\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # or use local embeddings with sentence_transformers\n",
    "# # Initialize your embedding model\n",
    "# embeddings_hf = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# # Create the vector store with a persist_directory\n",
    "# persist_directory = \"/Users/drucev/projects/AInewsbot/chroma_db_huggingface\"\n",
    "# vectorstore_hf = Chroma.from_documents(\n",
    "#     documents=docs,\n",
    "#     embedding=embeddings_hf,\n",
    "#     persist_directory=persist_directory\n",
    "# )\n",
    "\n",
    "# # Perform a similarity search\n",
    "# query = \"What is the latest with OpenAI?\"\n",
    "# results = vectorstore_hf.similarity_search(query, k=10)  # k is the number of results to return\n",
    "\n",
    "# # Print the results\n",
    "# for doc in results:\n",
    "#     print(f\"Content: {doc.page_content}\\n\")\n",
    "#     print(f\"Metadata: {doc.metadata}\\n\")\n",
    "#     print(\"---\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe07a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Markdown to HTML\n",
    "html_str = markdown.markdown(markdown_str, extensions=['extra'])\n",
    "# display(HTML(html_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165758da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save bullets\n",
    "with open('bullets.md', 'w') as f:\n",
    "    f.write(markdown_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Sending bullet points email\")\n",
    "subject = f'AI news bullets {datetime.now().strftime(\"%H:%M:%S\")}'\n",
    "send_gmail(subject, html_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dffcde",
   "metadata": {},
   "source": [
    "# Ask ChatGPT for top categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOP_CATEGORIES_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "              {\"role\": \"user\", \"content\": TOP_CATEGORIES_PROMPT + markdown_str\n",
    "              }],\n",
    "    n=1,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22965c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_categories = list(json.loads(response.choices[0].message.content).values())[0]\n",
    "suggested_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human category edit \n",
    "my_cats = [\n",
    " 'AI in drug discovery',\n",
    " 'Machine learning in structural biology',\n",
    " 'nference and Takeda AI partnership',\n",
    " 'Ant-inspired AI for drones',\n",
    " \"Fei-Fei Li's $1B AI startup\",\n",
    " \"Artificial Agency's $16M funding\",\n",
    " \"CTERA's $80M funding\",\n",
    " \"Anthropic's $100M AI fund\",\n",
    " \"Samsung's AI image generation\",\n",
    " 'EU antitrust probe on AI deals',\n",
    " 'Salesforce AI service agent',\n",
    " \"Meta's AI regulatory issues\",\n",
    " 'Nvidia and Mistral AI model',\n",
    " 'TSMC AI chip demand surge',\n",
    " 'AI search engines',\n",
    " 'AI in cybersecurity',\n",
    " 'AI in Healthcare & Precision Medicine',\n",
    " 'Autonomous Drones',\n",
    " 'AI Startup Funding',\n",
    " 'Claude AI for Data Analysis',\n",
    " 'AI Startup Funding',\n",
    " 'AI Smartphones',\n",
    " 'Big Tech Antitrust Investigations',\n",
    " 'Large Language Models',\n",
    " 'AI-Powered Customer Service Agents',\n",
    " 'AI Ethics and Regulation',\n",
    " 'AI Art by Microsoft',\n",
    " 'AI Chip Demand Surge',\n",
    " 'Text-To-Image Diffusion Models',\n",
    " 'ChatGPT in AI Dominance'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac52f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_str = \"\"\n",
    "doc_list = []\n",
    "docid_list = []\n",
    "similarity_cutoff = 1.25\n",
    "for cat in my_cats:\n",
    "    docstr = f\"# {cat} \\n\\n\"\n",
    "    # Perform a similarity search\n",
    "    results = vectorstore.similarity_search_with_score(cat, \n",
    "                                                       k=10,\n",
    "                                                      )\n",
    "    if results:\n",
    "        # Print the results\n",
    "        urldict = {}\n",
    "        for doc, score in results:\n",
    "            if urldict.get(doc.metadata['url']):\n",
    "                continue\n",
    "            urldict[doc.metadata['url']] = 1    \n",
    "            if score > similarity_cutoff:\n",
    "                break\n",
    "            docstr += f\"{doc.page_content}\\n\"\n",
    "            docid_list.append(doc.metadata['id'])\n",
    "        doc_list.append(docstr)\n",
    "        md_str += docstr\n",
    "        \n",
    "        \n",
    "display(Markdown(md_str))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "docid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f615b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write sections individually\n",
    "\n",
    "mail_md_str = \"\"\n",
    "\n",
    "for current_topic, cat in enumerate(my_cats):\n",
    "\n",
    "    section_prompt = f\"\"\"\n",
    "You are an advanced summarization assistant, a sophisticated AI system\n",
    "designed to write a compelling summary of news input.\n",
    "\n",
    "Input:\n",
    "I will provide a markdown list of today's news articles on the topic: {my_cats[current_topic]}.\n",
    "The input will be in the format\n",
    "[Site-name-s1](url-s1)\n",
    "Story-Title-s1\n",
    "\n",
    "Topics: s1-topic1, s1-topic2, s1-topic3\n",
    "\n",
    "- s1-bullet-point-1\n",
    "- s1-bullet-point-2\n",
    "- s1-bullet-point-3\n",
    "\n",
    "[Site-name-s2](url-s2)\n",
    "Story-Title-s2\n",
    "\n",
    "Topics: s2-topic1, s2-topic2, s2-topic3\n",
    "\n",
    "- s2-bullet-point-1\n",
    "- s2-bullet-point-2\n",
    "- s2-bullet-point-3\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Read the input closely.\n",
    "USE ONLY INFORMATION PROVIDED IN THE INPUT.\n",
    "Provide the most significant facts without commentary or elaboration.\n",
    "Write an engaging summary consisting of a title and at least 1 and no more than 5 bullet points.\n",
    "Use as few bullet points as you need to provide the most significant facts.\n",
    "Each bullet should contain one sentence with one link.\n",
    "Each bullet should not repeat points or information from previous bullet points.\n",
    "DO NOT REPEAT LINKS FROM PREVIOUS BULLET POINTS.\n",
    "Write in the professional but engaging, narrative style of a tech reporter for a national publication.\n",
    "Be balanced, professional, informative, providing accurate, clear, concise summaries in a respectful neutral tone.\n",
    "\n",
    "Please check carefully that you only use information provided in the following input, and that any bullet point\n",
    "does not repeat information or links prevously provided.\n",
    "\n",
    "Example Output Format Template (EXAMPLE ONLY, DO NOT OUTPUT THIS TEMPLATE):\n",
    "\n",
    "# Engaging title\n",
    "\n",
    "- bullet point a - [site name a](site url a)\n",
    "- bullet point b - [site name b ](site url b)\n",
    "\n",
    "Input:\n",
    "\n",
    "{doc_list[current_topic]}\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "                  {\"role\": \"user\", \"content\": section_prompt\n",
    "                  }],\n",
    "        n=1,   \n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    response_str = response.choices[0].message.content\n",
    "    response_str = response_str.replace(\"$\", \"\\\\$\")\n",
    "    mail_md_str += response_str + \" \\n\\n\"\n",
    "    display(Markdown(response_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mail_md_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db9ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Sending full summary email \")\n",
    "subject = f'AI news summary {datetime.now().strftime(\"%H:%M:%S\")}'\n",
    "final_html_str = markdown.markdown(mail_md_str, extensions=['extra'])\n",
    "display(HTML(final_html_str))\n",
    "send_gmail(subject, final_html_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e426c2c",
   "metadata": {},
   "source": [
    "# Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize by just giving selected stories in semantic order and hinting how to write it\n",
    "AIdf.loc[AIdf['id'].isin(set(docid_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd999b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[16].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make text for email and also collect data for vector store\n",
    "markdown_str = ''\n",
    "print()\n",
    "\n",
    "for i, row in enumerate(AIdf.loc[AIdf['id'].isin(set(docid_list))].itertuples()):\n",
    "    mdstr = docs[row.id].page_content\n",
    "    display(Markdown(mdstr.replace('$', '\\\\$')))\n",
    "    markdown_str += mdstr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86570530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the list of topics in human in the loop workflow\n",
    "# loop though each topic and summarize, then \n",
    "# then combine the summaries for a final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTPROMPT = f\"\"\"\n",
    "You are an advanced summarization assistant, a sophisticated AI system\n",
    "designed to write a compelling summary of news input. You are able to categorize information, \n",
    "and identify trends from large volumes of news.\n",
    "\n",
    "Objective: \n",
    "I will provide the text of today's news articles about AI and summary bullet points in markdown format.\n",
    "Bullet points will contain a title and URL, a list of topics discussed, and a bullet-point summary of\n",
    "the article. You are tasked with identifying and summarizing the most important news, recurring themes,\n",
    "common facts and items. Your job is to create a concise summary of today's topics and developments.\n",
    "You will write an engaging summary of today's news encompassing the most important and frequently \n",
    "mentioned topics and themes.\n",
    "You will write in the professional but engaging, narrative style of a tech reporter for a national publication.\n",
    "You will be balanced, professional, informative, providing accurate, clear, concise summaries in a neutral tone.\n",
    "You will group stories into related topics\n",
    "\n",
    "Input Format Template:\n",
    "\n",
    "[Site-name-s1](url-s1)\n",
    "Story-Title-s1\n",
    "\n",
    "Topics: s1-topic1, s1-topic2, s1-topic3\n",
    "\n",
    "- s1-bullet-point-1\n",
    "- s1-bullet-point-2\n",
    "- s1-bullet-point-3\n",
    "\n",
    "[Site-name-s2](url-s2)\n",
    "Story-Title-s2\n",
    "\n",
    "Topics: s2-topic1, s2-topic2, s2-topic3\n",
    "\n",
    "- s2-bullet-point-1\n",
    "- s2-bullet-point-2\n",
    "- s2-bullet-point-3\n",
    "\n",
    "Example Output Format Template (EXAMPLE ONLY, DO NOT OUTPUT THIS TEMPLATE):\n",
    "\n",
    "# Engaging-topic-title-1\n",
    "\n",
    "- bullet-point-1a - [site-name-1a](site-url-1a)\n",
    "- bullet-point-1b - [site-name-1b](site-url-1b)\n",
    "\n",
    "# Engaging-topic-title-2\n",
    "\n",
    "- bullet-point-2a - [site-name-2a](site-url-2a)\n",
    "- bullet-point-2b - [site-name-2b](site-url-2b)\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Read the input closely.\n",
    "Very important: USE ONLY INFORMATION PROVIDED IN THE INPUT.\n",
    "Provide the most significant facts without commentary or elaboration.\n",
    "Each bullet should contain one sentence with one link.\n",
    "Each bullet should not repeat points or information from previous bullet points.\n",
    "\n",
    "Please check carefully that you only use information provided in the following input, that you include\n",
    "all links in the input, and that any bullet point does not repeat information or links prevously provided.\n",
    "\n",
    "Input:\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "              {\"role\": \"user\", \"content\": TESTPROMPT + markdown_str\n",
    "              }],\n",
    "    n=1,   \n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "response_str = response.choices[0].message.content\n",
    "response_str = response_str.replace(\"$\", \"\\\\$\")\n",
    "display(Markdown(response_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dacf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_str = response.choices[0].message.content\n",
    "response_str = response_str.replace(\"$\", \"\\\\$\")\n",
    "display(Markdown(response_str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044beddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Sending full summary email \")\n",
    "subject = f'AI news summary {datetime.now().strftime(\"%H:%M:%S\")}'\n",
    "final_html_str = markdown.markdown(response_str, extensions=['extra'])\n",
    "display(HTML(final_html_str))\n",
    "send_gmail(subject, final_html_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a637f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You will act like a professional editor with expertise in content optimization.\n",
    "You are skilled at refining and enhancing written materials, specializing in\n",
    "ensuring clarity, conciseness, and coherence in various types of documents,\n",
    "including newsletters.\n",
    "\n",
    "Objective: Edit the markdown newsletter provided below by removing any redundant\n",
    "sentences or bullet points that restate previous points and contain the same link.\n",
    "Leave intact bullet points that are unique and provide distinct information.\n",
    "\n",
    "Step-by-step instructions:\n",
    "\n",
    "Carefully read through the entire newsletter to understand the overall structure and content.\n",
    "Identify sentences and bullet points that repeat information or provide identical links.\n",
    "Remove all redundant sentences and bullet points that do not contribute new information or unique links.\n",
    "Ensure that the remaining content flows logically and maintains the intended message and tone of the newsletter.\n",
    "Double-check the final edited version for any inconsistencies or errors introduced during the editing process.\n",
    "Take a deep breath and work on this problem step-by-step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"You will act like a professional editor with expertise in content optimization.\n",
    "You are skilled at reviewing and enhancing written materials, specializing in\n",
    "helping improve clarity, conciseness, and coherence in various types of documents,\n",
    "including newsletters.\n",
    "\n",
    "Objective: Review the markdown newsletter provided below and advise on ways to improve it.\n",
    "Note any links which are repeated, any sections which are similar and could be combined,\n",
    "and any copy edits. You will only provide suggestions, and not rewrite the copy.\n",
    "\n",
    "Step-by-step instructions:\n",
    "\n",
    "Carefully read through the entire newsletter to understand the overall structure and content.\n",
    "Identify sentences and bullet points that repeat information and provide identical links and should be removed.\n",
    "Identify any sections which could be combined because they contain similar but not identical content.\n",
    "Suggest improvements to any sections which are not clear, concise, and coherent.\n",
    "Take a deep breath and work on this problem step-by-step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d29820",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_md_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe77caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(mail_md_str.replace(\"$\", \"\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1b746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_prompt1 = f\"\"\"You will act like a professional editor with expertise in content optimization.\n",
    "You are skilled at reviewing and enhancing written materials, specializing in\n",
    "helping improve clarity, conciseness, and coherence in various types of documents,\n",
    "including newsletters.\n",
    "\n",
    "Objective: Review the markdown newsletter provided below.\n",
    "It consists of a series of sections, each of which contains several bullet points.\n",
    "For each section, review each bullet point and advise if it should be moved to a different section.\n",
    "You will only provide suggestions, and not rewrite the newsletter or provide other comments except\n",
    "instructions regarding moving bullet points between sections.\n",
    "\n",
    "Step-by-step instructions:\n",
    "\n",
    "Carefully read through the entire newsletter to understand the overall structure and content.\n",
    "Note the titles of the various sections.\n",
    "Identify sentences and bullet points that should be moved to a different section. Write the\n",
    "bullet point and the section in should be moved to.\n",
    "If no bullet points should be moved for a given section, state that no action is required for that section.\n",
    "\n",
    "Check carefully to make sure all similar bullet points end up grouped together in the same section.\n",
    "\n",
    "Take a deep breath and work on this problem step-by-step.\n",
    "\n",
    "Newsletter to edit: \n",
    "{mail_md_str}\n",
    "\"\"\"\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "              {\"role\": \"user\", \"content\": edit_prompt1\n",
    "              }],\n",
    "    n=1,   \n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "response_str1 = response.choices[0].message.content\n",
    "display(Markdown(response_str1.replace(\"$\", \"\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_prompt2 = f\"\"\"You will act like a professional editor with expertise in content optimization.\n",
    "You are skilled at reviewing and enhancing written materials, specializing in\n",
    "helping improve clarity, conciseness, and coherence in various types of documents,\n",
    "including newsletters.\n",
    "\n",
    "Objective: Below are editing instructions followed by a markdown newsletter.\n",
    "Carefully review the editing instructions and the markdown newsletter provided below.\n",
    "The newsletter consists of a series of sections, each of which contains several bullet points.\n",
    "Move bullet points according to the editing instructions below from one section to another \n",
    "If there is no change to a specific section, include it unchanged in the response as it appears in the input.\n",
    "Respond with the updated newsletter in markdown format.\n",
    "\n",
    "Editing instructions:\n",
    "\n",
    "Carefully read through the entire newsletter to understand the overall structure and content.\n",
    "Note the titles of the various sections. Then make only the following changes:\n",
    "{response_str1}\n",
    "\n",
    "Newsletter to edit: \n",
    "{mail_md_str}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "              {\"role\": \"user\", \"content\": edit_prompt2\n",
    "              }],\n",
    "    n=1,   \n",
    "    temperature=0.2\n",
    ")\n",
    "response_str2 = response.choices[0].message.content\n",
    "display(Markdown(response_str2.replace(\"$\", \"\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570eaf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_prompt3 = f\"\"\"You will act like a professional editor with expertise in content optimization.\n",
    "You are skilled at reviewing and enhancing written materials, specializing in\n",
    "helping improve clarity, conciseness, and coherence in various types of documents,\n",
    "including newsletters.\n",
    "\n",
    "Objective: Carefully review each section of the markdown newsletter provided below. \n",
    "Each section consists of several bullet points. \n",
    "\n",
    "For each section, identify and combine redundant bullet points:\n",
    "\n",
    "Instructions: \n",
    "For each section, identify bullet points containing identical URLs to other bullet points in the same section \n",
    "Rewrite the section, combining these similar bullet points to eliminate duplication.\n",
    "Do not duplicate any URLs within a section.\n",
    "Check the response carefully and ensure that no links are duplicated within a section.\n",
    "\n",
    "Newsletter to edit: \n",
    "{response_str2}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "              {\"role\": \"user\", \"content\": edit_prompt3\n",
    "              }],\n",
    "    n=1,   \n",
    "    temperature=0.2\n",
    ")\n",
    "response_str3 = response.choices[0].message.content\n",
    "display(Markdown(response_str3.replace(\"$\", \"\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response_str3[11:].replace(\"$\", \"\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b118a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_str3.replace(\"$\", \"\\\\$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54831a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = f\"\"\"You will act as a professional editor with a strong background in technology journalism.\n",
    "You have a deep understanding of current and emerging technology trends, and the ability to \n",
    "produce, edit, and curate high-quality content that engages and informs readers. You are \n",
    "especially skilled at reviewing and enhancing tech writing, helping improve clarity, conciseness, \n",
    "and coherence, and ensuring its accuracy and relevance.\n",
    "\n",
    "Objective: Carefully review each section of the markdown newsletter provided below, which\n",
    "contains several sections consistint of bullet points. Edit the newsletter for issues according\n",
    "to the detailed instructions below, and respond with the updated newsletter or 'Good' if no changes\n",
    "are needed.\n",
    "\n",
    "Instructions: \n",
    "For each section, review the title and edit it to be as short and engaging, and as consistent with the bullets\n",
    "in the section as possible\n",
    "Remove or combine bullet points which are highly duplicative or redundant.\n",
    "Make bullet points as concise as possible with facts.\n",
    "Respond with the updated newsletter only in markdown format, without editorial comment, or the word 'OK' \n",
    "if no changes are recommended.\n",
    "\n",
    "Newsletter to edit: \n",
    "{mail_md_str}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "              {\"role\": \"user\", \"content\": PROMPT\n",
    "              }],\n",
    "    n=1,   \n",
    "    temperature=0.2\n",
    ")\n",
    "response_str3 = response.choices[0].message.content\n",
    "display(Markdown(response_str3.replace(\"$\", \"\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db7ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_md_str = response_str3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = \"\"\"\n",
    "# AI in Drug Discovery\n",
    "\n",
    "- SLAC and Stanford develop AI method to enhance materials discovery - [Google News](https://news.google.com/articles/CBMiSGh0dHBzOi8vcGh5cy5vcmcvbmV3cy8yMDI0LTA3LWFpLWFwcHJvYWNoLW1hdGVyaWFscy1kaXNjb3Zlcnktc3RhZ2UuaHRtbNIBAA?hl=en-US&gl=US&ceid=US:en).\n",
    "- Intel and IOC create Athlete365, an AI chatbot for Olympic athletes - [Google News](https://news.google.com/articles/CBMidmh0dHBzOi8vd3d3LmludGMuY29tL25ld3MtZXZlbnRzL3ByZXNzLXJlbGVhc2VzL2RldGFpbC8xNzAyL2Zyb20tYXRobGV0ZXMtdG8tZ2VuYWktZGV2ZWxvcGVycy1pbnRlbC10YWNrbGVzLXJlYWwtd29ybGTSAQA?hl=en-US&gl=US&ceid=US:en).\n",
    "- Thoughtful AI raises \\$20M for AI-powered revenue cycle automation - [Google News](https://news.google.com/articles/CBMiSGh0dHBzOi8vd3d3LmZpbnNtZXMuY29tLzIwMjQvMDcvdGhvdWdodGZ1bC1haS1yYWlzZXMtMjBtLWluLWZ1bmRpbmcuaHRtbNIBAA?hl=en-US&gl=US&ceid=US:en).\n",
    "- OpenAI and Broadcom discuss new AI chip to reduce GPU reliance - [Google News](https://news.google.com/articles/CBMiVGh0dHBzOi8vZmluYW5jZS55YWhvby5jb20vbmV3cy9vcGVuYWktaG9sZHMtdGFsa3MtYnJvYWRjb20tZGV2ZWxvcGluZy0yMTAwMzEwODkuaHRtbNIBAA?hl=en-US&gl=US&ceid=US:en).\n",
    "- AI Fund and KX Venture Capital partner to boost Thai AI startups - [Google News](https://news.google.com/articles/CBMiZmh0dHBzOi8vd3d3LmJhbmdrb2twb3N0LmNvbS9idXNpbmVzcy9nZW5lcmFsLzI4MzE5MDMvYWktZnVuZC1reHZjLXBhcnRuZXItdG8tZGV2ZWxvcC1sb2NhbC1haS1zdGFydHVwc9IBAA?hl=en-US&gl=US&ceid=US:en).\n",
    "# Machine Learning in Structural Biology\n",
    "\n",
    "- MIT uses machine learning to enhance high-entropy materials design - [Google News](https://news.google.com/articles/CBMiSGh0dHBzOi8vdGVjaHhwbG9yZS5jb20vbmV3cy8yMDI0LTA3LW1hY2hpbmUtc2VjcmV0cy1hZHZhbmNlZC1hbGxveXMuaHRtbNIBAA?hl=en-US&gl=US&ceid=US:en).\n",
    "\n",
    "# OpenAI and Broadcom Partnership\n",
    "\n",
    "- OpenAI and Broadcom to develop new AI chip - [Financial Times](https://www.ft.com/content/496a0c33-1af3-4dbf-977f-04d6804a8d28).\n",
    "- Broadcom projects over \\$11B in AI sales for FY24 - [TipRanks](https://www.tipranks.com/news/broadcom-nasdaqavgo-eyes-ai-chip-collaboration-with-openai).\n",
    "- OpenAI hires ex-Google employees for AI server chip development - [Yahoo Finance](https://finance.yahoo.com/news/openai-holds-talks-broadcom-developing-210031089.html).\n",
    "\n",
    "# Ant-inspired AI for Drones\n",
    "\n",
    "- Army tests Black Hornet 3 drones for squad deployment - [Fox 7 Austin](https://news.google.com/articles/CBMiYGh0dHBzOi8vd3d3LmZveDdhdXN0aW4uY29tL25ld3MvYXJteS10ZXN0aW5nLXBvY2tldC1zaXplZC1kcm9uZXMtY291bGQtc29vbi1iZS1oYW5kcy1ldmVyeS1zcXVhZNIBZGh0dHBzOi8vd3d3LmZveDdhdXN0aW4uY29tL25ld3MvYXJteS10ZXN0aW5nLXBvY2tldC1zaXplZC1kcm9uZXMtY291bGQtc29vbi1iZS1oYW5kcy1ldmVyeS1zcXVhZC5hbXA?hl=en-US&gl=US&ceid=US:en).\n",
    "- Drone warfare in Ukraine shifts military helicopter tactics - [Defense News](https://news.google.com/articles/CBMie2h0dHBzOi8vd3d3LmRlZmVuc2VuZXdzLmNvbS9nbG9iYWwvZXVyb3BlLzIwMjQvMDcvMTkvZHJvbmUtd2FyZmFyZS1pbi11a3JhaW5lLXByb21wdHMtZnJlc2gtdGhpbmtpbmctaW4taGVsaWNvcHRlci10YWN0aWNzL9IBAA?hl=en-US&gl=US&ceid=US:en).\n",
    "- Ukraine's use of AI-driven unmanned systems raises ethical concerns - [American Magazine](https://news.google.com/articles/CBMigwFodHRwczovL3d3dy5hbWVyaWNhbWFnYXppbmUub3JnL3BvbGl0aWNzLXNvY2lldHkvMjAyNC8wNy8xOC91a3JhaW5lLWxldGhhbC1hdXRvbm9tb3VzLXdlYXBvbnMtc3lzdGVtcy1wb3BlLWZyYW5jaXMtdW4taW50ZXJuYXRpb25hbNIBAA?hl=en-US&gl=US&ceid=US:en).\n",
    "- U.S. and China discuss AI risks in military contexts - [Foreign Policy](https://news.google.com/articles/CBMiT2h0dHBzOi8vZm9yZWlnbnBvbGljeS5jb20vMjAyNC8wNy8xOC9jaGluYS1taWxpdGFyeS1haS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS_SAQA?hl=en-US&gl=US&ceid=US:en).\n",
    "- Pentagon emphasizes \"human-machine teaming\" in military - [IEEE Spectrum](https://spectrum.ieee.org/robot-dog-vacuum).\n",
    "\n",
    "# Fei-Fei Li's AI Startup\n",
    "\n",
    "- Blackstone aims to be the largest AI infrastructure investor with \\$2T in data center expenditures - [Google News](https://news.google.com/articles/CBMiaGh0dHBzOi8vd3d3LmJ1c2luZXNzaW5zaWRlci5jb20vYmxhY2tzdG9uZS1zdGV2ZS1zY2h3YXJ6bWFuLW9uLWFpLWluZnJhc3RydWN0dXJlLWludmVzdG1lbnQtZ29hbHMtMjAyNC030gEA?hl=en-US&gl=US&ceid=US:en).\n",
    "- OpenAI developing AI chip to reduce Nvidia reliance - [The Verge](https://www.theverge.com/2024/7/19/24201737/openai-wants-in-on-the-ai-chip-business).\n",
    "- Thoughtful AI raises \\$20M for AI-powered revenue cycle automation - [Google News](https://news.google.com/articles/CBMiSGh0dHBzOi8vd3d3LmZpbnNtZXMuY29tLzIwMjQvMDcvdGhvdWdodGZ1bC1haS1yYWlzZXMtMjBtLWluLWZ1bmRpbmcuaHRtbNIBAA?hl=en-US&gl=US&ceid=US:en).\n",
    "- Jared Leto invests in Captions, a generative AI startup valued at \\$500M - [Google News](https://news.google.com/articles/CBMihAFodHRwczovL3d3dy5mb3huZXdzLmNvbS9lbnRlcnRhaW5tZW50L2phcmVkLWxldG8taW52ZXN0cy01MDBtLWFpLXN0YXJ0dXAtZGVzcGl0ZS1jYWxscy1mcm9tLW90aGVyLXN0YXJzLXNodXQtZG93bi1jb250cm92ZXJzaWFsLXRlY2jSAYgBaHR0cHM6Ly93d3cuZm94bmV3cy5jb20vZW50ZXJ0YWlubWVudC9qYXJlZC1sZXRvLWludmVzdHMtNTAwbS1haS1zdGFydHVwLWRlc3BpdGUtY2FsbHMtZnJvbS1vdGhlci1zdGFycy1zaHV0LWRvd24tY29udHJvdmVyc2lhbC10ZWNoLmFtcA?hl=en-US&gl=US&ceid=US:en).\n",
    "- Major tech companies face financial risks in AI investments - [Google News](https://news.google.com/articles/CBMibGh0dHBzOi8vd3d3Lm1hcmtldHdhdGNoLmNvbS9zdG9yeS9taWNyb3NvZnQtbWV0YS1hbWF6b24tYW5kLWdvb2dsZS1mYWNlLXRoaXMtZ3Jvd2luZy1yaXNrLWFyb3VuZC1haS0yOGJjYTRhN9IBcGh0dHBzOi8vd3d3Lm1hcmtldHdhdGNoLmNvbS9hbXAvc3RvcnkvbWljcm9zb2Z0LW1ldGEtYW1hem9uLWFuZC1nb29nbGUtZmFjZS10aGlzLWdyb3dpbmctcmlzay1hcm91bmQtYWktMjhiY2E0YTc?hl=en-US&gl=US&ceid=US:en).\n",
    "\n",
    "# Artificial Agency's Funding\n",
    "\n",
    "- Saronic raises \\$175M for autonomous military boats, valued at over \\$1B - [Forbes](https://www.forbes.com/sites/davidjeans/2024/07/18/andreessen-horowitz-saronic-funding/).\n",
    "- Jared Leto invests in Captions, a generative AI startup valued at \\$500M - [Google News](https://news.google.com/articles/CBMihAFodHRwczovL3d3dy5mb3huZXdzLmNvbS9lbnRlcnRhaW5tZW50L2phcmVkLWxldG8taW52ZXN0cy01MDBtLWFpLXN0YXJ0dXAtZGVzcGl0ZS1jYWxscy1mcm9tLW90aGVyLXN0YXJzLXNodXQtZG93bi1jb250cm92ZXJzaWFsLXRlY2jSAYgBaHR0cHM6Ly93d3cuZm94bmV3cy5jb20vZW50ZXJ0YWlubWVudC9qYXJlZC1sZXRvLWludmVzdHMtNTAwbS1haS1zdGFydHVwLWRlc3BpdGUtY2FsbHMtZnJvbS1vdGhlci1zdGFycy1zaHV0LWRvd24tY29udHJvdmVyc2lhbC10ZWNoLmFtcA?hl=en-US&gl=US&ceid=US:en).\n",
    "- AI Fund and KX Venture Capital partner to boost Thai AI startups - [Google News](https://news.google.com/articles/CBMiZmh0dHBzOi8vd3d3LmJhbmdrb2twb3N0LmNvbS9idXNpbmVzcy9nZW5lcmFsLzI4MzE5MDMvYWktZnVuZC1reHZjLXBhcnRuZXItdG8tZGV2ZWxvcC1sb2NhbC1haS1zdGFydHVwc9IBAA?hl=en-US&gl=US&ceid=US:en).\n",
    "- TSMC to allocate chip capacity for OpenAI's chips if large orders are placed - [Google News](https://news.google.com/articles/CBMiU2h0dHBzOi8vd2NjZnRlY2guY29tL3RzbWMtaXMtd2lsbGluZy10by1hbGxvY2F0ZS1jYXBhY2l0eS1mb3Itb3BlbmFpcy1jaGlwcy1yZXBvcnQv0gFXaHR0cHM6Ly93Y2NmdGVjaC5jb20vdHNtYy1pcy13aWxsaW5nLXRvLWFsbG9jYXRlLWNhcGFjaXR5LWZvci1vcGVuYWlzLWNoaXBzLXJlcG9ydC9hbXAv?hl=en-US&gl=US&ceid=US:en).\n",
    "\n",
    "# CTERA's \\$80M Funding\n",
    "\n",
    "- CTERA raises \\$80M in Series D funding led by Red Dot Capital Partners - [TechCrunch](https://techcrunch.com/2023/10/01/ctera-80m-series-d-funding/).\n",
    "- Funds to accelerate product development, expand global sales, and enhance customer support - [TechCrunch](https://techcrunch.com/2023/10/01/ctera-80m-series-d-funding/).\n",
    "- CTERA to innovate cloud storage solutions and strengthen enterprise market position - [TechCrunch](https://techcrunch.com/2023/10/01/ctera-80m-series-d-funding/).\n",
    "\n",
    "# Anthropic's \\$100M AI Fund\n",
    "\n",
    "- Blackstone aims to be the largest AI infrastructure investor with \\$2T in data center expenditures - [Google News](https://news.google.com/articles/CBMiaGh0dHBzOi8vd3d3LmJ1c2luZXNzaW5zaWRlci5jb20vYmxhY2tzdG9uZS1zdGV2ZS1zY2h3YXJ6bWFuLW9uLWFpLWluZnJhc3RydWN0dXJlLWludmVzdG1lbnQtZ29hbHMtMjAyNC030gEA?hl=en-US&gl=US&ceid=US:en).\n",
    "- Jared Leto invests in Captions, a generative AI startup valued at \\$500M - [Google News](https://news.google.com/articles/CBMihAFodHRwczovL3d3dy5mb3huZXdzLmNvbS9lbnRlcnRhaW5tZW50L2phcmVkLWxldG8taW52ZXN0cy01MDBtLWFpLXN0YXJ0dXAtZGVzcGl0ZS1jYWxscy1mcm9tLW90aGVyLXN0YXJzLXNodXQtZG93bi1jb250cm92ZXJzaWFsLXRlY2jSAYgBaHR0cHM6Ly93d3cuZm94bmV3cy5jb20vZW50ZXJ0YWlubWVudC9qYXJlZC1sZXRvLWludmVzdHMtNTAwbS1haS1zdGFydHVwLWRlc3BpdGUtY2FsbHMtZnJvbS1vdGhlci1zdGFycy1zaHV0LWRvd24tY29udHJvdmVyc2lhbC10ZWNoLmFtcA?hl=en-US&gl=US&ceid=US:en).\n",
    "- TSMC to allocate chip capacity for OpenAI's chips if large orders are placed - [Google News](https://news.google.com/articles/CBMiU2h0dHBzOi8vd2NjZnRlY2guY29tL3RzbWMtaXMtd2lsbGluZy10by1hbGxvY2F0ZS1jYXBhY2l0eS1mb3Itb3BlbmFpcy1jaGlwcy1yZXBvcnQv0gFXaHR0cHM6Ly93Y2NmdGVjaC5jb20vdHNtYy1pcy13aWxsaW5nLXRvLWFsbG9jYXRlLWNhcGFjaXR5LWZvci1vcGVuYWlzLWNoaXBzLXJlcG9ydC9hbXAv?hl=en-US&gl=US&ceid=US:en)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "display(Markdown(z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openbb import obb\n",
    "import datetime\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# log in\n",
    "obb.account.login(email=os.environ['OPENBB_USER'], password=os.environ['OPENBB_PW'], remember_me=True)\n",
    "\n",
    "# Search for AI news from today\n",
    "results = obb.news.company(\"META, MSFT, GOOG, AAPL, AMZN, NVDA, TSLA\", provider='yfinance', limit=20).to_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd121b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33012d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainewsbot",
   "language": "python",
   "name": "ainewsbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
