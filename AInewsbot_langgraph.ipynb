{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c703a9",
   "metadata": {},
   "source": [
    "### Newsbot to write a daily AI news summary using langgraph\n",
    "- Save a list of HTML files from sources.yaml (tech news sites)\n",
    "- Extract URLs for the news stories\n",
    "- Filter URLs to remove duplicates, articles seen before, and non-AI articles (using a ChatGPT prompt)\n",
    "- Perform headline topic analysis and sort by topic to help the AI structure the response by topic\n",
    "- Scrape and summarize individual articles\n",
    "- Compose and email the summary\n",
    "- Used to generate a daily newsletter at skynetandchill.com \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74032f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to selectively re-import as needed\n",
    "import sys\n",
    "# del sys.modules['ainb_llm']\n",
    "# del sys.modules['ainb_const']\n",
    "# del sys.modules['ainb_utilities']\n",
    "# del sys.modules['ainb_webscrape']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "562be45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import sqlite3\n",
    "import json\n",
    "from collections import Counter\n",
    "import uuid\n",
    "from typing import TypedDict, Annotated\n",
    "import subprocess\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import operator\n",
    "import pickle\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, MessagesPlaceholder, PromptTemplate,\n",
    "                                    SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "from langchain_core.output_parsers import SimpleJsonOutputParser, JsonOutputParser, StrOutputParser\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import bs4\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import multiprocessing\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "from IPython.display import HTML, Image, Markdown, display\n",
    "import markdown\n",
    "\n",
    "import pyperclip\n",
    "import shlex\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "import trafilatura   # web scrape uses this to get clean news stories w/o a lot of js and boilerplate\n",
    "\n",
    "import pdb\n",
    "\n",
    "import dotenv\n",
    "VERBOSE=1\n",
    "from ainb_const import (DOWNLOAD_DIR, PAGES_DIR,\n",
    "                        MODEL, LOWCOST_MODEL, HIGHCOST_MODEL, CANONICAL_TOPICS,\n",
    "                        SOURCECONFIG, FILTER_PROMPT, TOPIC_PROMPT,\n",
    "                        SUMMARIZE_SYSTEM_PROMPT, SUMMARIZE_USER_PROMPT, FINAL_SUMMARY_PROMPT, \n",
    "                        TOP_CATEGORIES_PROMPT, TOPIC_REWRITE_PROMPT, REWRITE_PROMPT,\n",
    "                        MAX_INPUT_TOKENS, MAX_OUTPUT_TOKENS, MAX_RETRIES, TEMPERATURE, SQLITE_DB,\n",
    "                        HOSTNAME_SKIPLIST, SITE_NAME_SKIPLIST, TOPSOURCES\n",
    "                       )\n",
    "from ainb_utilities import (log, delete_files, filter_unseen_urls_db, insert_article,\n",
    "                            nearest_neighbor_sort, agglomerative_cluster_sort, traveling_salesman_sort_scipy,\n",
    "                            unicode_to_ascii, send_gmail)\n",
    "from ainb_webscrape import (get_driver, quit_drivers, launch_drivers, get_file, get_url, parse_file,\n",
    "                            get_og_tags, get_path_from_url, trimmed_href, process_source_queue_factory,\n",
    "                            process_url_queue_factory, get_google_news_redirects)\n",
    "from ainb_llm import (paginate_df, process_pages, fetch_pages, fetch_openai, fetch_all_summaries,\n",
    "                      fetch_openai_summary, count_tokens, trunc_tokens, \n",
    "                      categorize_headline, categorize_df, clean_topics,\n",
    "                      get_site_name, fetch_missing_site_names)\n",
    "\n",
    "\n",
    "import asyncio\n",
    "# need this to run async in jupyter since it already has an asyncio event loop running\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ba13ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python            3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:26:25) [Clang 17.0.6 ]\n",
      "LangChain         0.3.7\n",
      "OpenAI            1.53.0\n",
      "trafilatura       1.12.2\n",
      "numpy             1.26.4\n",
      "pandas            2.2.3\n",
      "sklearn           1.5.2\n",
      "umap              0.5.7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python            {sys.version}\")\n",
    "print(f\"LangChain         {langchain.__version__}\")\n",
    "print(f\"OpenAI            {openai.__version__}\")\n",
    "# print(f\"smtplib           {smtplib.sys.version}\")\n",
    "print(f\"trafilatura       {trafilatura.__version__}\")\n",
    "# print(f\"bs4               {bs4.__version__}\")\n",
    "print(f\"numpy             {np.__version__}\")\n",
    "print(f\"pandas            {pd.__version__}\")\n",
    "print(f\"sklearn           {sklearn.__version__}\")\n",
    "print(f\"umap              {umap.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb072688",
   "metadata": {},
   "source": [
    "# Test LLM calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed3b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic LLM call with langchain\n",
    "model = ChatOpenAI(model=MODEL)\n",
    "\n",
    "model.invoke([\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content='Listen to me. You are beautiful. You are perfect and I love you.'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a langchain template\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt_template | model | parser\n",
    "chain.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382e44e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time multiple templates (single-threaded)\n",
    "prompt_inputs = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "\n",
    "system_template = 'Translate the following into {language}:'\n",
    "user_template = 'Listen to me. You are {adjective1}. You are {adjective2} and I love you.'\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template),\n",
    "     (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "start_time = datetime.now()\n",
    "for tpl in prompt_inputs:\n",
    "    response = \"\"\n",
    "    # stream tokens as they are generated\n",
    "    for r in chain.stream(tpl):\n",
    "        print(r, end=\"\")\n",
    "        response += r\n",
    "end_time = datetime.now()\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659782d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same but send all at once using asyncio\n",
    "\n",
    "async def async_langchain(chain, input_dict):\n",
    "    response = await chain.ainvoke(input_dict)\n",
    "    return response\n",
    "\n",
    "\n",
    "prompt_templates = [\n",
    "    {\"language\": \"French\", \"adjective1\": \"flawless\", \"adjective2\": \"beautiful\"},\n",
    "    {\"language\": \"German\", \"adjective1\": \"immaculate\", \"adjective2\": \"exquisite\"},\n",
    "    {\"language\": \"Spanish\", \"adjective1\": \"perfect\", \"adjective2\": \"gorgeous\"},\n",
    "    {\"language\": \"Italian\", \"adjective1\": \"amazing\", \"adjective2\": \"magnificent\"},\n",
    "    {\"language\": \"Hungarian\", \"adjective1\": \"ravishing\", \"adjective2\": \"stunning\"},\n",
    "]\n",
    "\n",
    "start_time = datetime.now()\n",
    "tasks = []\n",
    "for d in prompt_templates:\n",
    "    task = asyncio.create_task(async_langchain(chain, d))\n",
    "    tasks.append(task)\n",
    "responses = await asyncio.gather(*tasks)\n",
    "end_time = datetime.now()\n",
    "\n",
    "\n",
    "difference = end_time - start_time\n",
    "total_seconds = difference.total_seconds()\n",
    "print(f\"\\n\\nElapsed seconds: {total_seconds:.6f}\")\n",
    "print(\"\\n\".join(responses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ddd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test o1-preview, note temperature, system prompt not supported\n",
    "# client = OpenAI()\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"o1-preview\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Write a Python script that takes a matrix represented as a string with format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\"\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "# print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554be5ce",
   "metadata": {},
   "source": [
    "# Functions for state graph\n",
    "- Each takes a state dict and returns updated state dict\n",
    "- We will use these later to construct a LangGraph agent workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "N_BROWSERS = 4\n",
    "MAX_EDITS = 2\n",
    "before_date = None\n",
    "# before_date = '2024-10-19 15:00:00'\n",
    "do_download = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b7857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to maintain state within graph\n",
    "# import pydantic\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    AIdf: list[dict]                    #  the current working set of headlines (pandas dataframe not supported)\n",
    "    before_date: str                    #  ignore stories before this date for deduplication (force reprocess since)\n",
    "    do_download: bool                   #  if False use existing files, else download from sources\n",
    "    sources: dict                       #  sources to scrap\n",
    "    sources_reverse: dict[str, str]     #  map file names to sources\n",
    "    bullets: list[str]                  #  bullet points for summary email\n",
    "    summary: str                        #  final summary\n",
    "    cluster_topics: list[str]           #  list of cluster topics\n",
    "    topics_str: str                     #  edited topics\n",
    "    n_edits: int                        #  count edit iterations so we don't keep editing forever\n",
    "    edit_complete: bool                 #  edit will update if no more edits to make\n",
    "    # message thread with OpenAI\n",
    "    # messages: Annotated[list[AnyMessage], operator.add]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state = AgentState(\n",
    "    {'AIdf': [{}],\n",
    "    'before_date': before_date,\n",
    "    'do_download': do_download,\n",
    "    'sources': {},\n",
    "    'sources_reverse': {},\n",
    "    'bullets': '',\n",
    "    'summary': '',\n",
    "    'cluster_topics': [],\n",
    "    'topics_str': '',\n",
    "    'n_edits': 0,\n",
    "    'edit_complete': False,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc712e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize reading configurations from YAML file\n",
    "\n",
    "def fn_initialize(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Initializes the agent state by loading source configurations from SOURCECONFIG (sources.yaml) .\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent.\n",
    "        verbose (bool, optional): Whether to print verbose output. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent.\n",
    "\n",
    "    Raises:\n",
    "        yaml.YAMLError: If there is an error while loading the YAML file.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #  load sources to scrape from sources.yaml\n",
    "    with open(SOURCECONFIG, \"r\") as stream:\n",
    "        try:\n",
    "            state['sources'] = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    log(f\"Initialized {len(state['sources'])} items in sources from {SOURCECONFIG}\")\n",
    "\n",
    "    # make a reverse dict to map file titles to source names\n",
    "    state['sources_reverse'] = {}\n",
    "    for k, v in state['sources'].items():\n",
    "        log(f\"{k} -> {v['url']} -> {v['title']}.html\")\n",
    "        v['sourcename'] = k\n",
    "        # map filename (title) to source name\n",
    "        state['sources_reverse'][v['title']] = k\n",
    "\n",
    "    log(f\"Initialized {len(state['sources_reverse'])} items in sources_reverse\")\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_initialize(test_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83608da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape sources with selenium and save local files in DOWNLOAD_DIR (htmldata)\n",
    "def fn_download_sources(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Scrapes sources and saves HTML files.\n",
    "    If state[\"do_download\"] is True, deletes all files in DOWNLOAD_DIR (htmldata) and scrapes fresh copies.\n",
    "    If state[\"do_download\"] is False, uses existing files in DOWNLOAD_DIR.\n",
    "    Uses state[\"sources\"] for config info on sources to scrape\n",
    "    For each source, saves the current filename to state[\"sources\"][sourcename]['latest']\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent.\n",
    "        do_delete (bool, optional): Whether to delete files in DOWNLOAD_DIR. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    if state.get(\"do_download\"):\n",
    "        # empty download directories\n",
    "        delete_files(DOWNLOAD_DIR)\n",
    "        delete_files(PAGES_DIR)\n",
    "\n",
    "        # save each file specified from sources\n",
    "        log(f\"Saving HTML files using {N_BROWSERS} browsers\")\n",
    "\n",
    "        # Create a queue for multiprocessing and populate it\n",
    "        queue = multiprocessing.Queue()\n",
    "        for item in state.get(\"sources\").values():\n",
    "            queue.put(item)\n",
    "\n",
    "        # Function to take the queue and pop entries off and process until none are left\n",
    "        # lets you create an array of functions with different args\n",
    "        callable = process_source_queue_factory(queue)\n",
    "\n",
    "        saved_pages = launch_drivers(N_BROWSERS, callable)\n",
    "        for sourcename, file in saved_pages:\n",
    "            log(f\"Downloaded {sourcename} to {file}\")\n",
    "            state['sources'][sourcename]['latest'] = file\n",
    "        log(f\"Saved {len(saved_pages)} HTML files\")\n",
    "\n",
    "    else:   # use existing files\n",
    "        log(f\"Web fetch disabled, using existing files in {DOWNLOAD_DIR}\")\n",
    "        # Get the current date\n",
    "        datestr = datetime.now().strftime(\"%m_%d_%Y\")\n",
    "        files = [os.path.join(DOWNLOAD_DIR, file)\n",
    "                 for file in os.listdir(DOWNLOAD_DIR)]\n",
    "        # filter files with today's date ending in .html\n",
    "        files = [\n",
    "            file for file in files if datestr in file and file.endswith(\".html\")]\n",
    "        log(f\"Found {len(files)} previously downloaded files\")\n",
    "        for file in files:\n",
    "            log(file)\n",
    "\n",
    "        saved_pages = []\n",
    "        for file in files:\n",
    "            filename = os.path.basename(file)\n",
    "            # locate date like '01_14_2024' in filename\n",
    "            position = filename.find(\" (\" + datestr)\n",
    "            basename = filename[:position]\n",
    "            # match to source name\n",
    "            sourcename = state.get(\"sources_reverse\", {}).get(basename)\n",
    "            if sourcename is None:\n",
    "                log(f\"Skipping {basename}, no sourcename metadata\")\n",
    "                continue\n",
    "            state[\"sources\"][sourcename]['latest'] = file\n",
    "\n",
    "    return state\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_download_sources(test_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65845bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_extract_urls(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Extracts news URLs from the latest HTML files matching the patterns defined in the state['sources'] configuration info.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the extracted URLs stored in state['AIdf'].\n",
    "    \"\"\"\n",
    "    # Parse news URLs and titles from downloaded HTML files\n",
    "    log(\"Parsing html files\")\n",
    "    all_urls = []\n",
    "    for sourcename, sourcedict in state['sources'].items():\n",
    "        filename = sourcedict.get('latest')\n",
    "        if not filename:\n",
    "            log(f\"no filename found for {sourcename}\")\n",
    "            continue\n",
    "\n",
    "        log(sourcename + ' -> ' + filename)\n",
    "        links = parse_file(state['sources'][sourcename])\n",
    "        log(f\"{len(links)} links found\")\n",
    "        all_urls.extend(links)\n",
    "\n",
    "    log(f\"Saved {len(all_urls)} links\")\n",
    "\n",
    "    # make a pandas dataframe of all the links found\n",
    "    AIdf = (\n",
    "        pd.DataFrame(all_urls)\n",
    "        .groupby(\"url\")\n",
    "        .first()\n",
    "        .reset_index()\n",
    "        .sort_values(\"src\")[[\"src\", \"title\", \"url\"]]\n",
    "        .reset_index(drop=True)\n",
    "        .reset_index(drop=False)\n",
    "        .rename(columns={\"index\": \"id\"})\n",
    "    )\n",
    "    state['AIdf'] = AIdf.to_dict(orient='records')\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_extract_urls(test_state)\n",
    "\n",
    "    # s/b 17 but if some like bloomberg\" or wsj are missing, maybe got a robot block\n",
    "    # if so, download missing ones manually and then re-run fn_extract_urls\n",
    "    print(len(pd.DataFrame(test_state[\"AIdf\"]).groupby('src').count()))\n",
    "    display(pd.DataFrame(test_state[\"AIdf\"]).groupby('src').count()[['id']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44940fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get AI news via newscatcher\n",
    "# https://docs.newscatcherapi.com/api-docs/endpoints/search-news\n",
    "\n",
    "newscatcher_sources = ['247wallst.com',\n",
    " '9to5mac.com',\n",
    " 'androidauthority.com',\n",
    " 'androidcentral.com',\n",
    " 'androidheadlines.com',\n",
    " 'appleinsider.com',\n",
    " 'benzinga.com',\n",
    " 'cnet.com',\n",
    " 'cnn.com',\n",
    " 'digitaltrends.com',\n",
    " 'engadget.com',\n",
    " 'fastcompany.com',\n",
    " 'finextra.com',\n",
    " 'fintechnews.sg',\n",
    " 'fonearena.com',\n",
    " 'ft.com',\n",
    " 'gadgets360.com',\n",
    " 'geekwire.com',\n",
    " 'gizchina.com',\n",
    " 'gizmochina.com',\n",
    " 'gizmodo.com',\n",
    " 'gsmarena.com',\n",
    " 'hackernoon.com',\n",
    " 'howtogeek.com',\n",
    " 'ibtimes.co.uk',\n",
    " 'itwire.com',\n",
    " 'lifehacker.com',\n",
    " 'macrumors.com',\n",
    " 'mashable.com',\n",
    "#  'medium.com',\n",
    " 'mobileworldlive.com',\n",
    " 'msn.com',\n",
    " 'nypost.com',\n",
    " 'phonearena.com',\n",
    " 'phys.org',\n",
    " 'popsci.com',\n",
    " 'scmp.com',\n",
    " 'sify.com',\n",
    " 'siliconangle.com',\n",
    " 'siliconera.com',\n",
    " 'siliconrepublic.com',\n",
    " 'slashdot.org',\n",
    " 'slashgear.com',\n",
    " 'statnews.com',\n",
    " 'tech.co',\n",
    " 'techcrunch.com',\n",
    " 'techdirt.com',\n",
    " 'technode.com',\n",
    " 'technologyreview.com',\n",
    " 'techopedia.com',\n",
    " 'techradar.com',\n",
    " 'techraptor.net',\n",
    " 'techtimes.com',\n",
    " 'techxplore.com',\n",
    " 'telecomtalk.info',\n",
    " 'thecut.com',\n",
    " 'thedrum.com',\n",
    " 'thehill.com',\n",
    " 'theregister.com',\n",
    " 'theverge.com',\n",
    " 'thurrott.com',\n",
    " 'tipranks.com',\n",
    " 'tweaktown.com',\n",
    " 'videocardz.com',\n",
    " 'washingtonpost.com',\n",
    " 'wccftech.com',\n",
    " 'wired.com',\n",
    " 'xda-developers.com',\n",
    " 'yahoo.com',\n",
    " 'zdnet.com']\n",
    "\n",
    "def fn_extract_newscatcher(state: AgentState) -> AgentState:\n",
    "    \n",
    "    q = 'Artificial Intelligence'\n",
    "    page_size = 100\n",
    "    log(f\"Fetching top {page_size} stories matching {q} from Newscatcher\")\n",
    "    base_url = \"https://api.newscatcherapi.com/v2/search\"\n",
    "    time_24h_ago = datetime.now() - timedelta(hours=24)\n",
    "\n",
    "    # Put API key in headers \n",
    "    headers = {'x-api-key': os.getenv('NEWSCATCHER_API_KEY')}\n",
    "\n",
    "    # Define search parameters\n",
    "    params = {\n",
    "        'q': q,\n",
    "        'lang': 'en',\n",
    "        'sources': ','.join(newscatcher_sources),\n",
    "        'from': time_24h_ago.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'to': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'page_size': page_size, # by default should be most highly relevant to the search\n",
    "        'page': 1\n",
    "        }\n",
    "\n",
    "    # Make API call with headers and params\n",
    "    response = requests.get(base_url, headers=headers, params=params)\n",
    "\n",
    "    # Encode received results\n",
    "    results = json.loads(response.text.encode())\n",
    "    if response.status_code != 200:\n",
    "        print('ERROR: API call failed.')\n",
    "        print(results)\n",
    "        \n",
    "    # merge into existing df\n",
    "    newscatcher_df = pd.DataFrame(results['articles'])[['title', 'link']]\n",
    "    newscatcher_df['src']='Newscatcher'\n",
    "    newscatcher_df = newscatcher_df.rename(columns={'link': 'url'})\n",
    "#     display(newscatcher_df.head())\n",
    "    AIdf = pd.DataFrame(state['AIdf'])\n",
    "#     display(AIdf.head())\n",
    "    \n",
    "    max_id = AIdf['id'].max()\n",
    "    # add id column to newscatcher_df\n",
    "    newscatcher_df['id'] = range(max_id + 1, max_id + 1 + len(newscatcher_df))\n",
    "    AIdf = pd.concat([AIdf, newscatcher_df], ignore_index=True)\n",
    "    state['AIdf'] = AIdf.to_dict(orient='records')\n",
    "    return state\n",
    "    \n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_extract_newscatcher(test_state)\n",
    "    print(len(pd.DataFrame(test_state[\"AIdf\"]).groupby('src').count()))\n",
    "    display(pd.DataFrame(test_state[\"AIdf\"]).groupby('src').count()[['id']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3143ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some debugging stuff\n",
    "# zdf = pd.DataFrame(test_state[\"AIdf\"])\n",
    "# zdf.loc[zdf[\"src\"]=='Techmeme']\n",
    "\n",
    "# connection = sqlite3.connect('articles.db')\n",
    "# xdf = pd.read_sql_query(\"SELECT * FROM news_articles\", connection)\n",
    "# connection.close()\n",
    "\n",
    "# zdf.loc[zdf[\"title\"].str.startswith(\"NHTSA\")] \n",
    "# xdf.loc[xdf[\"title\"].str.startswith(\"NHTSA\")] \n",
    "\n",
    "# pd.set_option('display.max_rows', 300)  # Ensure up to 300 rows are shown\n",
    "\n",
    "# xdf.loc[xdf['isAI']> 0].groupby('actual_src') \\\n",
    "#     .count() \\\n",
    "#     .reset_index()[['actual_src', 'id']] \\\n",
    "#     .sort_values('id', ascending=False) \\\n",
    "#     .head(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb53b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlite schema to store previously processed \n",
    "# CREATE TABLE news_articles (\n",
    "#     id INTEGER PRIMARY KEY,\n",
    "#     src TEXT,\n",
    "#     title TEXT,\n",
    "#     url TEXT UNIQUE,\n",
    "#     isAI BOOLEAN,\n",
    "#     article_date DATE\n",
    "# , timestamp DATETIME, actual_url TEXT, actual_src TEXT);\n",
    "#\n",
    "# CREATE TABLE sites (\n",
    "#     id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "#     hostname TEXT UNIQUE NOT NULL,\n",
    "#     site_name TEXT NOT NULL\n",
    "# );\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter and clean URLs for new AI stories\n",
    "\n",
    "def fn_filter_urls(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Filters the URLs in state[\"AIdf\"] to include only those that have not been previously seen,\n",
    "    and are related to AI according to the response from a ChatGPT prompt.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent.\n",
    "        before_date (str, optional): The date before which the URLs should be filtered. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "\n",
    "\n",
    "        AgentState: The updated state of the agent with the filtered URLs stored in state[\"AIdf\"].\n",
    "\n",
    "    \"\"\"\n",
    "    # filter to URL not previously seen\n",
    "    AIdf = pd.DataFrame(state['AIdf'])\n",
    "\n",
    "    AIdf = filter_unseen_urls_db(AIdf, before_date=state.get(\"before_date\"))\n",
    "\n",
    "    if len(AIdf) == 0:\n",
    "        log(\"No new URLs, returning\")\n",
    "        return state\n",
    "\n",
    "    # dedupe identical headlines\n",
    "    # filter similar titles differing by type of quote or something\n",
    "    AIdf['title'] = AIdf['title'].apply(unicode_to_ascii)\n",
    "    AIdf['title_clean'] = AIdf['title'].map(lambda s: \"\".join(s.split()))\n",
    "    AIdf = AIdf.sort_values(\"src\") \\\n",
    "        .groupby(\"title_clean\") \\\n",
    "        .first() \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .drop(columns=['id']) \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'index': 'id'})\n",
    "    log(f\"Found {len(AIdf)} unique new headlines\")\n",
    "\n",
    "    # structured response format\n",
    "    json_schema = {\n",
    "        \"name\": \"json_schema\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"isai_array\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"id\": {\n",
    "                                        \"type\": \"number\"\n",
    "                                    },\n",
    "                                    \"isAI\": {\n",
    "                                        \"type\": \"boolean\"\n",
    "                                    }\n",
    "                                },\n",
    "                        \"required\": [\"id\", \"isAI\"],\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"isai_array\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # filter AI-related headlines using a prompt\n",
    "    pages = paginate_df(AIdf)\n",
    "    enriched_urls = asyncio.run(fetch_pages(pages, prompt=FILTER_PROMPT, json_schema=json_schema))\n",
    "    filter_df = pd.DataFrame(enriched_urls)\n",
    "    \n",
    "    try:  # for idempotency\n",
    "        AIdf = AIdf.drop(columns=['isAI'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # merge returned df with isAI column into original df on id column\n",
    "    AIdf = pd.merge(AIdf, filter_df, on=\"id\", how=\"outer\")\n",
    "    log(AIdf.columns)\n",
    "    # set hostname based on actualurl\n",
    "    AIdf['actual_url'] = AIdf['url']    # ideally resolve redirects but Google News blocks\n",
    "    AIdf['hostname']=AIdf['actual_url'].apply(lambda url: urlparse(url).netloc)\n",
    "\n",
    "    # update SQLite database with all seen URLs (we are doing this using url and ignoring redirects)\n",
    "    log(f\"Inserting {len(AIdf)} URLs into {SQLITE_DB}\")\n",
    "    conn = sqlite3.connect(SQLITE_DB)\n",
    "    cursor = conn.cursor()\n",
    "    for row in AIdf.itertuples():\n",
    "        insert_article(conn, cursor, row.src, row.hostname, row.title,\n",
    "                       row.url, row.actual_url, row.isAI, datetime.now().date())\n",
    "\n",
    "    # keep headlines that are related to AI\n",
    "    AIdf = AIdf.loc[AIdf[\"isAI\"]==1] \\\n",
    "        .reset_index(drop=True)  \\\n",
    "        .reset_index()  \\\n",
    "        .drop(columns=[\"id\"])  \\\n",
    "        .rename(columns={'index': 'id'})\n",
    "\n",
    "    log(f\"Found {len(AIdf)} AI headlines\")\n",
    "\n",
    "    # update actual URLs for Google News redirects\n",
    "    # I think Google changed something so this no longer works, instead of a 301 redirct\n",
    "    # get a javascript page that redirects. Also tomorrow we might see different URLs for same stories\n",
    "    # AIdf = get_google_news_redirects(AIdf)\n",
    "\n",
    "    conn = sqlite3.connect('articles.db')\n",
    "    query = \"select * from sites\"\n",
    "    sites_df = pd.read_sql_query(query, conn)\n",
    "    sites_dict = {row.hostname:row.site_name for row in sites_df.itertuples()}\n",
    "    conn.close()\n",
    "\n",
    "    # get clean site_name\n",
    "    AIdf['site_name'] = AIdf['hostname'].apply(lambda hostname: sites_dict.get(hostname, hostname))\n",
    "\n",
    "    # if any missing clean site names, populate them using OpenAI\n",
    "    missing_site_names =  len(AIdf.loc[AIdf['site_name']==\"\"])\n",
    "    if missing_site_names:\n",
    "        log(f\"Asking OpenAI for {missing_site_names} missing site names\")\n",
    "        responses = asyncio.run(fetch_missing_site_names(AIdf))\n",
    "        # update site_dict from responses\n",
    "        new_urls = []\n",
    "        for r in responses:\n",
    "            if r['url'].startswith('https://'):\n",
    "                r['url'] = r['url'][8:]\n",
    "            new_urls.append(r['url'])\n",
    "            sites_dict[r['url']] = r['site_name']\n",
    "            log(f\"Looked up {r['url']} -> {r['site_name']}\")\n",
    "        # update sites table with new names\n",
    "        for url in new_urls:\n",
    "            sqlstr = \"INSERT OR IGNORE INTO sites (hostname, site_name) VALUES (?, ?);\"\n",
    "            log(f\"Updated {url} -> {sites_dict[url]}\")\n",
    "            conn.execute(sqlstr, (url, sites_dict[url]))\n",
    "            conn.commit()\n",
    "        # reapply to AIdf with updated sites\n",
    "        AIdf['site_name'] = AIdf['hostname'].apply(lambda hostname: sites_dict.get(hostname, hostname))\n",
    "    else:\n",
    "        log(\"No missing site names\")\n",
    "  \n",
    "    # drop banned slop sites\n",
    "\n",
    "    AIdf = AIdf.loc[~AIdf[\"hostname\"].str.lower().isin(HOSTNAME_SKIPLIST)]\n",
    "    AIdf = AIdf.loc[~AIdf[\"site_name\"].str.lower().isin(SITE_NAME_SKIPLIST)]\n",
    "    \n",
    "    state[\"AIdf\"] = AIdf.to_dict(orient='records')\n",
    "    return state\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_filter_urls(test_state)\n",
    "    display(pd.DataFrame(test_state[\"AIdf\"]).groupby('src').count()[['id']])\n",
    "    display(list(pd.DataFrame(test_state[\"AIdf\"]).columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e968501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in pd.DataFrame(test_state[\"AIdf\"]).itertuples():\n",
    "#     display(Markdown(f\"[{row.id}. {row.title} - {row.src}]({row.url})\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c42f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for topic extraction\n",
    "# TODO: could send more prompts to gpt-4o-mini and not hit rate limits\n",
    "def clean_topics(row, lcategories):\n",
    "    \"\"\"\n",
    "    Cleans the extracted_topics and assigned_topics by removing certain common topics and combining them into a single list.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series): The row containing the extracted_topics and assigned_topics.\n",
    "        lcategories (set): The set of lowercase categories.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned and combined list of topics.\n",
    "    \"\"\"\n",
    "    extracted_topics = [x.title() for x in row.extracted_topics if x.lower() not in {\"technology\", \"ai\", \"artificial intelligence\"}]\n",
    "    assigned_topics = [x.title() for x in row.assigned_topics if x.lower() in lcategories]\n",
    "    combined = sorted(list(set(extracted_topics + assigned_topics)))\n",
    "    combined = [s.replace(\"Ai\", \"AI\") for s in combined]\n",
    "    combined = [s.replace(\"Genai\", \"Gen AI\") for s in combined]\n",
    "    combined = [s.replace(\"Openai\", \"OpenAI\") for s in combined]\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "async def do_cat(AIdf, categories):\n",
    "    \"\"\"\n",
    "    Sends a prompt to ChatGPT to select topics for the title for each row in AIdf\n",
    "    which match the topics in categories.\n",
    "\n",
    "    Args:\n",
    "        AIdf (pandas.DataFrame): The DataFrame containing the headlines.\n",
    "        categories (list): The list of topics to match with the headlines.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are the row IDs and the values are lists\n",
    "        of selected topics for each headline.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    catdict = {}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for i, row in enumerate(AIdf.itertuples()):\n",
    "            tasks = []\n",
    "            log(f\"Categorizing headline {row.id+1} of {len(AIdf)}\")\n",
    "            h = row.title\n",
    "            log(h)\n",
    "            for c in categories:\n",
    "                task = asyncio.create_task(categorize_headline(h, c, session))\n",
    "                tasks.append(task)\n",
    "            responses = await asyncio.gather(*tasks)\n",
    "            catdict[row.id] = [item for sublist in responses for item in sublist]\n",
    "            log(str(catdict[row.id]))\n",
    "\n",
    "    return catdict\n",
    "\n",
    "\n",
    "def fn_topic_analysis(state: AgentState) -> AgentState:\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts and selects topics for each headline in the state['AIdf'] dataframe, scrubs them, and stores them back in the dataframe.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the extracted and selected topics stored in state['AIdf'].\n",
    "    \"\"\"\n",
    "    AIdf = pd.DataFrame(state['AIdf'])\n",
    "    pages = paginate_df(AIdf)\n",
    "    # apply topic extraction prompt to AI headlines\n",
    "    log(\"start free-form topic extraction\")\n",
    "    json_schema = {\n",
    "        \"name\": \"extracted_topics\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"extracted_topics\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"id\": {\n",
    "                                \"type\": \"number\",\n",
    "                            },\n",
    "                            \"topics\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                },\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"id\", \"topics\"],\n",
    "                        \"additionalProperties\": False,\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"extracted_topics\"],\n",
    "            \"additionalProperties\": False,\n",
    "        }\n",
    "    }\n",
    "    response = asyncio.run(fetch_pages(pages, prompt=TOPIC_PROMPT, json_schema=json_schema))\n",
    "    topic_df = pd.DataFrame(response)\n",
    "\n",
    "    topic_df = topic_df.rename(columns={'topics': 'extracted_topics'})\n",
    "    log(f\"{len(topic_df)} free-form topics extracted\")\n",
    "    all_topics = [item.lower() for row in topic_df.itertuples() for item in row.extracted_topics]\n",
    "    item_counts = Counter(all_topics)\n",
    "    filtered_topics = [item for item in item_counts if item_counts[item] >= 2 and item not in {'technology', 'ai', 'artificial intelligence'}]\n",
    "\n",
    "    categories = sorted(CANONICAL_TOPICS)\n",
    "    # use categories that are canonical or show up twice in freeform\n",
    "    lcategories = set([c.lower() for c in categories] + [c.lower() for c in filtered_topics])\n",
    "    # new topics\n",
    "    log([c for c in filtered_topics if c not in categories])\n",
    "\n",
    "    catdict = asyncio.run(categorize_headline(AIdf, categories=categories))\n",
    "    topic_df['assigned_topics'] = topic_df['id'].apply(lambda id: catdict.get(id, \"\"))\n",
    "\n",
    "#     pdb.set_trace()\n",
    "\n",
    "    topic_df[\"topics\"] = topic_df.apply(lambda t: clean_topics(t, lcategories), axis=1)\n",
    "    topic_df[\"topic_str\"] = topic_df.apply(lambda row: \", \".join(row.topics), axis=1)\n",
    "\n",
    "    try: # for idempotency\n",
    "        AIdf = AIdf.drop(columns=['topic_str', 'title_topic_str'])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    AIdf = pd.merge(AIdf, topic_df[[\"id\", \"topic_str\"]], on=\"id\", how=\"outer\")\n",
    "    AIdf['title_topic_str'] = AIdf.apply(lambda row: f'{row.title} (Topics: {row.topic_str})', axis=1)\n",
    "\n",
    "    state[\"AIdf\"] = AIdf.to_dict(orient='records')\n",
    "    return state\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_topic_analysis(test_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426fbbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def write_topic_name(topic_list_str, max_retries=3, model=LOWCOST_MODEL):\n",
    "    \"\"\"\n",
    "    Generates a name for a cluster based on a list of headline topics.\n",
    "\n",
    "    Parameters:\n",
    "    session (aiohttp.ClientSession): The client session for making async HTTP requests.\n",
    "    topic_list_str (str): A string containing the list of headline topics.\n",
    "    max_retries (int, optional): The maximum number of retries in case of an error. Defaults to 3.\n",
    "    model (str, optional): The model to use for generating the topic name. Defaults to LOWCOST_MODEL.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the generated topic name.\n",
    "\n",
    "    Example Usage:\n",
    "    title_topic_str_list = \"Headline 1 (Topic: Topic 1)\\n\\nHeadline 2 (Topic: Topic 2)\"\n",
    "    result = await write_topic_name(session, title_topic_str_list)\n",
    "    print(result)\n",
    "\n",
    "    Output:\n",
    "    {\"topic_title\": \"Generated Topic Name\"}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    TOPIC_WRITER_PROMPT = f\"\"\"\n",
    "You are a topic writing assistant. I will provide a list of headlines with extracted topics in parentheses.\n",
    "Your task is to propose a name for a topic that very simply, clearly and accurately captures all the provided\n",
    "headlines in less than 7 words. You will output a JSON object with the key \"topic_title\".\n",
    "\n",
    "Example Input:\n",
    "In the latest issue of Caixins weekly magazine: CATL Bets on 'Skateboard Chassis' and Battery Swaps to Dispell Market Concerns (powered by AI) (Topics: Battery Swaps, Catl, China, Market Concerns, Skateboard Chassis)\n",
    "\n",
    "AI, cheap EVs, future Chevy  the week (Topics: Chevy, Evs)\n",
    "\n",
    "Electric Vehicles and AI: Driving the Consumer & World Forward (Topics: Consumer, Electric Vehicles, Technology)\n",
    "\n",
    "Example Output:\n",
    "{{\"topic_title\": \"Electric Vehicles\"}}\n",
    "\n",
    "Task\n",
    "Propose the name for the overall topic based on the following provided headlines and individual topics:\n",
    "\n",
    "{topic_list_str}\n",
    "\"\"\"\n",
    "\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            messages=[\n",
    "                      {\"role\": \"user\", \"content\": TOPIC_WRITER_PROMPT\n",
    "                      }]\n",
    "\n",
    "            payload = {\"model\":  model,\n",
    "                       \"response_format\": {\"type\": \"json_object\"},\n",
    "                       \"messages\": messages,\n",
    "                       \"temperature\": 0\n",
    "                       }\n",
    "#             print(topic_list_str)\n",
    "\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                response = asyncio.run(fetch_openai(session, payload))\n",
    "            response_dict = json.loads(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "            log(response_dict)\n",
    "            \n",
    "            return response_dict\n",
    "        except Exception as exc:\n",
    "            log(f\"Error: {exc}\")\n",
    "\n",
    "    return {}\n",
    "\n",
    "\n",
    "def fn_topic_clusters(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Fetches embeddings for the headlines, creates clusters of similar articles using DBSCAN, and sorts\n",
    "    using the clusters and a traveling salesman shortest traversal in embedding space.\n",
    "\n",
    "    Parameters:\n",
    "    state (AgentState): The state of the agent.\n",
    "\n",
    "    Returns:\n",
    "    AgentState: The updated state of the agent.\n",
    "\n",
    "    \"\"\"\n",
    "    AIdf = pd.DataFrame(state['AIdf'])\n",
    "\n",
    "    log(f\"Fetching embeddings for {len(AIdf)} headlines\")\n",
    "    embedding_model = 'text-embedding-3-large'\n",
    "    client = OpenAI()\n",
    "    response = client.embeddings.create(input=AIdf['title_topic_str'].tolist(),\n",
    "                                        model=embedding_model)\n",
    "    embedding_df = pd.DataFrame([e.model_dump()['embedding'] for e in response.data])\n",
    "\n",
    "    # greedy traveling salesman sort\n",
    "    log(f\"Sort with nearest_neighbor_sort sort\")\n",
    "    sorted_indices = nearest_neighbor_sort(embedding_df)\n",
    "    AIdf['sort_order'] = sorted_indices\n",
    "\n",
    "    # do dimensionality reduction on embedding_df and cluster analysis\n",
    "    log(f\"Load umap dimensionality reduction model\")\n",
    "    with open(\"reducer.pkl\", 'rb') as file:\n",
    "        # Load the model from the file\n",
    "        reducer = pickle.load(file)\n",
    "    log(f\"Perform dimensionality reduction\")\n",
    "    reduced_data = reducer.transform(embedding_df)\n",
    "    log(f\"Cluster with DBSCAN\")\n",
    "    dbscan = DBSCAN(eps=0.4, min_samples=3)  # Adjust eps and min_samples as needed\n",
    "    AIdf['cluster'] = dbscan.fit_predict(reduced_data)\n",
    "    AIdf.loc[AIdf['cluster'] == -1, 'cluster'] = 999\n",
    "\n",
    "    # sort first by clusters found by DBSCAN, then by semantic ordering\n",
    "    AIdf = AIdf.sort_values(['cluster', 'sort_order']) \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .reset_index() \\\n",
    "        .drop(columns=[\"id\"]) \\\n",
    "        .rename(columns={'index': 'id'})\n",
    "\n",
    "    # show clusters\n",
    "    state[\"cluster_topics\"] = []\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "        for i in range(30):\n",
    "            try:\n",
    "                tmpdf = AIdf.loc[AIdf['cluster']==i][[\"id\", \"title_topic_str\"]]\n",
    "                if len(tmpdf) ==0:\n",
    "                    break\n",
    "                display(tmpdf)\n",
    "                title_topic_str_list = (\"\\n\\n\".join(tmpdf['title_topic_str'].to_list()))\n",
    "                cluster_topic = asyncio.run(write_topic_name(title_topic_str_list))\n",
    "                cluster_topic = cluster_topic['topic_title']\n",
    "                state[\"cluster_topics\"].append(cluster_topic)\n",
    "                log(f\"I dub this cluster: {cluster_topic}\")\n",
    "            except Exception as exc:\n",
    "                log(exc)\n",
    "    AIdf[\"cluster_name\"] = AIdf['cluster'].apply(lambda i: state[\"cluster_topics\"][i] \n",
    "                                                 if i < len(state[\"cluster_topics\"]) \n",
    "                                                 else \"\")\n",
    "    state[\"AIdf\"] = AIdf.to_dict(orient='records')\n",
    "    print(state[\"cluster_topics\"])\n",
    "    return state\n",
    "\n",
    "# TODO: could add a quality rating for stories based on site reputation, length, complexity of story\n",
    "# could then add the quality rating to the summaries and tell the prompt to favor high-quality stories\n",
    "# could put summaries into vector store and retrieve stories by topic. but then you will have to deal\n",
    "# with duplicates across categories, ask the prompt to dedupe\n",
    "\n",
    "# def fn_topic_clusters(state: AgentState) -> AgentState:\n",
    "#     \"call async afn_topic_clusters on state\"\n",
    "#     state = asyncio.run(afn_topic_clusters(state))\n",
    "#     return state\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_topic_clusters(test_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f796cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape individual pages\n",
    "def fn_download_pages(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Uses several Selenium browser sessions to download all the pages referenced in the\n",
    "    state[\"AIdf\"] DataFrame and store their pathnames.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent with the downloaded pages' pathnames stored in the `state[\"AIdf\"]` DataFrame.\n",
    "    \"\"\"\n",
    "    log(\"Queuing URLs for scraping\")\n",
    "    AIdf = pd.DataFrame(state['AIdf'])\n",
    "    queue = multiprocessing.Queue()\n",
    "\n",
    "    count = 0\n",
    "    for row in AIdf.itertuples():\n",
    "#         if row.cluster < 999:\n",
    "        queue.put((row.id, row.url, row.title))\n",
    "        count +=1\n",
    "    # scrape urls in queue asynchronously\n",
    "    num_browsers = 4\n",
    "\n",
    "    callable = process_url_queue_factory(queue)\n",
    "\n",
    "    log(f\"fetching {count} pages using {num_browsers} browsers\")\n",
    "    saved_pages = launch_drivers(num_browsers, callable)\n",
    "\n",
    "    pages_df = pd.DataFrame(saved_pages)\n",
    "    if len(pages_df):\n",
    "        pages_df.columns = ['id', 'url', 'title', 'path']\n",
    "\n",
    "        try: # for idempotency\n",
    "            AIdf = AIdf.drop(columns=['path'])\n",
    "        except:\n",
    "            pass        \n",
    "        AIdf = pd.merge(AIdf, pages_df[[\"id\", \"path\"]], on='id', how=\"inner\")\n",
    "    state[\"AIdf\"] = AIdf.to_dict(orient='records')\n",
    "    return state\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_download_pages(test_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40abdff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize individual pages\n",
    "\n",
    "def fn_summarize_pages(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Reads all the articles, summarizes each one using a ChatGPT prompt, and sends an email with the summaries.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): The current state of the agent.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: The updated state of the agent.\n",
    "\n",
    "    \"\"\"\n",
    "    log(\"Starting summarize\")\n",
    "    AIdf = pd.DataFrame(state['AIdf'])\n",
    "    responses = asyncio.run(fetch_all_summaries(AIdf))\n",
    "    log(f\"Received {len(responses)} summaries\")\n",
    "    response_dict = {}\n",
    "    for i, response in responses:\n",
    "        try:\n",
    "            response_str = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            response_dict[i] = response_str\n",
    "        except Exception as exc:\n",
    "            print(exc)\n",
    "\n",
    "    markdown_str = ''\n",
    "    bullets = []\n",
    "\n",
    "    for i, row in enumerate(AIdf.itertuples()):\n",
    "        try:\n",
    "            topics = []\n",
    "            if row.cluster_name:\n",
    "                topics.append(row.cluster_name)\n",
    "            if row.topic_str:\n",
    "                topics.append(row.topic_str)\n",
    "            topic_str = \", \".join(topics)\n",
    "\n",
    "            mdstr = f\"[{i+1}. {row.title} - {row.site_name}]({row.actual_url})  \\n\\n {topic_str}  \\n\\n{response_dict[row.id]} \\n\\n\"\n",
    "            bullets.append(f\"[{row.title} - {row.site_name}]({row.actual_url})\\n\\nTopics: {row.topic_str} \\n\\n{response_dict[row.id]}\\n\\n\")\n",
    "            display(Markdown(mdstr.replace(\"$\",\"\\\\\\\\$\")))\n",
    "            markdown_str += mdstr\n",
    "        except Exception as exc:\n",
    "            print(\"Error:\", exc)\n",
    "\n",
    "    state['bullets'] = bullets\n",
    "    # Convert Markdown to HTML\n",
    "    html_str = markdown.markdown(markdown_str, extensions=['extra'])\n",
    "    # save bullets\n",
    "    with open('bullets.md', 'w') as f:\n",
    "        f.write(markdown_str)\n",
    "    # send email\n",
    "    log(\"Sending bullet points email\")\n",
    "    subject = f'AI news bullets {datetime.now().strftime(\"%H:%M:%S\")}'\n",
    "    send_gmail(subject, html_str)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_summarize_pages(test_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8696ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_schema = {\n",
    "                \"name\": \"topic_schema\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"topics\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"string\",\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    \"additionalProperties\": False,\n",
    "                    \"required\": [\"topics\"],\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "def topic_rewrite(client, \n",
    "                  model, \n",
    "                  prompt_template,\n",
    "                  topics_str,\n",
    "                  json_schema\n",
    "                 ):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": json_schema\n",
    "        },\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_template.format(topics_str=topics_str)\n",
    "        }])\n",
    "    response_str = response.choices[0].message.content\n",
    "#     print(response_str)\n",
    "    return response_str\n",
    "\n",
    "\n",
    "def fn_propose_cats(state: AgentState) -> AgentState:\n",
    "    # ask chatgpt for top categories\n",
    "    log(f\"Proposing categories using {MODEL}\")\n",
    "\n",
    "    model = ChatOpenAI(\n",
    "        model=MODEL,\n",
    "        temperature=0.3,\n",
    "        model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    "    )\n",
    "\n",
    "    chain = ChatPromptTemplate.from_template(\"{p}\") | model | SimpleJsonOutputParser()\n",
    "    response = chain.invoke({ \"p\": TOP_CATEGORIES_PROMPT + \"\\n\\n\".join(state.get(\"bullets\", []))})\n",
    "    suggested_categories = []\n",
    "    for k, v in response.items():\n",
    "        suggested_categories.extend(v)\n",
    "    state[\"cluster_topics\"] = list(set(state[\"cluster_topics\"] + suggested_categories))\n",
    "    state[\"cluster_topics\"].sort()\n",
    "    topics_str = \"\\n\".join(state[\"cluster_topics\"])\n",
    "    log(f\"Original topics:\\n{topics_str}\")\n",
    "    # rewrite\n",
    "    response_str = topic_rewrite(OpenAI(), 'gpt-4o', TOPIC_REWRITE_PROMPT, topics_str, topic_schema)\n",
    "#     print(response_str)\n",
    "    state[\"topics_str\"] = \"\\n\".join(sorted(json.loads(response_str)['topics']))\n",
    "    log(f\"Updated topics:\\n{state['topics_str']}\")\n",
    "    return state\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_propose_cats(test_state)\n",
    "    print(\"edit the following proposed topic list and update state['cluster_topics']:\")\n",
    "    print('\"' + '\",\\n\"'.join(test_state[\"cluster_topics\"]) + '\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476589f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_compose_summary(state: AgentState) -> AgentState:\n",
    "    log(f\"Composing summary using {HIGHCOST_MODEL}\")\n",
    "\n",
    "    cat_str = state['topics_str']\n",
    "    bullet_str = \"\\n~~~\\n\".join(state.get(\"bullets\", []))\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=HIGHCOST_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": FINAL_SUMMARY_PROMPT.format(cat_str=cat_str, bullet_str=bullet_str)\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "#     print(response)\n",
    "    \n",
    "#     model = ChatOpenAI(\n",
    "#         model=HIGHCOST_MODEL,\n",
    "#         temperature=0.3,\n",
    "#         model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    "#     )\n",
    "\n",
    "#     chain = ChatPromptTemplate.from_template(FINAL_SUMMARY_PROMPT) | model | SimpleJsonOutputParser()\n",
    "#     response = chain.invoke({ \"cat_str\": cat_str, \"bullet_str\": bullet_str})\n",
    "#     print(response)\n",
    "    state[\"summary\"] = response.choices[0].message.content\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_compose_summary(test_state)\n",
    "    display(Markdown(test_state[\"summary\"].replace(\"$\",\"\\\\\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8947de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#post manually to o1-preview and grab response b/c I don't have API yet\n",
    "# cat_str = \"\\n\".join(test_state['cluster_topics'])\n",
    "# bullet_str = \"\\n~~~\\n\".join(test_state[\"bullets\"])\n",
    "# final_summary_prompt = FINAL_SUMMARY_PROMPT.format(cat_str=cat_str, bullet_str=bullet_str)\n",
    "# count_tokens(final_summary_prompt)\n",
    "# # copy to clipboard to paste in o1-preview\n",
    "# subprocess.run(\"pbcopy\", universal_newlines=True, input=final_summary_prompt)\n",
    "# print(final_summary_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db415214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_state[\"summary\"] = \"\"\"\n",
    "# \"\"\"\n",
    "\n",
    "# display(Markdown(test_state[\"summary\"].replace(\"$\",\"\\\\\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_rewrite_summary(state: AgentState) -> AgentState:\n",
    "\n",
    "#     model = ChatOpenAI(\n",
    "#         model=HIGHCOST_MODEL,\n",
    "#         temperature=0.3,\n",
    "#         model_kwargs={\"response_format\": {\"type\": \"json_object\"}}\n",
    "#     )\n",
    "\n",
    "#     chain = ChatPromptTemplate.from_template(REWRITE_PROMPT) | model | SimpleJsonOutputParser()\n",
    "#     response = chain.invoke({ \"summary\": state[\"summary\"]})\n",
    "    log(f\"Rewriting summary using {HIGHCOST_MODEL}\")\n",
    "\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=HIGHCOST_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": REWRITE_PROMPT.format(summary=state[\"summary\"])\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    response_str = response.choices[0].message.content\n",
    "    state[\"n_edits\"] += 1    \n",
    "    if response_str.strip().lower().startswith('ok'):\n",
    "        log(\"No edits made, edit complete\")\n",
    "        state[\"edit_complete\"] = True\n",
    "    else:\n",
    "        state[\"summary\"] = response_str\n",
    "    return state\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state = fn_rewrite_summary(test_state)\n",
    "    display(Markdown(test_state[\"summary\"].replace(\"$\",\"\\\\\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf4496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewrite_prompt = (REWRITE_PROMPT.format(summary=test_state[\"summary\"]))\n",
    "# subprocess.run(\"pbcopy\", universal_newlines=True, input=rewrite_prompt)\n",
    "# print(rewrite_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d40128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_state[\"summary\"] = \"\"\"\n",
    "# \"\"\"\n",
    "\n",
    "# if DEBUG:\n",
    "#     display(Markdown(test_state[\"summary\"].replace(\"$\",\"\\\\\\\\$\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f56983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_is_revision_complete(state: AgentState) -> str:\n",
    "    \"\"\"update edit_complete if MAX_EDITS exceeded\"\n",
    "    return \"complete\" if edit_complete else \"incomplete\"\n",
    "    \"\"\"\n",
    "\n",
    "    if state[\"n_edits\"] >= MAX_EDITS:\n",
    "        log(\"Max edits reached\")\n",
    "        state[\"edit_complete\"] = True\n",
    "\n",
    "\n",
    "    return \"complete\" if state[\"edit_complete\"] else \"incomplete\"\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    test_state[\"edit_complete\"] = fn_is_revision_complete(test_state)\n",
    "    display(test_state[\"edit_complete\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a651d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_send_mail(state: AgentState) -> AgentState:\n",
    "\n",
    "    log(\"Sending summary email\")\n",
    "    # Convert Markdown to HTML\n",
    "    html_str = markdown.markdown(state['summary'], extensions=['extra'])\n",
    "    # extract subject, match a top-level Markdown heading (starts with \"# \")\n",
    "    match = re.search(r\"^# (.+)$\", state[\"summary\"], re.MULTILINE)\n",
    "    \n",
    "    # If a match is found, return the first captured group (the heading text)\n",
    "    if match:\n",
    "        subject = match.group(1).strip()\n",
    "    else:\n",
    "        subject = f'AI news summary {datetime.now().strftime(\"%H:%M:%S\")}'\n",
    "    log(f\"Email subject {subject}\")\n",
    "    log(f\"Email length {len(html_str)}\")\n",
    "        \n",
    "    # send email\n",
    "    send_gmail(subject, html_str)\n",
    "    return state\n",
    "\n",
    "if DEBUG:\n",
    "    fn_send_mail(test_statestate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83bbf66",
   "metadata": {},
   "source": [
    "# Define LangGraph agent state graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae374e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, state):\n",
    "        \n",
    "        self.state = state\n",
    "        \n",
    "        graph_builder = StateGraph(AgentState)\n",
    "        graph_builder.add_node(\"initialize\", self.initialize)\n",
    "        graph_builder.add_node(\"download_sources\", self.download_sources)\n",
    "        graph_builder.add_node(\"extract_web_urls\", self.extract_web_urls)\n",
    "        graph_builder.add_node(\"extract_newscatcher_urls\", self.extract_newscatcher_urls)\n",
    "        graph_builder.add_node(\"filter_urls\", self.filter_urls)\n",
    "        graph_builder.add_node(\"topic_analysis\", self.topic_analysis)\n",
    "        graph_builder.add_node(\"topic_clusters\", self.topic_clusters)\n",
    "        graph_builder.add_node(\"download_pages\", self.download_pages)\n",
    "        graph_builder.add_node(\"summarize_pages\", self.summarize_pages)\n",
    "        graph_builder.add_node(\"propose_topics\", self.propose_topics)\n",
    "        graph_builder.add_node(\"compose_summary\", self.compose_summary)\n",
    "        graph_builder.add_node(\"rewrite_summary\", self.rewrite_summary)\n",
    "        graph_builder.add_node(\"send_mail\", self.send_mail)\n",
    "\n",
    "        graph_builder.add_edge(START, \"initialize\")\n",
    "        graph_builder.add_edge(\"initialize\", \"download_sources\")\n",
    "        graph_builder.add_edge(\"download_sources\", \"extract_web_urls\")\n",
    "        graph_builder.add_edge(\"extract_web_urls\", \"extract_newscatcher_urls\")\n",
    "        graph_builder.add_edge(\"extract_newscatcher_urls\", \"filter_urls\")\n",
    "        graph_builder.add_edge(\"filter_urls\", \"topic_analysis\")\n",
    "        graph_builder.add_edge(\"topic_analysis\", \"topic_clusters\")\n",
    "        graph_builder.add_edge(\"topic_clusters\", \"download_pages\")\n",
    "        graph_builder.add_edge(\"download_pages\", \"summarize_pages\")\n",
    "        graph_builder.add_edge(\"summarize_pages\", \"propose_topics\")\n",
    "        graph_builder.add_edge(\"propose_topics\", \"compose_summary\")\n",
    "        graph_builder.add_edge(\"compose_summary\", \"rewrite_summary\")\n",
    "        graph_builder.add_conditional_edges(\"rewrite_summary\",\n",
    "                                            self.is_revision_complete,\n",
    "                                            {\"incomplete\": \"rewrite_summary\",\n",
    "                                             \"complete\": \"send_mail\",\n",
    "                                            })\n",
    "        graph_builder.add_edge(\"send_mail\", END)\n",
    "\n",
    "        # human in the loop should check web pages downloaded ok, and edit proposed categories \n",
    "#         self.conn = sqlite3.connect('lg_checkpointer.db')\n",
    "#         self.checkpointer = SqliteSaver(conn=self.conn)\n",
    "        self.checkpointer = MemorySaver()\n",
    "        graph = graph_builder.compile(checkpointer=self.checkpointer,\n",
    "                                      interrupt_before=[\"filter_urls\", \"compose_summary\",])\n",
    "        self.graph = graph\n",
    "\n",
    "\n",
    "    def initialize(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_initialize(state)\n",
    "        return self.state\n",
    "\n",
    "    def download_sources(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_download_sources(state)\n",
    "        return self.state\n",
    "\n",
    "    def extract_web_urls(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_extract_urls(state)\n",
    "        return self.state\n",
    "\n",
    "    def extract_newscatcher_urls(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_extract_newscatcher(state)\n",
    "        return self.state\n",
    "\n",
    "    def filter_urls(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_filter_urls(state)\n",
    "        return self.state\n",
    "\n",
    "    def topic_analysis(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_topic_analysis(state)\n",
    "        return self.state\n",
    "\n",
    "    def topic_clusters(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_topic_clusters(state)\n",
    "        return self.state\n",
    "\n",
    "    def download_pages(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_download_pages(state)\n",
    "        return self.state\n",
    "\n",
    "    def summarize_pages(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_summarize_pages(state)\n",
    "        return self.state\n",
    "\n",
    "    def propose_topics(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_propose_cats(state)\n",
    "        return self.state\n",
    "\n",
    "    def compose_summary(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_compose_summary(state)\n",
    "        return self.state\n",
    "\n",
    "    def rewrite_summary(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_rewrite_summary(state)\n",
    "        return self.state\n",
    "\n",
    "    def is_revision_complete(self, state: AgentState) -> str:\n",
    "        return fn_is_revision_complete(state)\n",
    "\n",
    "    def send_mail(self, state: AgentState) -> AgentState:\n",
    "        self.state = fn_send_mail(state)\n",
    "        return self.state\n",
    "    \n",
    "    def run(self, state, config):\n",
    "        # The config is the **second positional argument** to stream() or invoke()!\n",
    "        events = self.graph.stream(state, config, stream_mode=\"values\"\n",
    "        )\n",
    "        \n",
    "        for event in events:\n",
    "            try:\n",
    "                if event.get('summary'):\n",
    "                    display(Markdown(event.get('summary').replace(\"$\",\"\\\\\\\\$\")))\n",
    "                elif event.get('bullets'):\n",
    "                    display(Markdown(\"\\n\\n\".join(event.get('bullets')).replace(\"$\",\"\\\\\\\\$\")))\n",
    "                elif event.get('cluster_topics'):\n",
    "                    display(Markdown(\"\\n\\n\".join(event.get('cluster_topics'))))\n",
    "                elif event.get('AIdf'):\n",
    "                    display(pd.DataFrame(event.get('AIdf')).groupby(\"src\").count()[['id']])\n",
    "                elif event.get('sources'):\n",
    "                    print([k for k in event.get('sources').keys()])\n",
    "            except Exception as exc: \n",
    "                print(exc)\n",
    "                \n",
    "        return self.state\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c93915",
   "metadata": {},
   "source": [
    "# Instantiate agent and show flowchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9585f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_download = False\n",
    "before_date = None\n",
    "before_date = '2024-11-11 017:00:00'\n",
    "# use everything since this date even if already seen\n",
    "# like if you want to rerun it for any reason after sqlite updated\n",
    "\n",
    "# initial state\n",
    "state = AgentState(\n",
    "    {'AIdf': [{}],\n",
    "    'before_date': before_date,\n",
    "    'do_download': do_download,\n",
    "    'sources': {},\n",
    "    'sources_reverse': {},\n",
    "    'bullets': '',\n",
    "    'summary': '',\n",
    "    'cluster_topics': [],\n",
    "    'topics_str': '',\n",
    "    'n_edits': 0,\n",
    "    'edit_complete': False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configuration with thread ID for checkpointing\n",
    "# Generate a random UUID\n",
    "thread_id = uuid.uuid4().hex\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "log(f\"Initializing with before_date={state.get('before_date')}, do_download={do_download}, thread_id={thread_id}\")\n",
    "lg_agent = Agent(state)\n",
    "display(Image(lg_agent.graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ff0b02",
   "metadata": {},
   "source": [
    "# Run each step individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0108a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract, human in the loop checks all sources downloaded OK\n",
    "state = lg_agent.initialize(state)\n",
    "state = lg_agent.download_sources(state)\n",
    "state = lg_agent.extract_web_urls(state)\n",
    "state = lg_agent.extract_newscatcher_urls(state)\n",
    "# s/b 18 check all downloaded, if any got hit by a bot block then download manually\n",
    "print(len(pd.DataFrame(state[\"AIdf\"]).groupby(\"src\").count()[['id']]))\n",
    "pd.DataFrame(state[\"AIdf\"]).groupby(\"src\").count()[['id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580116ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = lg_agent.filter_urls(state)\n",
    "pd.DataFrame(state[\"AIdf\"]).groupby(\"src\").count()[['id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d288bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do topic analysis, identify good topics\n",
    "state = lg_agent.topic_analysis(state)\n",
    "state = lg_agent.topic_clusters(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and summarize pages\n",
    "state = lg_agent.download_pages(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5478962",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = lg_agent.summarize_pages(state)\n",
    "# free-form extract topics from summarize, combine with clusters, propose clean topics\n",
    "state = lg_agent.propose_topics(state)\n",
    "print(state[\"topics_str\"])\n",
    "# compose using o1-preview and rewrite summary 3 times\n",
    "state = lg_agent.compose_summary(state)\n",
    "display(Markdown(state[\"summary\"].replace(\"$\",\"\\\\\\\\$\")))\n",
    "for _ in range(MAX_EDITS):\n",
    "    if lg_agent.is_revision_complete(state)=='complete':\n",
    "        break\n",
    "    state = lg_agent.rewrite_summary(state)\n",
    "    display(Markdown(state[\"summary\"].replace(\"$\",\"\\\\\\\\$\")))\n",
    "state = lg_agent.send_mail(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5305b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit topics in the state graph\n",
    "# topics_str = state[\"topics_str\"]\n",
    "# ct2 = [s.replace('\"', '\\\\\"') for s in topics_str]\n",
    "# Copy to clipboard\n",
    "# os.system(shlex.quote(f\"echo {topics_str} | pbcopy\"))\n",
    "\n",
    "# pyperclip.copy(topics_str)\n",
    "\n",
    "# print(pyperclip.paste())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0dcbc7",
   "metadata": {},
   "source": [
    "# Instantiate agent and run workflow graph end to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed071173",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_download = True\n",
    "before_date = ''\n",
    "# before_date = \"2024-11-11 08:00:00\"\n",
    "\n",
    "# initial state# initial state\n",
    "state = AgentState(\n",
    "    {'AIdf': [{}],\n",
    "    'before_date': before_date,\n",
    "    'do_download': do_download,\n",
    "    'sources': {},\n",
    "    'sources_reverse': {},\n",
    "    'bullets': '',\n",
    "    'summary': '',\n",
    "    'cluster_topics': [],\n",
    "    'topics_str': '',\n",
    "    'n_edits': 0,\n",
    "    'edit_complete': False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configuration with thread ID for checkpointing\n",
    "# Generate a random UUID\n",
    "thread_id = uuid.uuid4().hex\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "log(f\"Initializing with before_date={state.get('before_date')}, do_download={do_download}, thread_id={thread_id}\"\n",
    "    )\n",
    "lg_agent = Agent(state)\n",
    "display(Image(lg_agent.graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf6214",
   "metadata": {},
   "outputs": [],
   "source": [
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb89028",
   "metadata": {},
   "outputs": [],
   "source": [
    "state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = lg_agent.run(state, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c8c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# it stopped here because we compiled with interrupt_before=[\"filter_urls\", \"compose_summary\",]\n",
    "last_state = lg_agent.graph.get_state(config)\n",
    "print(len(last_state.values[\"AIdf\"]))\n",
    "print(last_state.values.get(\"before_date\", ''))\n",
    "last_state.values.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd2b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(last_state.values['AIdf']).groupby('src').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if something got a robot block, download manually and rerun from beginning\n",
    "# if all OK, then proceed\n",
    "# state=None proceeds from last state\n",
    "last_state = lg_agent.graph.get_state(config)\n",
    "state = lg_agent.run(None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edaf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it stopped here because we compiled with interrupt_before=[\"filter_urls\", \"compose_summary\",]\n",
    "last_state = lg_agent.graph.get_state(config)\n",
    "print('next', last_state.next)\n",
    "print('config', last_state.config)\n",
    "print(last_state.values[\"topics_str\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18fc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update cluster_topics if desired\n",
    "lg_agent.graph.update_state (\n",
    "    last_state.config, \n",
    "    {\n",
    "        \"topics_str\": \"\"\"\n",
    "African Healthcare & Agriculture\n",
    "Anthropic & Palantir Defense Partnership\n",
    "Apple iOS 18.2 Updates\n",
    "China AI Chips\n",
    "China Talent Race\n",
    "Construction Industry Transformations\n",
    "Credit Innovation at FinTech Festival\n",
    "Doubao AI Video Generator\n",
    "Google Gemini iOS Launch\n",
    "Grok AI Free Plan\n",
    "Grok API \n",
    "Healthcare Applications\n",
    "Hollywood's 'Heretic' AI Movie\n",
    "Humanoid Robot Artwork Sale\n",
    "Moore Threads Sanction IPO\n",
    "NHS Doctor Assistant\n",
    "Net Zero Emissions Target\n",
    "Passenger Plane Flight Smoother\n",
    "Restored Historical Photos\n",
    "Scientific Data Fabrication\n",
    "Sydney Opera House Image Hoax\n",
    "TSMC Chip Exports Halted\n",
    "UK Immigration Decision Tool        \n",
    "\"\"\"       \n",
    "    },\n",
    "    as_node='propose_topics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0246532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_state = lg_agent.graph.get_state(config)\n",
    "print('next', last_state.next)\n",
    "print('config', last_state.config)\n",
    "last_state.values[\"topics_str\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d28255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume running the graph\n",
    "state = lg_agent.run(None, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_state = lg_agent.graph.get_state(config)\n",
    "display(Markdown(last_state.values[\"summary\"].replace(\"$\",\"\\\\\\\\$\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8fdec2",
   "metadata": {},
   "source": [
    "# Create AI-generated podcast from the summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6d258d",
   "metadata": {},
   "source": [
    "- Uses Podcastfy by Tharzis Souza https://github.com/souzatharsis/podcastfy\n",
    "- IIUC it fetches the URLs and generates a very long prompt saying 'make a podcast script'\n",
    "- For my purpose I have sections and bullet points\n",
    "- I could probably send each section individually, something like\n",
    "    - prompt to do intro, let's dive in, do the first section with a title, bullet text, article texts via trafilatura\n",
    "    - iteratively loop with a complex prompt, the podcast script so far, the next items to add\n",
    "    - add a rewrite step after completion to clean it up, spice it up \n",
    "- try different elevenlabs options and maybe look for the google tts that notebooklm uses and try to get a more natural inflection\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af0885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# absolute path of the inner podcastfy module directory\n",
    "module_path = os.path.abspath(os.path.join('podcastfy_module'))\n",
    "print(module_path)\n",
    "# Add it to sys.path\n",
    "sys.path.append(module_path)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%ls /Users/drucev/projects/AInewsbot/podcastfy_module/podcastfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878df6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import podcastfy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c22613",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(podcastfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from podcastfy.client import generate_podcast\n",
    "from IPython.display import Audio, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_audio(audio_file):\n",
    "    \"\"\"\n",
    "    Embeds an audio file in the notebook, making it playable.\n",
    "\n",
    "    Args:\n",
    "        audio_file (str): Path to the audio file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        display(Audio(audio_file))\n",
    "        print(f\"Audio player embedded for: {audio_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error embedding audio: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c85e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summary to local file\n",
    "filename = 'summary.md'\n",
    "\n",
    "try:\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(state.get(\"summary\"))\n",
    "        print(f\"Markdown content successfully saved to {filename}.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3f8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git push it because we want to download from a URL\n",
    "debate_config = {\n",
    "    \"word_count\": 3000,  # Longer content for in-depth discussions\n",
    "    \"conversation_style\": [\"conversational\", \"fast-paced\", \"informal\", \"engaging\", \"thoughtful\", \"analytical\", \"balanced\"],\n",
    "    \"roles_person1\": \"main summarizer\",\n",
    "    \"roles_person2\": \"questioner/clarifier\",\n",
    "    \"dialogue_structure\": [\"Introduction\", \"Discuss News of the day\", \"Conclusion\"],\n",
    "    \"podcast_name\": \"Skynet and Chill\",\n",
    "    \"podcast_tagline\": \"Today's AI news, lovingly curated by man and machine\",\n",
    "    \"output_language\": \"English\",\n",
    "    \"engagement_techniques\": [\"questions\", \"analogies\", \"humor\"],\n",
    "    \"creativity\": 0.5  # Lower creativity for more factual content\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa589c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = generate_podcast(urls=[\"https://github.com/druce/AInewsbot/blob/main/summary.md\"],\n",
    "                              conversation_config=debate_config,\n",
    "                              tts_model='elevenlabs'\n",
    "                            \n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6db5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_audio(audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the file into ./podcast.mp3\n",
    "\n",
    "try:\n",
    "    # Extract the filename from the given path\n",
    "    filename = os.path.basename(audio_file)\n",
    "\n",
    "    # Define the destination path (current directory)\n",
    "    destination = os.path.join(os.getcwd(), 'podcast.mp3')\n",
    "\n",
    "    # Move the file to the current directory\n",
    "    shutil.move(audio_file, destination)\n",
    "    print(f\"File '{filename}' successfully moved to the current directory.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file '{pathname}' does not exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a video with ffmpeg and a static image\n",
    "\n",
    "!ffmpeg -y -loop 1 -i test.jpg -i podcast.mp3 -c:v mpeg4 -c:a aac -b:a 192k -shortest -pix_fmt yuv420p podcast.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undetected_chromedriver might be faster than Firefox which takes a long time to launch with selenium\n",
    "# but chrome causes some hassles with conflicting profiles\n",
    "\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "# import re\n",
    "# import os\n",
    "# # import uuid\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# # from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# import undetected_chromedriver as uc\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# # use firefox v. chrome b/c it updates less often, can disable updates\n",
    "# # recommend importing profile from Chrome for cookies, passwords\n",
    "# # looks less like a bot with more user cruft in the profile\n",
    "# # from selenium.webdriver.firefox.options import Options\n",
    "# # from selenium.webdriver.firefox.service import Service\n",
    "\n",
    "# # import bs4\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# from ainb_const import DOWNLOAD_DIR, PAGES_DIR, CHROME_PROFILE_PATH, CHROME_PROFILE, CHROME_DRIVER_PATH, GECKODRIVER_PATH, FIREFOX_PROFILE_PATH, MINTITLELEN, sleeptime\n",
    "# from ainb_utilities import log\n",
    "\n",
    "# # get a page title from html or tags if you have not-descriptive link titles like 'link'\n",
    "\n",
    "\n",
    "# CHROME_PROFILE_PATH = '/Users/drucev/Library/Application Support/Google/Chrome'\n",
    "# CHROME_PROFILE =  'Default'\n",
    "# CHROME_DRIVER_PATH =  '/Users/drucev/Library/Application Support/undetected_chromedriver/undetected_chromedriver'\n",
    "# # initialize selenium driver\n",
    "# log(f\"{os.getpid()} Initializing webdriver\", \"get_driver\")\n",
    "\n",
    "# options = uc.ChromeOptions()\n",
    "# options.add_argument(f'--user-data-dir={CHROME_PROFILE_PATH}')\n",
    "# options.add_argument(f'--profile-directory={CHROME_PROFILE_PATH}')\n",
    "# log(\"Initialized webdriver profile\", \"get_driver\")\n",
    "\n",
    "\n",
    "# driver = uc.Chrome(options=options,\n",
    "# #                    driver_executable_path=CHROME_DRIVER_PATH,\n",
    "#                    version_main=130,\n",
    "#                    )\n",
    "# driver.get(\"https://www.yahoo.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca079f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db17bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ainewsbot",
   "language": "python",
   "name": "ainewsbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
